{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rousselpaul/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk.internals\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Initialisation de données# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "questions = {}\n",
    "pairs_train = []\n",
    "y_train = []\n",
    "with open('train.csv','r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in questions:\n",
    "            questions[l[1]] = l[3]\n",
    "        if l[2] not in questions:\n",
    "            questions[l[2]] = l[4]\n",
    "\n",
    "        pairs_train.append([l[1], l[2]])\n",
    "        y_train.append(int(l[5][:-1]))\n",
    "\n",
    "pairs_test = []\n",
    "with open('test.csv','r',encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        if l[1] not in questions:\n",
    "            questions[l[1]] = l[3]\n",
    "        if l[2] not in questions:\n",
    "            questions[l[2]] = l[4][:-1]\n",
    "\n",
    "        pairs_test.append([l[1], l[2]])\n",
    "\n",
    "\n",
    "ids2ind = {}\n",
    "for qid in questions:\n",
    "    ids2ind[qid] = len(ids2ind) \n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "A = vec.fit_transform(questions.values())\n",
    "\n",
    "\n",
    "pairs_train_everything = pairs_train\n",
    "pairs_test_submission = pairs_test\n",
    "\n",
    "corpus = [questions[k] for k in questions]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découpage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76095, 3)\n",
      "(4005, 3)\n"
     ]
    }
   ],
   "source": [
    "Pourcentage_test = 0.05\n",
    "pairs_split_train, pairs_split_test = train_test_split(np.c_[ np.array(pairs_train), np.array(y_train) ], test_size=Pourcentage_test, random_state=42)\n",
    "\n",
    "print(pairs_split_train.shape)\n",
    "print(pairs_split_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['264621', '285217', '1'],\n",
       "       ['37774', '331401', '1'],\n",
       "       ['199268', '44306', '1'],\n",
       "       ...,\n",
       "       ['190489', '324591', '0'],\n",
       "       ['274647', '345606', '0'],\n",
       "       ['266795', '261371', '1']], dtype='<U21')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_split_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_train = pairs_split_train[:,0:2]\n",
    "y_train = pairs_split_train[:,2]\n",
    "pairs_test = pairs_split_test[:,0:2]\n",
    "y_test = pairs_split_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['199954', '384085'],\n",
       " ['128681', '237407'],\n",
       " ['170846', '240621'],\n",
       " ['55110', '177468'],\n",
       " ['425513', '400256'],\n",
       " ['105990', '256943'],\n",
       " ['366314', '224793'],\n",
       " ['167249', '167582'],\n",
       " ['344939', '281696'],\n",
       " ['107721', '138968'],\n",
       " ['70380', '189775'],\n",
       " ['422283', '387926'],\n",
       " ['353154', '165955'],\n",
       " ['116890', '223712'],\n",
       " ['56193', '284398'],\n",
       " ['192860', '103497'],\n",
       " ['419061', '401379'],\n",
       " ['365562', '261605'],\n",
       " ['407441', '232114'],\n",
       " ['26559', '148265'],\n",
       " ['17731', '345422'],\n",
       " ['365785', '246499'],\n",
       " ['253071', '379651'],\n",
       " ['191369', '110891'],\n",
       " ['415414', '208199'],\n",
       " ['284484', '407521'],\n",
       " ['216343', '420785'],\n",
       " ['167736', '124268'],\n",
       " ['419030', '355200'],\n",
       " ['48073', '311559'],\n",
       " ['439901', '409602'],\n",
       " ['191427', '178840'],\n",
       " ['92533', '140695'],\n",
       " ['371314', '442265'],\n",
       " ['79616', '41245'],\n",
       " ['22415', '7769'],\n",
       " ['232757', '329622'],\n",
       " ['447323', '426653'],\n",
       " ['150300', '102863'],\n",
       " ['352998', '60737'],\n",
       " ['274167', '239269'],\n",
       " ['20081', '186848'],\n",
       " ['206218', '381587'],\n",
       " ['78910', '83625'],\n",
       " ['155045', '437715'],\n",
       " ['345175', '161264'],\n",
       " ['220432', '277599'],\n",
       " ['64491', '230709'],\n",
       " ['127295', '154855'],\n",
       " ['224495', '380810'],\n",
       " ['268254', '71398'],\n",
       " ['189782', '426801'],\n",
       " ['127720', '245405'],\n",
       " ['305729', '271221'],\n",
       " ['364406', '250345'],\n",
       " ['87644', '78385'],\n",
       " ['21895', '177938'],\n",
       " ['378990', '249988'],\n",
       " ['140727', '408458'],\n",
       " ['377576', '220744'],\n",
       " ['294351', '244841'],\n",
       " ['438637', '307789'],\n",
       " ['282699', '353042'],\n",
       " ['20274', '26236'],\n",
       " ['108889', '40298'],\n",
       " ['348491', '315307'],\n",
       " ['127945', '165463'],\n",
       " ['67874', '145651'],\n",
       " ['154438', '169492'],\n",
       " ['412125', '181723'],\n",
       " ['10056', '350434'],\n",
       " ['96322', '322553'],\n",
       " ['66417', '90481'],\n",
       " ['424390', '296062'],\n",
       " ['78385', '73321'],\n",
       " ['83209', '136128'],\n",
       " ['174230', '299780'],\n",
       " ['33923', '242288'],\n",
       " ['42781', '46500'],\n",
       " ['112727', '183558'],\n",
       " ['417714', '72251'],\n",
       " ['186141', '320776'],\n",
       " ['34036', '22285'],\n",
       " ['267205', '83893'],\n",
       " ['340125', '344892'],\n",
       " ['268762', '449140'],\n",
       " ['300239', '412020'],\n",
       " ['205411', '82553'],\n",
       " ['30437', '194360'],\n",
       " ['400005', '141572'],\n",
       " ['81026', '195968'],\n",
       " ['286032', '74900'],\n",
       " ['255005', '366057'],\n",
       " ['128795', '374203'],\n",
       " ['385010', '391827'],\n",
       " ['377008', '394015'],\n",
       " ['377141', '391890'],\n",
       " ['195387', '10548'],\n",
       " ['424440', '282854'],\n",
       " ['151132', '238517'],\n",
       " ['60926', '226692'],\n",
       " ['397099', '261371'],\n",
       " ['274188', '272005'],\n",
       " ['268711', '128011'],\n",
       " ['406884', '211784'],\n",
       " ['172284', '371776'],\n",
       " ['142775', '412415'],\n",
       " ['298947', '46987'],\n",
       " ['201796', '291831'],\n",
       " ['119133', '117238'],\n",
       " ['303175', '82662'],\n",
       " ['248882', '203396'],\n",
       " ['9190', '74990'],\n",
       " ['126429', '48474'],\n",
       " ['252144', '351230'],\n",
       " ['219155', '251394'],\n",
       " ['408058', '117403'],\n",
       " ['232355', '200941'],\n",
       " ['99791', '341302'],\n",
       " ['414653', '178213'],\n",
       " ['168071', '311527'],\n",
       " ['326178', '98542'],\n",
       " ['438548', '131829'],\n",
       " ['385180', '389372'],\n",
       " ['342115', '372346'],\n",
       " ['100519', '228139'],\n",
       " ['164877', '100425'],\n",
       " ['268990', '99375'],\n",
       " ['112809', '127454'],\n",
       " ['298423', '407727'],\n",
       " ['373637', '383607'],\n",
       " ['179924', '173855'],\n",
       " ['394701', '168209'],\n",
       " ['118711', '172338'],\n",
       " ['449861', '288244'],\n",
       " ['294095', '46284'],\n",
       " ['42131', '42061'],\n",
       " ['75641', '380161'],\n",
       " ['97964', '93459'],\n",
       " ['390315', '51352'],\n",
       " ['116159', '60918'],\n",
       " ['429452', '173202'],\n",
       " ['323669', '3021'],\n",
       " ['140353', '382013'],\n",
       " ['2086', '283811'],\n",
       " ['109276', '426600'],\n",
       " ['80269', '261167'],\n",
       " ['168378', '174496'],\n",
       " ['208766', '427463'],\n",
       " ['8389', '320094'],\n",
       " ['133616', '233114'],\n",
       " ['273180', '361335'],\n",
       " ['320594', '192997'],\n",
       " ['10926', '47568'],\n",
       " ['1409', '222665'],\n",
       " ['183410', '305729'],\n",
       " ['225808', '431512'],\n",
       " ['305900', '117738'],\n",
       " ['145971', '110941'],\n",
       " ['136263', '441808'],\n",
       " ['281476', '445457'],\n",
       " ['360543', '30827'],\n",
       " ['115126', '440408'],\n",
       " ['344256', '437589'],\n",
       " ['405017', '267288'],\n",
       " ['258164', '99427'],\n",
       " ['357724', '409951'],\n",
       " ['428013', '164027'],\n",
       " ['363795', '194543'],\n",
       " ['411030', '43687'],\n",
       " ['266090', '128045'],\n",
       " ['143377', '391554'],\n",
       " ['205301', '349550'],\n",
       " ['232310', '164428'],\n",
       " ['262447', '293622'],\n",
       " ['130619', '269841'],\n",
       " ['17458', '121787'],\n",
       " ['169893', '445601'],\n",
       " ['413851', '104178'],\n",
       " ['301609', '225572'],\n",
       " ['175744', '378522'],\n",
       " ['439514', '12310'],\n",
       " ['193050', '288050'],\n",
       " ['256943', '292300'],\n",
       " ['164274', '198437'],\n",
       " ['31113', '265775'],\n",
       " ['163973', '414853'],\n",
       " ['351061', '28571'],\n",
       " ['205388', '353562'],\n",
       " ['243887', '46352'],\n",
       " ['353335', '98126'],\n",
       " ['148751', '421923'],\n",
       " ['15969', '273668'],\n",
       " ['399481', '385146'],\n",
       " ['450482', '198286'],\n",
       " ['280099', '284958'],\n",
       " ['201475', '374130'],\n",
       " ['253774', '64019'],\n",
       " ['143379', '114059'],\n",
       " ['169818', '423423'],\n",
       " ['100312', '72863'],\n",
       " ['9751', '87482'],\n",
       " ['152717', '107867'],\n",
       " ['415394', '196490'],\n",
       " ['436963', '437020'],\n",
       " ['44580', '69692'],\n",
       " ['79229', '446221'],\n",
       " ['162920', '58681'],\n",
       " ['32218', '30715'],\n",
       " ['284808', '113021'],\n",
       " ['31473', '129494'],\n",
       " ['372647', '123489'],\n",
       " ['349138', '329734'],\n",
       " ['146811', '257087'],\n",
       " ['61185', '117695'],\n",
       " ['45633', '86476'],\n",
       " ['384640', '376106'],\n",
       " ['279995', '36043'],\n",
       " ['229359', '248352'],\n",
       " ['90811', '107374'],\n",
       " ['35679', '29820'],\n",
       " ['1181', '113729'],\n",
       " ['147240', '133693'],\n",
       " ['378444', '148884'],\n",
       " ['138231', '243021'],\n",
       " ['292826', '440197'],\n",
       " ['113703', '232197'],\n",
       " ['147835', '189860'],\n",
       " ['268120', '204796'],\n",
       " ['129976', '100076'],\n",
       " ['165509', '153898'],\n",
       " ['432579', '410957'],\n",
       " ['4330', '412267'],\n",
       " ['118455', '211169'],\n",
       " ['47293', '184857'],\n",
       " ['256621', '223595'],\n",
       " ['203111', '129737'],\n",
       " ['24575', '99229'],\n",
       " ['387381', '452426'],\n",
       " ['29518', '146651'],\n",
       " ['111132', '156485'],\n",
       " ['302953', '426062'],\n",
       " ['37335', '226791'],\n",
       " ['315192', '394018'],\n",
       " ['391999', '428892'],\n",
       " ['348887', '267350'],\n",
       " ['354865', '414618'],\n",
       " ['400183', '52111'],\n",
       " ['256393', '53835'],\n",
       " ['257537', '414962'],\n",
       " ['337928', '357888'],\n",
       " ['62446', '82762'],\n",
       " ['36565', '123141'],\n",
       " ['337491', '214410'],\n",
       " ['14233', '380162'],\n",
       " ['89550', '35051'],\n",
       " ['334645', '209184'],\n",
       " ['36867', '217362'],\n",
       " ['342215', '414770'],\n",
       " ['236944', '59235'],\n",
       " ['88693', '305253'],\n",
       " ['255875', '96376'],\n",
       " ['65161', '291727'],\n",
       " ['3881', '290473'],\n",
       " ['209834', '51778'],\n",
       " ['286725', '39356'],\n",
       " ['437350', '120926'],\n",
       " ['412020', '44487'],\n",
       " ['93038', '32124'],\n",
       " ['72203', '126603'],\n",
       " ['410511', '432115'],\n",
       " ['40839', '155070'],\n",
       " ['167049', '196704'],\n",
       " ['136445', '167595'],\n",
       " ['366846', '279494'],\n",
       " ['439256', '21817'],\n",
       " ['54770', '253487'],\n",
       " ['90645', '103774'],\n",
       " ['353814', '106816'],\n",
       " ['43059', '417503'],\n",
       " ['89147', '310395'],\n",
       " ['33412', '83850'],\n",
       " ['341943', '84148'],\n",
       " ['273628', '292807'],\n",
       " ['211215', '386763'],\n",
       " ['240178', '204184'],\n",
       " ['309845', '442018'],\n",
       " ['338671', '38791'],\n",
       " ['212088', '94258'],\n",
       " ['236795', '131702'],\n",
       " ['8327', '442221'],\n",
       " ['96330', '148611'],\n",
       " ['446687', '69864'],\n",
       " ['440199', '341645'],\n",
       " ['71402', '253130'],\n",
       " ['190924', '40491'],\n",
       " ['324057', '178490'],\n",
       " ['283974', '49041'],\n",
       " ['129845', '98478'],\n",
       " ['134861', '191777'],\n",
       " ['364191', '99935'],\n",
       " ['155788', '230388'],\n",
       " ['13107', '392004'],\n",
       " ['122342', '437768'],\n",
       " ['145162', '247216'],\n",
       " ['80828', '226987'],\n",
       " ['10995', '427067'],\n",
       " ['407471', '30437'],\n",
       " ['428962', '325418'],\n",
       " ['311975', '115840'],\n",
       " ['36984', '336905'],\n",
       " ['7750', '193104'],\n",
       " ['49001', '385884'],\n",
       " ['405599', '199426'],\n",
       " ['434138', '100392'],\n",
       " ['324162', '341537'],\n",
       " ['59652', '98478'],\n",
       " ['108630', '18698'],\n",
       " ['93038', '24944'],\n",
       " ['370225', '113240'],\n",
       " ['246029', '240708'],\n",
       " ['206780', '252280'],\n",
       " ['25097', '266855'],\n",
       " ['169937', '191676'],\n",
       " ['122339', '95029'],\n",
       " ['414352', '146511'],\n",
       " ['163637', '315480'],\n",
       " ['228836', '194732'],\n",
       " ['242573', '328122'],\n",
       " ['302998', '139574'],\n",
       " ['402180', '214283'],\n",
       " ['65163', '152471'],\n",
       " ['360203', '280143'],\n",
       " ['421340', '424003'],\n",
       " ['374900', '391942'],\n",
       " ['298296', '274312'],\n",
       " ['395280', '128190'],\n",
       " ['155806', '196'],\n",
       " ['272756', '164628'],\n",
       " ['330759', '389648'],\n",
       " ['159902', '34839'],\n",
       " ['150410', '71293'],\n",
       " ['92863', '331645'],\n",
       " ['407285', '79214'],\n",
       " ['263888', '257758'],\n",
       " ['48306', '446459'],\n",
       " ['378151', '33991'],\n",
       " ['88636', '267181'],\n",
       " ['101127', '209558'],\n",
       " ['112831', '27425'],\n",
       " ['97254', '44729'],\n",
       " ['268791', '29558'],\n",
       " ['184867', '322811'],\n",
       " ['265558', '105919'],\n",
       " ['140484', '229541'],\n",
       " ['3488', '150410'],\n",
       " ['434974', '292290'],\n",
       " ['179504', '383485'],\n",
       " ['336636', '217788'],\n",
       " ['381443', '443285'],\n",
       " ['361029', '152324'],\n",
       " ['216397', '228372'],\n",
       " ['26854', '107949'],\n",
       " ['385024', '108217'],\n",
       " ['318791', '274201'],\n",
       " ['8042', '91172'],\n",
       " ['434717', '348462'],\n",
       " ['214062', '353497'],\n",
       " ['50110', '51930'],\n",
       " ['388342', '223020'],\n",
       " ['403518', '403136'],\n",
       " ['93266', '251729'],\n",
       " ['278576', '195577'],\n",
       " ['385051', '16678'],\n",
       " ['134300', '415038'],\n",
       " ['161706', '353020'],\n",
       " ['62183', '10564'],\n",
       " ['23360', '342115'],\n",
       " ['230293', '299094'],\n",
       " ['118101', '46703'],\n",
       " ['182938', '445035'],\n",
       " ['316100', '68078'],\n",
       " ['295801', '245866'],\n",
       " ['16992', '387249'],\n",
       " ['128631', '66910'],\n",
       " ['309439', '276738'],\n",
       " ['397895', '439710'],\n",
       " ['293224', '403412'],\n",
       " ['444120', '234083'],\n",
       " ['236911', '223491'],\n",
       " ['168514', '121014'],\n",
       " ['261161', '409191'],\n",
       " ['211481', '293684'],\n",
       " ['427238', '73950'],\n",
       " ['229621', '287814'],\n",
       " ['297879', '432023'],\n",
       " ['208162', '20343'],\n",
       " ['48260', '292611'],\n",
       " ['75639', '181723'],\n",
       " ['236885', '54327'],\n",
       " ['23309', '261785'],\n",
       " ['293684', '53379'],\n",
       " ['436230', '26874'],\n",
       " ['24384', '253788'],\n",
       " ['353768', '152411'],\n",
       " ['191211', '31638'],\n",
       " ['100014', '150357'],\n",
       " ['91605', '378813'],\n",
       " ['287440', '393988'],\n",
       " ['205254', '18665'],\n",
       " ['384838', '255663'],\n",
       " ['13461', '370163'],\n",
       " ['28962', '6962'],\n",
       " ['124071', '428210'],\n",
       " ['186652', '381748'],\n",
       " ['73761', '290997'],\n",
       " ['299627', '242793'],\n",
       " ['51866', '141286'],\n",
       " ['123590', '9439'],\n",
       " ['422505', '327873'],\n",
       " ['137581', '356596'],\n",
       " ['431239', '340877'],\n",
       " ['232276', '390873'],\n",
       " ['378736', '242338'],\n",
       " ['154788', '386766'],\n",
       " ['42441', '15380'],\n",
       " ['244682', '179511'],\n",
       " ['2275', '172631'],\n",
       " ['77810', '378126'],\n",
       " ['220015', '102317'],\n",
       " ['182376', '61348'],\n",
       " ['338304', '7204'],\n",
       " ['154073', '26876'],\n",
       " ['60728', '240684'],\n",
       " ['335283', '179019'],\n",
       " ['436978', '291098'],\n",
       " ['431617', '14062'],\n",
       " ['157101', '286234'],\n",
       " ['147031', '360067'],\n",
       " ['79229', '438755'],\n",
       " ['418388', '180116'],\n",
       " ['159744', '270358'],\n",
       " ['293886', '103771'],\n",
       " ['152807', '232893'],\n",
       " ['114800', '60535'],\n",
       " ['345974', '203534'],\n",
       " ['74079', '17449'],\n",
       " ['222380', '359682'],\n",
       " ['308404', '375518'],\n",
       " ['279121', '440628'],\n",
       " ['332995', '389345'],\n",
       " ['429641', '30437'],\n",
       " ['384753', '78552'],\n",
       " ['34119', '342548'],\n",
       " ['412231', '106285'],\n",
       " ['178575', '239692'],\n",
       " ['359583', '234584'],\n",
       " ['90237', '36402'],\n",
       " ['127544', '397487'],\n",
       " ['113729', '200408'],\n",
       " ['149016', '101602'],\n",
       " ['284661', '5741'],\n",
       " ['39385', '281696'],\n",
       " ['317061', '400218'],\n",
       " ['60954', '252737'],\n",
       " ['246047', '58918'],\n",
       " ['255361', '303429'],\n",
       " ['410813', '103382'],\n",
       " ['177606', '40736'],\n",
       " ['130396', '231763'],\n",
       " ['290082', '56915'],\n",
       " ['22709', '368271'],\n",
       " ['390977', '372866'],\n",
       " ['380144', '304188'],\n",
       " ['63735', '188103'],\n",
       " ['105628', '72678'],\n",
       " ['16678', '104952'],\n",
       " ['314826', '451673'],\n",
       " ['363433', '66422'],\n",
       " ['68852', '259101'],\n",
       " ['350830', '299158'],\n",
       " ['68406', '289633'],\n",
       " ['122457', '368249'],\n",
       " ['158986', '338969'],\n",
       " ['353850', '413799'],\n",
       " ['14303', '270790'],\n",
       " ['315651', '34500'],\n",
       " ['105824', '188033'],\n",
       " ['82148', '303575'],\n",
       " ['177193', '19107'],\n",
       " ['133751', '308559'],\n",
       " ['13330', '447168'],\n",
       " ['380113', '177634'],\n",
       " ['120926', '213871'],\n",
       " ['253927', '211160'],\n",
       " ['358209', '330917'],\n",
       " ['85611', '47351'],\n",
       " ['37211', '150485'],\n",
       " ['24799', '27461'],\n",
       " ['310019', '267258'],\n",
       " ['177309', '118253'],\n",
       " ['325015', '59611'],\n",
       " ['44036', '25061'],\n",
       " ['153531', '28719'],\n",
       " ['232447', '225926'],\n",
       " ['219615', '72321'],\n",
       " ['138843', '25995'],\n",
       " ['213939', '257193'],\n",
       " ['438973', '119582'],\n",
       " ['411878', '142616'],\n",
       " ['305458', '317009'],\n",
       " ['97263', '337881'],\n",
       " ['116892', '170249'],\n",
       " ['347789', '172498'],\n",
       " ['208766', '251774'],\n",
       " ['31495', '443528'],\n",
       " ['81921', '380054'],\n",
       " ['27425', '223008'],\n",
       " ['315567', '225333'],\n",
       " ['33009', '125157'],\n",
       " ['180089', '160276'],\n",
       " ['383551', '112831'],\n",
       " ['70003', '196596'],\n",
       " ['303604', '302202'],\n",
       " ['134458', '199236'],\n",
       " ['398883', '448912'],\n",
       " ['257255', '193458'],\n",
       " ['194127', '27498'],\n",
       " ['269471', '218033'],\n",
       " ['298230', '220378'],\n",
       " ['118217', '386439'],\n",
       " ['104744', '164018'],\n",
       " ['185412', '299724'],\n",
       " ['407060', '400511'],\n",
       " ['383754', '29045'],\n",
       " ['311214', '294299'],\n",
       " ['424331', '197249'],\n",
       " ['382794', '240370'],\n",
       " ['183086', '87324'],\n",
       " ['12265', '280351'],\n",
       " ['10002', '245507'],\n",
       " ['292579', '92579'],\n",
       " ['286935', '187215'],\n",
       " ['395757', '355393'],\n",
       " ['32493', '369694'],\n",
       " ['166698', '39143'],\n",
       " ['358664', '183226'],\n",
       " ['413079', '357729'],\n",
       " ['125805', '68433'],\n",
       " ['360713', '142241'],\n",
       " ['246688', '178228'],\n",
       " ['341235', '275191'],\n",
       " ['346099', '422964'],\n",
       " ['441053', '245507'],\n",
       " ['64075', '288520'],\n",
       " ['1427', '70627'],\n",
       " ['87158', '254808'],\n",
       " ['416027', '391438'],\n",
       " ['221524', '447661'],\n",
       " ['204688', '61524'],\n",
       " ['140533', '360846'],\n",
       " ['150636', '290671'],\n",
       " ['22841', '449652'],\n",
       " ['187883', '395647'],\n",
       " ['206047', '111964'],\n",
       " ['309183', '264876'],\n",
       " ['268196', '232613'],\n",
       " ['142072', '121216'],\n",
       " ['151207', '184672'],\n",
       " ['401492', '196082'],\n",
       " ['387041', '420247'],\n",
       " ['62274', '125447'],\n",
       " ['150974', '156875'],\n",
       " ['119561', '197502'],\n",
       " ['17071', '95074'],\n",
       " ['173362', '185078'],\n",
       " ['118731', '406597'],\n",
       " ['406462', '407545'],\n",
       " ['198834', '276657'],\n",
       " ['193595', '314577'],\n",
       " ['108242', '44675'],\n",
       " ['212870', '360288'],\n",
       " ['394551', '33550'],\n",
       " ['173937', '448913'],\n",
       " ['179909', '428810'],\n",
       " ['384342', '381835'],\n",
       " ['205254', '366036'],\n",
       " ['348925', '44354'],\n",
       " ['142174', '198594'],\n",
       " ['231916', '115690'],\n",
       " ['268149', '179671'],\n",
       " ['155985', '333338'],\n",
       " ['57224', '118704'],\n",
       " ['377679', '332705'],\n",
       " ['250408', '198572'],\n",
       " ['215295', '91167'],\n",
       " ['300588', '363260'],\n",
       " ['264530', '41788'],\n",
       " ['425457', '70859'],\n",
       " ['186123', '251063'],\n",
       " ['116911', '299853'],\n",
       " ['77904', '67633'],\n",
       " ['284750', '227039'],\n",
       " ['151061', '438392'],\n",
       " ['25487', '318849'],\n",
       " ['240669', '407550'],\n",
       " ['53805', '337516'],\n",
       " ['44306', '445488'],\n",
       " ['211215', '359913'],\n",
       " ['299997', '138027'],\n",
       " ['366003', '144375'],\n",
       " ['316771', '313345'],\n",
       " ['59280', '110628'],\n",
       " ['363286', '363089'],\n",
       " ['216706', '410940'],\n",
       " ['116318', '133443'],\n",
       " ['386885', '214982'],\n",
       " ['364654', '394515'],\n",
       " ['64075', '176191'],\n",
       " ['408143', '260724'],\n",
       " ['224638', '45575'],\n",
       " ['319444', '371633'],\n",
       " ['24837', '431463'],\n",
       " ['116896', '171489'],\n",
       " ['386639', '388567'],\n",
       " ['369090', '111498'],\n",
       " ['320462', '12468'],\n",
       " ['71839', '356729'],\n",
       " ['338746', '76116'],\n",
       " ['15143', '171655'],\n",
       " ['190221', '432337'],\n",
       " ['67326', '73301'],\n",
       " ['284171', '185844'],\n",
       " ['421678', '367532'],\n",
       " ['234501', '410511'],\n",
       " ['123033', '46635'],\n",
       " ['415456', '331060'],\n",
       " ['146889', '61003'],\n",
       " ['191345', '343743'],\n",
       " ['339947', '11602'],\n",
       " ['215297', '243449'],\n",
       " ['89028', '352048'],\n",
       " ['365532', '96877'],\n",
       " ['97588', '396687'],\n",
       " ['410741', '12022'],\n",
       " ['243149', '82884'],\n",
       " ['352410', '256476'],\n",
       " ['26721', '259729'],\n",
       " ['413669', '142531'],\n",
       " ['193986', '70343'],\n",
       " ['267899', '242338'],\n",
       " ['167544', '383305'],\n",
       " ['326208', '4408'],\n",
       " ['237595', '157070'],\n",
       " ['418810', '121205'],\n",
       " ['138501', '77702'],\n",
       " ['63144', '441579'],\n",
       " ['44306', '249414'],\n",
       " ['409930', '173231'],\n",
       " ['362404', '128042'],\n",
       " ['427746', '210973'],\n",
       " ['51286', '225572'],\n",
       " ['2075', '295666'],\n",
       " ['196616', '321855'],\n",
       " ['238675', '59114'],\n",
       " ['186953', '396006'],\n",
       " ['336450', '4201'],\n",
       " ['426259', '162777'],\n",
       " ['187038', '35585'],\n",
       " ['389097', '311653'],\n",
       " ['120259', '79828'],\n",
       " ['172143', '188498'],\n",
       " ['269313', '125770'],\n",
       " ['225561', '337719'],\n",
       " ['50047', '363122'],\n",
       " ['131536', '310049'],\n",
       " ['64846', '78517'],\n",
       " ['39797', '271498'],\n",
       " ['337153', '58419'],\n",
       " ['57489', '143098'],\n",
       " ['49307', '398029'],\n",
       " ['300545', '204838'],\n",
       " ['434992', '29844'],\n",
       " ['416215', '205981'],\n",
       " ['281652', '10392'],\n",
       " ['393426', '281405'],\n",
       " ['269286', '320957'],\n",
       " ['293470', '404504'],\n",
       " ['273435', '307071'],\n",
       " ['190587', '59589'],\n",
       " ['18843', '120029'],\n",
       " ['95301', '348590'],\n",
       " ['245922', '208867'],\n",
       " ['445148', '425066'],\n",
       " ['433607', '372179'],\n",
       " ['38557', '61348'],\n",
       " ['274692', '307186'],\n",
       " ['283160', '165340'],\n",
       " ['403697', '245063'],\n",
       " ['45464', '35585'],\n",
       " ['236618', '442521'],\n",
       " ['386166', '122873'],\n",
       " ['36939', '292998'],\n",
       " ['194500', '12303'],\n",
       " ['303790', '115553'],\n",
       " ['273584', '447798'],\n",
       " ['301761', '6685'],\n",
       " ['285723', '412239'],\n",
       " ['290716', '122844'],\n",
       " ['344869', '41391'],\n",
       " ['413338', '214304'],\n",
       " ['139069', '68888'],\n",
       " ['244636', '155553'],\n",
       " ['21872', '444488'],\n",
       " ['414290', '8389'],\n",
       " ['403102', '242762'],\n",
       " ['13939', '64517'],\n",
       " ['208770', '381031'],\n",
       " ['380106', '327268'],\n",
       " ['421388', '408215'],\n",
       " ['25553', '159115'],\n",
       " ['51260', '399633'],\n",
       " ['63624', '42131'],\n",
       " ['175592', '222127'],\n",
       " ['2108', '18627'],\n",
       " ['139618', '210628'],\n",
       " ['326499', '212442'],\n",
       " ['434674', '76778'],\n",
       " ['288489', '394268'],\n",
       " ['205773', '153684'],\n",
       " ['215081', '281804'],\n",
       " ['148460', '181697'],\n",
       " ['377904', '151856'],\n",
       " ['100119', '222880'],\n",
       " ['242288', '126085'],\n",
       " ['439235', '64676'],\n",
       " ['62183', '451137'],\n",
       " ['114523', '285572'],\n",
       " ['368542', '209126'],\n",
       " ['433680', '366796'],\n",
       " ['272005', '115969'],\n",
       " ['69646', '337159'],\n",
       " ['237790', '383295'],\n",
       " ['341302', '387988'],\n",
       " ['347017', '406780'],\n",
       " ['205301', '57221'],\n",
       " ['51096', '388037'],\n",
       " ['200644', '432952'],\n",
       " ['252142', '226754'],\n",
       " ['123269', '79490'],\n",
       " ['335528', '15277'],\n",
       " ['349042', '116247'],\n",
       " ['10082', '206907'],\n",
       " ['49675', '211964'],\n",
       " ['13410', '36402'],\n",
       " ['390090', '225146'],\n",
       " ['313471', '84742'],\n",
       " ['418405', '285065'],\n",
       " ['246756', '151080'],\n",
       " ['374997', '273387'],\n",
       " ['161749', '41823'],\n",
       " ['158423', '377886'],\n",
       " ['186068', '10181'],\n",
       " ['138112', '426801'],\n",
       " ['361727', '69484'],\n",
       " ['397821', '420292'],\n",
       " ['125573', '413124'],\n",
       " ['100929', '169680'],\n",
       " ['234010', '355645'],\n",
       " ['200349', '99427'],\n",
       " ['163532', '243021'],\n",
       " ['413067', '249368'],\n",
       " ['244126', '78293'],\n",
       " ['158332', '210557'],\n",
       " ['439563', '271769'],\n",
       " ['419768', '435203'],\n",
       " ['68466', '165986'],\n",
       " ['215281', '62654'],\n",
       " ['254467', '33154'],\n",
       " ['194959', '94768'],\n",
       " ['99772', '192146'],\n",
       " ['44045', '303175'],\n",
       " ['252924', '368827'],\n",
       " ['333786', '70704'],\n",
       " ['95724', '206086'],\n",
       " ['27402', '234322'],\n",
       " ['243515', '388585'],\n",
       " ['12382', '429128'],\n",
       " ['79350', '170655'],\n",
       " ['298739', '291950'],\n",
       " ['94909', '260467'],\n",
       " ['324250', '283561'],\n",
       " ['57066', '165158'],\n",
       " ['75780', '408693'],\n",
       " ['129153', '383745'],\n",
       " ['406081', '335942'],\n",
       " ['394514', '194568'],\n",
       " ['363083', '25097'],\n",
       " ['139684', '406982'],\n",
       " ['202944', '52426'],\n",
       " ['127636', '446472'],\n",
       " ['376689', '426308'],\n",
       " ['100312', '12612'],\n",
       " ['79000', '47250'],\n",
       " ['349138', '298731'],\n",
       " ['450643', '198775'],\n",
       " ['198090', '288224'],\n",
       " ['268411', '207752'],\n",
       " ['155494', '249343'],\n",
       " ['403102', '336695'],\n",
       " ['312747', '88477'],\n",
       " ['80828', '232613'],\n",
       " ['4953', '450825'],\n",
       " ['192782', '376890'],\n",
       " ['93559', '45418'],\n",
       " ['291727', '346052'],\n",
       " ['285765', '115175'],\n",
       " ['290036', '442747'],\n",
       " ['243706', '299021'],\n",
       " ['394254', '239863'],\n",
       " ['435789', '251932'],\n",
       " ['131909', '291604'],\n",
       " ['202382', '6155'],\n",
       " ['242651', '135404'],\n",
       " ['151911', '167654'],\n",
       " ['343301', '387859'],\n",
       " ['246068', '44585'],\n",
       " ['184732', '352450'],\n",
       " ['350321', '316326'],\n",
       " ['281594', '220072'],\n",
       " ['46950', '20084'],\n",
       " ['94849', '75007'],\n",
       " ['211734', '120805'],\n",
       " ['300776', '426218'],\n",
       " ['170332', '414431'],\n",
       " ['237799', '145630'],\n",
       " ['136788', '296786'],\n",
       " ['27184', '409621'],\n",
       " ['61063', '199200'],\n",
       " ['56007', '53022'],\n",
       " ['207357', '185079'],\n",
       " ['50383', '297690'],\n",
       " ['98262', '442577'],\n",
       " ['93780', '425084'],\n",
       " ['354496', '313166'],\n",
       " ['249495', '66179'],\n",
       " ['318671', '343138'],\n",
       " ['440027', '196334'],\n",
       " ['368113', '431756'],\n",
       " ['331199', '203111'],\n",
       " ['144926', '237959'],\n",
       " ['84095', '105004'],\n",
       " ['322370', '156985'],\n",
       " ['246687', '51531'],\n",
       " ['379101', '167834'],\n",
       " ['60491', '44742'],\n",
       " ['367334', '443562'],\n",
       " ['259218', '365333'],\n",
       " ['88658', '241375'],\n",
       " ['2996', '373966'],\n",
       " ['274647', '345606'],\n",
       " ['341747', '139335'],\n",
       " ['298382', '29170'],\n",
       " ['377597', '64991'],\n",
       " ['414335', '187307'],\n",
       " ['299542', '19701'],\n",
       " ['205723', '50503'],\n",
       " ['8276', '190948'],\n",
       " ['365683', '314723'],\n",
       " ['439612', '154522'],\n",
       " ['337527', '218747'],\n",
       " ['315444', '95362'],\n",
       " ['285051', '143375'],\n",
       " ['157386', '311137'],\n",
       " ['280186', '355901'],\n",
       " ['282196', '407870'],\n",
       " ['343469', '102481'],\n",
       " ['377237', '364179'],\n",
       " ['26631', '343469'],\n",
       " ['77863', '307360'],\n",
       " ['218008', '193470'],\n",
       " ['100318', '218033'],\n",
       " ['117403', '283852'],\n",
       " ['96725', '163642'],\n",
       " ['83659', '169988'],\n",
       " ['311983', '30507'],\n",
       " ['15120', '91120'],\n",
       " ['377171', '223008'],\n",
       " ['100341', '335058'],\n",
       " ['76081', '214952'],\n",
       " ['157956', '430946'],\n",
       " ['294062', '108474'],\n",
       " ['17927', '404080'],\n",
       " ['156650', '136774'],\n",
       " ['330189', '137894'],\n",
       " ['69354', '395080'],\n",
       " ['347453', '334212'],\n",
       " ['46685', '188186'],\n",
       " ['53517', '322107'],\n",
       " ['273070', '325034'],\n",
       " ['34918', '369090'],\n",
       " ['147639', '239108'],\n",
       " ['381363', '412095'],\n",
       " ['155494', '115581'],\n",
       " ['209034', '366904'],\n",
       " ['379697', '341164'],\n",
       " ['340346', '208719'],\n",
       " ['189653', '14934'],\n",
       " ['45292', '9925'],\n",
       " ['65137', '421087'],\n",
       " ['112794', '394015'],\n",
       " ['441603', '193345'],\n",
       " ['308214', '113531'],\n",
       " ['230833', '301605'],\n",
       " ['257504', '447241'],\n",
       " ['362349', '178555'],\n",
       " ['109712', '52866'],\n",
       " ['299368', '15774'],\n",
       " ['155933', '14514'],\n",
       " ['411705', '185553'],\n",
       " ['17633', '16706'],\n",
       " ['104744', '275945'],\n",
       " ['393743', '174248'],\n",
       " ['300717', '272131'],\n",
       " ['368621', '327873'],\n",
       " ['231608', '337153'],\n",
       " ['82914', '134754'],\n",
       " ['89856', '397535'],\n",
       " ['291137', '76072'],\n",
       " ['188481', '254634'],\n",
       " ['120184', '305302'],\n",
       " ['9807', '246896'],\n",
       " ['287982', '401829'],\n",
       " ['442152', '259822'],\n",
       " ['18439', '147313'],\n",
       " ['208766', '354865'],\n",
       " ['145130', '382627'],\n",
       " ['403623', '367458'],\n",
       " ['20105', '301527'],\n",
       " ['346026', '2567'],\n",
       " ['269646', '40501'],\n",
       " ['162683', '397496'],\n",
       " ['320952', '439318'],\n",
       " ['322959', '346217'],\n",
       " ['263255', '363260'],\n",
       " ['133467', '446635'],\n",
       " ['432361', '145630'],\n",
       " ['199999', '81954'],\n",
       " ['233926', '122660'],\n",
       " ['452229', '307423'],\n",
       " ['60490', '6425'],\n",
       " ['248879', '447807'],\n",
       " ['189922', '241491'],\n",
       " ['85521', '32608'],\n",
       " ['292604', '168086'],\n",
       " ['415394', '132189'],\n",
       " ['103296', '106602'],\n",
       " ['98986', '126565'],\n",
       " ['145126', '68566'],\n",
       " ['312724', '374167'],\n",
       " ['48406', '328362'],\n",
       " ['224097', '119701'],\n",
       " ['354319', '11304'],\n",
       " ['332713', '241375'],\n",
       " ['405682', '431624'],\n",
       " ['46593', '55365'],\n",
       " ['209248', '85841'],\n",
       " ['184834', '154797'],\n",
       " ['420760', '201840'],\n",
       " ['79179', '244200'],\n",
       " ['33923', '158763'],\n",
       " ['219267', '104873'],\n",
       " ['20453', '350306'],\n",
       " ['31315', '166534'],\n",
       " ['450927', '266234'],\n",
       " ['294334', '389372'],\n",
       " ['156642', '367127'],\n",
       " ['84394', '377597'],\n",
       " ['150968', '352862'],\n",
       " ['135556', '301872'],\n",
       " ['366697', '253194'],\n",
       " ['426151', '114448'],\n",
       " ['61724', '266383'],\n",
       " ['293171', '367131'],\n",
       " ['396678', '71327'],\n",
       " ['82783', '134340'],\n",
       " ['316897', '212940'],\n",
       " ['323274', '134625'],\n",
       " ['17129', '138988'],\n",
       " ['344722', '395860'],\n",
       " ['360620', '119184'],\n",
       " ['174230', '237761'],\n",
       " ['69512', '290779'],\n",
       " ['425066', '288581'],\n",
       " ['272749', '181781'],\n",
       " ['65479', '28920'],\n",
       " ['16625', '445758'],\n",
       " ['375148', '338501'],\n",
       " ['160151', '416123'],\n",
       " ['450204', '277699'],\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig = pd.DataFrame({'id':[j for j in range(len(pairs_train))],'qid1':pairs_train[:,0],'qid2':pairs_train[:,1],'question1':[questions[x] for x in pairs_train[:,0]],'question2':[questions[x] for x in pairs_train[:,1]],'is_duplicate':y_train})\n",
    "test_orig = pd.DataFrame({'id':[j for j in range(len(pairs_test))],'qid1':pairs_test[:,0],'qid2':pairs_test[:,1],'question1':[questions[x] for x in pairs_test[:,0]],'question2':[questions[x] for x in pairs_test[:,1]]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "https://towardsdatascience.com/finding-similar-quora-questions-with-bow-tfidf-and-random-forest-c54ad88d1370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'something'\n",
    "}\n",
    "\n",
    "def clean(text, stem_words=True):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "\n",
    "#    stops = set(stopwords.words(\"english\"))\n",
    "    # Clean the text, with the option to stem words.\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"can't\", \"can not\", text)\n",
    "    text = re.sub(\"n't\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "#     # all numbers should separate from words, this is too aggressive\n",
    "    \n",
    "#     def pad_number(pattern):\n",
    "#         matched_string = pattern.group(0)\n",
    "#         return pad_str(matched_string)\n",
    "#     text = re.sub('[0-9]+', pad_number, text)\n",
    "    \n",
    "    # add padding to punctuations and special chars, we still need them later\n",
    "    \n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "#    def pad_pattern(pattern):\n",
    "#        matched_string = pattern.group(0)\n",
    "#       return pad_str(matched_string)\n",
    "#    text = re.sub('[\\!\\?\\@\\^\\+\\*\\/\\,\\~\\|\\`\\=\\:\\;\\.\\#\\\\\\]', pad_pattern, text) \n",
    "        \n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
    "    \n",
    "    # indian dollar\n",
    "    \n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
    "    \n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "  \n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation]).lower()\n",
    "       # Return a list of words\n",
    "    return text\n",
    "    \n",
    "#df['question1'] = df['question1'].apply(clean)\n",
    "#df['question2'] = df['question2'].apply(clean)\n",
    "\n",
    "questions_cleaned = {}\n",
    "for qid in questions:\n",
    "    questions_cleaned[qid] = clean(questions[qid])\n",
    "    \n",
    " \n",
    "vec_cleaned = TfidfVectorizer()\n",
    "A_cleaned = vec_cleaned.fit_transform(questions_cleaned.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_colums = len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction cadre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function doit renvoyer X_train et X_test comme features\n",
    "nom_fichier doit être de la forme 'page_rank'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def creat_features(function,nom_fichier):\n",
    "    X_train,X_test = function()\n",
    "    number_colums = len(X_train[0])\n",
    "    N_train = len(pairs_train)\n",
    "    N_test = len(pairs_test)\n",
    "    \n",
    "    nom_features = ''\n",
    "    for j in range(number_colums):        \n",
    "        if j == (number_colums-1):\n",
    "            nom_features = nom_features +'f'+ str(j+1)\n",
    "        else:\n",
    "            nom_features = nom_features +'f'+ str(j+1)+','\n",
    "        \n",
    "        \n",
    "    nom_fichier_train = nom_fichier +'_' +str(N_train) +'_train.csv'\n",
    "    with open(nom_fichier_train, 'w') as f: \n",
    "        f.write(nom_features+ '\\n')\n",
    "        for i in range(len(pairs_train)):\n",
    "            for j in range(number_colums):\n",
    "                if j == (number_colums-1):\n",
    "                    f.write(str(X_train[i,j]))\n",
    "                else:\n",
    "                    f.write(str(X_train[i,j])+',')\n",
    "            f.write('\\n')\n",
    "        \n",
    "    nom_fichier_test = nom_fichier  +'_' +str(N_train) +'_test.csv'   \n",
    "    with open(nom_fichier_test, 'w') as f:\n",
    "        f.write(nom_features+ '\\n')\n",
    "        for i in range(len(pairs_test)):\n",
    "            for j in range(number_colums):\n",
    "                if j == (number_colums-1):\n",
    "                    f.write(str(X_test[i,j]))\n",
    "                else:\n",
    "                    f.write(str(X_test[i,j])+',')\n",
    "            f.write('\\n')\n",
    "    open('names_of_features', 'a+').write(nom_fichier_train+',' + nom_fichier_test+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "creat_features(features_teacher,'features_teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(nom_fichier):\n",
    "    \n",
    "    with open(nom_fichier,'r',encoding='utf8') as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            l = line.rstrip().split(',')\n",
    "            \n",
    "            if first:\n",
    "                X = []\n",
    "                first = False\n",
    "            else:\n",
    "                vect = [float(x) for x in l]\n",
    "                X.append(vect)\n",
    "        return(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46985791, 14.        ,  2.        ],\n",
       "       [ 0.73824329, 25.        ,  1.        ],\n",
       "       [ 0.54415074, 18.        ,  4.        ],\n",
       "       ...,\n",
       "       [ 0.8582973 , 30.        ,  0.        ],\n",
       "       [ 0.23492271, 16.        ,  2.        ],\n",
       "       [ 0.4260053 , 16.        ,  0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_features('features_teacher_76095_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_all_features():\n",
    "    with open('names_of_features','r',encoding='utf8') as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            l = line.rstrip().split(',')\n",
    "            if first:\n",
    "                first = False\n",
    "                X_train = load_features(l[0])\n",
    "                X_test = load_features(l[1])\n",
    "            else:\n",
    "                X_train = np.c_[ X_train, load_features(l[0]) ]\n",
    "                X_test = np.c_[ X_test, load_features(l[1]) ]\n",
    "    return(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test = load_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features du prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_teacher():\n",
    "    N_train = len(pairs_train)\n",
    "    X_train = np.zeros((N_train, 3))\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_train)):\n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        X_train[i,0] = cosine_similarity(A_cleaned[ids2ind[q1],:], A_cleaned[ids2ind[q2],:])\n",
    "        X_train[i,1] = len(questions_cleaned[q1].split()) + len(questions_cleaned[q2].split())\n",
    "        X_train[i,2] = abs(len(questions_cleaned[q1].split()) - len(questions_cleaned[q2].split()))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "\n",
    "    N_test = len(pairs_test)\n",
    "    X_test = np.zeros((N_test, 3))\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_test)):\n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "        X_test[i,0] = cosine_similarity(A_cleaned[ids2ind[q1],:], A_cleaned[ids2ind[q2],:])\n",
    "        X_test[i,1] = len(questions_cleaned[q1].split()) + len(questions_cleaned[q2].split())\n",
    "        X_test[i,2] = abs(len(questions_cleaned[q1].split()) - len(questions_cleaned[q2].split()))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"testing examples processsed\")\n",
    "    return(X_train,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_doc2vec():\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]\n",
    "\n",
    "    max_epochs = 2\n",
    "    vec_size = 100\n",
    "    alpha = 0.025\n",
    "\n",
    "    modeldoc = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    modeldoc.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        modeldoc.train(tagged_data,\n",
    "                    total_examples=modeldoc.corpus_count,\n",
    "                    epochs=modeldoc.iter)\n",
    "        # decrease the learning rate\n",
    "        modeldoc.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        modeldoc.min_alpha = modeldoc.alpha\n",
    "\n",
    "\n",
    "    doc2vec_train1 = []\n",
    "    doc2vec_train2 = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_train)):\n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        doc2vec_train1.append(modeldoc.infer_vector(questions[q1]))\n",
    "        doc2vec_train2.append(modeldoc.infer_vector(questions[q2]))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "\n",
    "    doc2vec_test1 = []\n",
    "    doc2vec_test2 = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_test)):\n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "        doc2vec_test1.append(modeldoc.infer_vector(questions[q1]))\n",
    "        doc2vec_test2.append(modeldoc.infer_vector(questions[q2]))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "\n",
    "\n",
    "    X_train = np.c_[ np.array(doc2vec_train1),np.array(doc2vec_train2) ]\n",
    "    X_test = np.c_[np.array(doc2vec_test1),np.array(doc2vec_test2) ]\n",
    "    return(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "creat_features(features_doc2vec,'features_doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test = load_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features word2vec - non fini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    " \n",
    "    \n",
    "def word2vec_pretrained_sim_features():\n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    def filter(tokens):\n",
    "        tokens_filtered = []\n",
    "        for x in tokens:\n",
    "            if x in model.vocab:\n",
    "                tokens_filtered.append(x)\n",
    "        if not tokens_filtered:\n",
    "            return(['empty'])\n",
    "        return(tokens_filtered)\n",
    "\n",
    "    text_sim_training = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_train)): \n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        text_sim_training.append([model.n_similarity(filter(questions_cleaned[q1].split()),filter(questions_cleaned[q2].split()))])\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "    #text_sim_training\n",
    "\n",
    "    text_sim_testing = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_test)): \n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "        text_sim_testing.append([model.n_similarity(filter(questions_cleaned[q1].split()),filter(questions_cleaned[q2].split()))])\n",
    "\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"testing examples processsed\")\n",
    "    #text_sim_testing\n",
    "\n",
    "    return(np.array(text_sim_training),np.array(text_sim_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "creat_features(word2vec_pretrained_sim_features,'word2vec_pretrained_sim_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_pretrained_features():\n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    def filter(tokens):\n",
    "        tokens_filtered = []\n",
    "        for x in tokens:\n",
    "            if x in model.vocab:\n",
    "                tokens_filtered.append(x)\n",
    "        if not tokens_filtered:\n",
    "            return(['empty'])\n",
    "        return(tokens_filtered)\n",
    "    \n",
    "    word2vec_train1 = []\n",
    "    word2vec_train2 = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_train)):\n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        word2vec_train1.append(model.infer_vector(filter(questions_cleaned[q1].split())))\n",
    "        word2vec_train2.append(model.infer_vector(filter(questions_cleaned[q1].split())))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "\n",
    "    word2vec_test1 = []\n",
    "    word2vec_test2 = []\n",
    "    counter = 0\n",
    "    for i in range(len(pairs_test)):\n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "        word2vec_test1.append(model.infer_vector(filter(questions_cleaned[q1].split())))\n",
    "        word2vec_test2.append(model.infer_vector(filter(questions_cleaned[q1].split())))\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print (counter, \"training examples processsed\")\n",
    "\n",
    "\n",
    "    X_train = np.c_[ np.array(word2vec_train1),np.array(word2vec_train2) ]\n",
    "    X_test = np.c_[np.array(word2vec_test1),np.array(word2vec_test2) ]\n",
    "    return(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EuclideanKeyedVectors' object has no attribute 'infer_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-af55c30f9348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_pretrained_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-219-4177dede013c>\u001b[0m in \u001b[0;36mword2vec_pretrained_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mword2vec_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mword2vec_train2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EuclideanKeyedVectors' object has no attribute 'infer_vector'"
     ]
    }
   ],
   "source": [
    "X_train,X_test = word2vec_pretrained_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-333e722c13ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodeldoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# add info about each word's Huffman encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msort_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate([questions_cleaned[k] for k in questions_cleaned])]\n",
    "\n",
    "max_epochs = 20\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "  \n",
    "modeldoc.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    modeldoc.train(tagged_data,\n",
    "                total_examples=modeldoc.corpus_count,\n",
    "                epochs=modeldoc.iter)\n",
    "    # decrease the learning rate\n",
    "    modeldoc.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    modeldoc.min_alpha = modeldoc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_test = np.delete(X_test, [3,4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_LDA():\n",
    "\n",
    "    corpus_cleaned = [questions_cleaned[k] for k in questions_cleaned]\n",
    "\n",
    "\n",
    "    list_of_list_of_tokens =[]\n",
    "    for i in range(len(list(questions_cleaned.values()))):\n",
    "        text=corpus_cleaned[i].lower()\n",
    "        nltk_tokens = nltk.word_tokenize(text)\n",
    "        stemmed_tokens=[]\n",
    "        for w in nltk_tokens:\n",
    "            if (len(w)>3):\n",
    "                lemmatized_token=wordnet_lemmatizer.lemmatize(w)\n",
    "                stemmed_tokens.append(porter_stemmer.stem(lemmatized_token))\n",
    "        list_of_list_of_tokens.append(stemmed_tokens)\n",
    "\n",
    "    dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
    "    dictionary_LDA.filter_extremes(no_below=3)\n",
    "    corpusLDA = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
    "\n",
    "    num_topics = 20\n",
    "    lda_model = models.LdaModel(corpusLDA, num_topics=num_topics, \n",
    "                                      id2word=dictionary_LDA, \n",
    "                                      passes=4, alpha=[0.01]*num_topics,\n",
    "                                      eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "    for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "        print(str(i)+\": \"+ topic)\n",
    "        print()\n",
    "\n",
    "    counter=0\n",
    "    lda_similarity_training=[]\n",
    "    for i in range (len(pairs_train)):\n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "        all_topics_source=[0]*num_topics\n",
    "        all_topics_target=[0]*num_topics\n",
    "        topics_source= lda_model[corpusLDA[ids2ind[q1]]]\n",
    "        topics_target=lda_model[corpusLDA[ids2ind[q2]]]\n",
    "        for k in topics_source:\n",
    "            all_topics_source[k[0]]=k[1]\n",
    "        for k in topics_target:\n",
    "            all_topics_target[k[0]]=k[1]\n",
    "        cos_sim = dot(all_topics_source, all_topics_target)/(norm(all_topics_source)*norm(all_topics_target))\n",
    "        lda_similarity_training.append([cos_sim])\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print(counter, \"training examples processsed\")\n",
    "    counter=0\n",
    "    lda_similarity_testing=[]\n",
    "    for i in range (len(pairs_test)):\n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "\n",
    "        all_topics_source=[0]*num_topics\n",
    "        all_topics_target=[0]*num_topics\n",
    "        topics_source= lda_model[corpusLDA[ids2ind[q1]]]\n",
    "        topics_target=lda_model[corpusLDA[ids2ind[q2]]]\n",
    "        for k in topics_source:\n",
    "            all_topics_source[k[0]]=k[1]\n",
    "        for k in topics_target:\n",
    "            all_topics_target[k[0]]=k[1]\n",
    "        cos_sim = dot(all_topics_source, all_topics_target)/(norm(all_topics_source)*norm(all_topics_target))\n",
    "        lda_similarity_testing.append([cos_sim])\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print(counter, \"testing examples processsed\")\n",
    "    \n",
    "    return(np.array(lda_similarity_training),np.array(lda_similarity_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.184*\"what\" + 0.182*\"best\" + 0.087*\"learn\" + 0.057*\"book\" + 0.040*\"which\" + 0.033*\"program\" + 0.026*\"languag\" + 0.025*\"2016\" + 0.024*\"countri\" + 0.021*\"comput\"\n",
      "\n",
      "1: 0.152*\"what\" + 0.097*\"differ\" + 0.083*\"life\" + 0.077*\"between\" + 0.038*\"websit\" + 0.033*\"song\" + 0.030*\"live\" + 0.023*\"your\" + 0.022*\"real\" + 0.021*\"human\"\n",
      "\n",
      "2: 0.075*\"realli\" + 0.048*\"could\" + 0.045*\"were\" + 0.042*\"note\" + 0.039*\"help\" + 0.036*\"black\" + 0.032*\"what\" + 0.032*\"true\" + 0.030*\"govern\" + 0.024*\"1000\"\n",
      "\n",
      "3: 0.049*\"improv\" + 0.045*\"studi\" + 0.041*\"follow\" + 0.040*\"need\" + 0.037*\"english\" + 0.037*\"colleg\" + 0.036*\"mobil\" + 0.035*\"play\" + 0.033*\"just\" + 0.032*\"write\"\n",
      "\n",
      "4: 0.177*\"what\" + 0.124*\"some\" + 0.071*\"make\" + 0.065*\"good\" + 0.047*\"time\" + 0.042*\"money\" + 0.041*\"best\" + 0.029*\"onlin\" + 0.026*\"way\" + 0.024*\"that\"\n",
      "\n",
      "5: 0.195*\"with\" + 0.061*\"love\" + 0.057*\"person\" + 0.052*\"without\" + 0.035*\"what\" + 0.035*\"world\" + 0.032*\"busi\" + 0.024*\"onli\" + 0.021*\"idea\" + 0.019*\"hair\"\n",
      "\n",
      "6: 0.114*\"phone\" + 0.062*\"number\" + 0.061*\"what\" + 0.051*\"friend\" + 0.043*\"chang\" + 0.035*\"site\" + 0.035*\"read\" + 0.032*\"come\" + 0.030*\"histori\" + 0.024*\"creat\"\n",
      "\n",
      "7: 0.133*\"what\" + 0.055*\"year\" + 0.052*\"engin\" + 0.041*\"some\" + 0.037*\"interview\" + 0.035*\"through\" + 0.033*\"tip\" + 0.024*\"process\" + 0.023*\"make\" + 0.021*\"anim\"\n",
      "\n",
      "8: 0.087*\"use\" + 0.061*\"prepar\" + 0.058*\"what\" + 0.048*\"word\" + 0.045*\"lose\" + 0.040*\"start\" + 0.040*\"stop\" + 0.037*\"should\" + 0.035*\"weight\" + 0.030*\"month\"\n",
      "\n",
      "9: 0.068*\"account\" + 0.064*\"much\" + 0.050*\"more\" + 0.047*\"than\" + 0.043*\"facebook\" + 0.035*\"card\" + 0.030*\"other\" + 0.028*\"compani\" + 0.027*\"cultur\" + 0.025*\"googl\"\n",
      "\n",
      "10: 0.165*\"doe\" + 0.100*\"like\" + 0.069*\"what\" + 0.060*\"girl\" + 0.048*\"feel\" + 0.044*\"work\" + 0.042*\"have\" + 0.031*\"compar\" + 0.030*\"thi\" + 0.024*\"increas\"\n",
      "\n",
      "11: 0.163*\"which\" + 0.111*\"quora\" + 0.089*\"best\" + 0.066*\"question\" + 0.041*\"answer\" + 0.031*\"develop\" + 0.031*\"what\" + 0.025*\"india\" + 0.023*\"under\" + 0.022*\"drug\"\n",
      "\n",
      "12: 0.109*\"what\" + 0.097*\"would\" + 0.083*\"mean\" + 0.056*\"doe\" + 0.046*\"there\" + 0.036*\"game\" + 0.034*\"android\" + 0.029*\"give\" + 0.028*\"name\" + 0.026*\"that\"\n",
      "\n",
      "13: 0.126*\"peopl\" + 0.089*\"about\" + 0.044*\"think\" + 0.042*\"what\" + 0.041*\"mani\" + 0.036*\"india\" + 0.029*\"them\" + 0.028*\"instagram\" + 0.026*\"that\" + 0.024*\"messag\"\n",
      "\n",
      "14: 0.101*\"will\" + 0.061*\"becom\" + 0.058*\"trump\" + 0.057*\"what\" + 0.044*\"indian\" + 0.044*\"america\" + 0.044*\"happen\" + 0.041*\"donald\" + 0.034*\"after\" + 0.034*\"presid\"\n",
      "\n",
      "15: 0.152*\"have\" + 0.118*\"what\" + 0.069*\"movi\" + 0.063*\"ever\" + 0.040*\"most\" + 0.033*\"watch\" + 0.033*\"download\" + 0.029*\"that\" + 0.027*\"best\" + 0.027*\"thing\"\n",
      "\n",
      "16: 0.150*\"should\" + 0.088*\"what\" + 0.072*\"know\" + 0.067*\"thing\" + 0.065*\"their\" + 0.052*\"first\" + 0.049*\"some\" + 0.045*\"into\" + 0.043*\"go\" + 0.029*\"employe\"\n",
      "\n",
      "17: 0.181*\"from\" + 0.084*\"what\" + 0.065*\"they\" + 0.046*\"doe\" + 0.038*\"univers\" + 0.037*\"look\" + 0.034*\"want\" + 0.028*\"video\" + 0.027*\"major\" + 0.023*\"over\"\n",
      "\n",
      "18: 0.098*\"where\" + 0.066*\"find\" + 0.065*\"nonasciiword\" + 0.057*\"what\" + 0.052*\"woman\" + 0.042*\"free\" + 0.029*\"market\" + 0.027*\"servic\" + 0.024*\"when\" + 0.023*\"data\"\n",
      "\n",
      "19: 0.123*\"what\" + 0.114*\"your\" + 0.052*\"take\" + 0.040*\"doe\" + 0.039*\"long\" + 0.027*\"major\" + 0.021*\"do\" + 0.020*\"reason\" + 0.020*\"term\" + 0.019*\"china\"\n",
      "\n",
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "creat_features(features_LDA,'features_LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test = load_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76095, 204)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start,stop,step=2,12,1\n",
    "plot_graph(clean_text,start,stop,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"What', 'are', 'the', 'some', 'of', 'the', 'best', 'novels?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'greatest',\n",
       "  'novels',\n",
       "  'of',\n",
       "  'all',\n",
       "  'time?',\n",
       "  'Why',\n",
       "  'are',\n",
       "  'they',\n",
       "  'great?\"'],\n",
       " ['\"What', 'are', 'the', 'pictures', 'that', 'made', 'you', 'look', 'twice?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'amazing',\n",
       "  'pictures',\n",
       "  'one',\n",
       "  'has',\n",
       "  'to',\n",
       "  'see',\n",
       "  'twice',\n",
       "  'to',\n",
       "  'understand?\"'],\n",
       " ['\"Have',\n",
       "  'the',\n",
       "  'ellectoral',\n",
       "  'college',\n",
       "  'members',\n",
       "  'ever',\n",
       "  'voted',\n",
       "  'differently',\n",
       "  'then',\n",
       "  'the',\n",
       "  'popular',\n",
       "  'vote',\n",
       "  'suggested',\n",
       "  'they',\n",
       "  'should',\n",
       "  'vote?\"'],\n",
       " ['\"When',\n",
       "  'has',\n",
       "  'the',\n",
       "  'electoral',\n",
       "  'college',\n",
       "  'voted',\n",
       "  'against',\n",
       "  'the',\n",
       "  'popular',\n",
       "  'vote?\"'],\n",
       " ['\"Did', 'Ravana', 'really', 'have', '10', 'heads?\"'],\n",
       " ['\"Why', 'did', 'Ravana', 'have', '10', 'heads?\"'],\n",
       " ['\"What\\'s',\n",
       "  'a',\n",
       "  'book',\n",
       "  'that',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'helped',\n",
       "  'you',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'intellectually?\"'],\n",
       " ['\"What',\n",
       "  'books',\n",
       "  'or',\n",
       "  'magazines',\n",
       "  'should',\n",
       "  'I',\n",
       "  'read',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'English?\"'],\n",
       " ['\"Is', 'astrology', 'true?', 'Should', 'we', 'believe', 'it', 'or', 'not?\"'],\n",
       " ['\"Should',\n",
       "  'you',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'astrology',\n",
       "  'and',\n",
       "  'astrologers',\n",
       "  '(pandits)?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'lies',\n",
       "  'that',\n",
       "  'you',\n",
       "  'ever',\n",
       "  'told?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'lie',\n",
       "  'ever',\n",
       "  'told',\n",
       "  'by',\n",
       "  'any',\n",
       "  'government?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'advertise',\n",
       "  'my',\n",
       "  'YouTube',\n",
       "  'Channel',\n",
       "  'to',\n",
       "  'get',\n",
       "  'more',\n",
       "  'views?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'more',\n",
       "  'traffic',\n",
       "  'to',\n",
       "  'my',\n",
       "  'YouTube',\n",
       "  'videos?\"'],\n",
       " ['\"Is', 'time', 'travel', 'possible', 'through', 'cosmic', 'strings?\"'],\n",
       " ['\"Is', 'time', 'travel', 'already', 'possible', 'on', 'Earth?\"'],\n",
       " ['\"What', 'should', 'I', 'do', 'to', 'earn', 'money', 'online?\"'],\n",
       " ['\"How', 'can', 'I', 'earn', 'money', 'on', 'internet?\"'],\n",
       " ['\"Why', 'did', 'you', 'lose', 'your', 'virginity?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'lose', 'your', 'virginity?\"'],\n",
       " ['\"What', 'does', 'surgical', 'strike', 'mean?\"'],\n",
       " ['\"What', 'is', 'the', 'surgical', 'strike?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'apply',\n",
       "  'aloe',\n",
       "  'vera',\n",
       "  'gel',\n",
       "  'on',\n",
       "  'the',\n",
       "  'face',\n",
       "  'overnight?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'buy',\n",
       "  'the',\n",
       "  'best',\n",
       "  'aloe',\n",
       "  'vera',\n",
       "  'gel',\n",
       "  'for',\n",
       "  'face?\"'],\n",
       " ['\"Does',\n",
       "  'drinking',\n",
       "  'lemon',\n",
       "  'juice',\n",
       "  'in',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'on',\n",
       "  'an',\n",
       "  'empty',\n",
       "  'stomach',\n",
       "  'help',\n",
       "  'in',\n",
       "  'reducing',\n",
       "  'belly',\n",
       "  'fat?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'belly',\n",
       "  'fat',\n",
       "  'and',\n",
       "  'build',\n",
       "  'flat',\n",
       "  'stomach?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'thing(s)',\n",
       "  'about',\n",
       "  'raising',\n",
       "  'children',\n",
       "  'in',\n",
       "  'São',\n",
       "  'Tomé',\n",
       "  'and',\n",
       "  'Príncipe?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'thing(s)',\n",
       "  'about',\n",
       "  'raising',\n",
       "  'children',\n",
       "  'in',\n",
       "  'Nigeria?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'spank', 'someone?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'spank', 'myself?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'tell',\n",
       "  'if',\n",
       "  'someone',\n",
       "  'blocked',\n",
       "  'me',\n",
       "  'on',\n",
       "  'snapchat?\"'],\n",
       " ['\"Someone',\n",
       "  'blocked',\n",
       "  'me',\n",
       "  'on',\n",
       "  'snapchat.',\n",
       "  'How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'destroy',\n",
       "  'their',\n",
       "  'social',\n",
       "  'life',\n",
       "  'as',\n",
       "  'revenge?\"'],\n",
       " ['\"What', 'makes', 'a', 'person', 'truly', 'happy?\"'],\n",
       " ['\"What', 'truly', 'makes', 'you', 'happy?\"'],\n",
       " ['\"How',\n",
       "  'does',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'view',\n",
       "  'US-India',\n",
       "  'relations',\n",
       "  'if',\n",
       "  'she',\n",
       "  'gets',\n",
       "  'elected',\n",
       "  'as',\n",
       "  'the',\n",
       "  'President',\n",
       "  'of',\n",
       "  'the',\n",
       "  'United',\n",
       "  'States?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'foreign',\n",
       "  'policy',\n",
       "  'of',\n",
       "  'the',\n",
       "  'United',\n",
       "  'States',\n",
       "  'towards',\n",
       "  'India',\n",
       "  'be',\n",
       "  'if',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'were',\n",
       "  'elected',\n",
       "  'its',\n",
       "  'president?\"'],\n",
       " ['\"How', 'do', 'I', 'start', 'learning', 'machine', 'learning?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'get',\n",
       "  'started',\n",
       "  'with',\n",
       "  'Machine',\n",
       "  'Learning?\"'],\n",
       " ['\"Who',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'underrated',\n",
       "  'actors',\n",
       "  'in',\n",
       "  'Bollywood?\"'],\n",
       " ['\"Who',\n",
       "  'is',\n",
       "  'the',\n",
       "  'most',\n",
       "  'versatile',\n",
       "  'but',\n",
       "  'underrated',\n",
       "  'actress',\n",
       "  'in',\n",
       "  'Bollywood?\"'],\n",
       " ['\"Are',\n",
       "  'there',\n",
       "  'any',\n",
       "  'theories',\n",
       "  'on',\n",
       "  'what',\n",
       "  'caused',\n",
       "  'the',\n",
       "  'big',\n",
       "  'bang?\"'],\n",
       " ['\"What', 'caused', 'the', 'Big', 'Bang?\"'],\n",
       " ['\"Is',\n",
       "  'is',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'take',\n",
       "  'laxative',\n",
       "  'pills',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'weight?',\n",
       "  'If',\n",
       "  'so',\n",
       "  'which',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ones?\"'],\n",
       " ['\"How', 'do', 'laxative', 'teas', 'make', 'you', 'lose', 'weight?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'Hillary',\n",
       "  \"Clinton's\",\n",
       "  'diplomatic',\n",
       "  'attitude',\n",
       "  'toward',\n",
       "  'India?',\n",
       "  'If',\n",
       "  'she',\n",
       "  'becomes',\n",
       "  'president',\n",
       "  'what',\n",
       "  'India',\n",
       "  'should',\n",
       "  'expect',\n",
       "  'from',\n",
       "  'her?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'Hillary',\n",
       "  \"Clinton's\",\n",
       "  'policy',\n",
       "  'towards',\n",
       "  'India',\n",
       "  'if',\n",
       "  'she',\n",
       "  'becomes',\n",
       "  'president?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'you',\n",
       "  'want',\n",
       "  'to',\n",
       "  'do',\n",
       "  'before',\n",
       "  'you',\n",
       "  'die?\"'],\n",
       " ['\"What', 'do', 'you', 'want', 'to', 'do', 'before', 'you', 'die?\"'],\n",
       " ['\"What', 'is', 'the', 'meaning', 'of', '\"\"brisado\"\"?\"'],\n",
       " ['\"What', 'is', 'the', 'meaning', 'of', 'meaning?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'people',\n",
       "  'ask',\n",
       "  'Quora',\n",
       "  'questions',\n",
       "  'which',\n",
       "  'can',\n",
       "  'be',\n",
       "  'answered',\n",
       "  'easily',\n",
       "  'by',\n",
       "  'Google?\"'],\n",
       " ['\"Why',\n",
       "  \"don't\",\n",
       "  'Quora',\n",
       "  'people',\n",
       "  'just',\n",
       "  'look',\n",
       "  'up',\n",
       "  'the',\n",
       "  'answer',\n",
       "  'on',\n",
       "  'Google?\"'],\n",
       " ['\"How', 'I', 'can', 'I', 'logout', 'from', 'Quora?\"'],\n",
       " ['\"How', 'should', 'I', 'logout', 'from', 'qoura?\"'],\n",
       " ['\"Why',\n",
       "  'are',\n",
       "  'all',\n",
       "  'my',\n",
       "  'questions',\n",
       "  'immediately',\n",
       "  'being',\n",
       "  'marked',\n",
       "  'as',\n",
       "  'needing',\n",
       "  'improvement?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'my',\n",
       "  'dank',\n",
       "  'questions',\n",
       "  'keep',\n",
       "  'being',\n",
       "  'marked',\n",
       "  'as',\n",
       "  'need',\n",
       "  'improvement?\"'],\n",
       " ['\"What',\n",
       "  'would',\n",
       "  'be',\n",
       "  'the',\n",
       "  'next',\n",
       "  'step',\n",
       "  'by',\n",
       "  'Narendra',\n",
       "  'Modi',\n",
       "  'to',\n",
       "  'eradicate',\n",
       "  'black',\n",
       "  'money?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'Modi',\n",
       "  's',\n",
       "  'next',\n",
       "  'step',\n",
       "  'against',\n",
       "  'disproportionate',\n",
       "  'properties',\n",
       "  'and',\n",
       "  'assets?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'fat',\n",
       "  'and',\n",
       "  'gain',\n",
       "  'muscle',\n",
       "  'simultaneously?\"'],\n",
       " ['\"How', 'can', 'I', 'reduce', 'my', 'fat', 'and', 'keep', 'my', 'muscle?\"'],\n",
       " ['\"Why',\n",
       "  'are',\n",
       "  'so',\n",
       "  'many',\n",
       "  'questions',\n",
       "  'posted',\n",
       "  'to',\n",
       "  'Quora',\n",
       "  'that',\n",
       "  'are',\n",
       "  'so',\n",
       "  'easily',\n",
       "  'answered',\n",
       "  'by',\n",
       "  'using',\n",
       "  'Google?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'many',\n",
       "  'Quora',\n",
       "  'users',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'they',\n",
       "  'could',\n",
       "  'look',\n",
       "  'up',\n",
       "  'online?\"'],\n",
       " ['\"Why',\n",
       "  \"doesn't\",\n",
       "  'a',\n",
       "  'bicycle',\n",
       "  'move',\n",
       "  'backwards',\n",
       "  'when',\n",
       "  'one',\n",
       "  'pedals',\n",
       "  'back?\"'],\n",
       " ['\"How', 'do', 'I', 'ride', 'a', 'bicycle?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'a',\n",
       "  'full',\n",
       "  'array',\n",
       "  'of',\n",
       "  'colors',\n",
       "  'and',\n",
       "  'sizes',\n",
       "  'for',\n",
       "  'bridesmaids',\n",
       "  'dresses',\n",
       "  'in',\n",
       "  'Gold',\n",
       "  'Coast?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'a',\n",
       "  'vast',\n",
       "  'collection',\n",
       "  'of',\n",
       "  'bridal',\n",
       "  'dresses',\n",
       "  'in',\n",
       "  'Gold',\n",
       "  'Coast?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'think',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'is',\n",
       "  'a',\n",
       "  'serial',\n",
       "  'liar?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'many',\n",
       "  'people',\n",
       "  'call',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'a',\n",
       "  '\"\"crooked\"\"',\n",
       "  'liar?',\n",
       "  'What',\n",
       "  'did',\n",
       "  'she',\n",
       "  'lie',\n",
       "  'about?\"'],\n",
       " ['\"Can', 'you', 'give', 'me', 'some', 'advice', 'on', 'losing', 'weight?\"'],\n",
       " ['\"How', 'can', 'I', 'really', 'start', 'losing', 'weight?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'craziest',\n",
       "  'question',\n",
       "  'ever',\n",
       "  'asked',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'weirdest',\n",
       "  'questions',\n",
       "  'asked',\n",
       "  'in',\n",
       "  'Quora?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'we',\n",
       "  'know',\n",
       "  'if',\n",
       "  'we',\n",
       "  'are',\n",
       "  'living',\n",
       "  'in',\n",
       "  'a',\n",
       "  'computer',\n",
       "  'simulation',\n",
       "  'or',\n",
       "  'video',\n",
       "  'game?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'we',\n",
       "  'know',\n",
       "  'that',\n",
       "  \"we're\",\n",
       "  'not',\n",
       "  'living',\n",
       "  'in',\n",
       "  'a',\n",
       "  'computer',\n",
       "  'simulation?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'comments',\n",
       "  'on',\n",
       "  'a',\n",
       "  \"guy's\",\n",
       "  'profile',\n",
       "  'picture?\"'],\n",
       " ['\"How', 'do', 'I', 'comment', 'on', 'a', 'girls', 'picture', 'on', 'fb?\"'],\n",
       " ['\"How', 'can', 'I', 'build', 'a', 'car?\"'],\n",
       " ['\"How', 'would', 'I', 'build', 'my', 'own', 'car?\"'],\n",
       " ['\"What', 'are', 'fossil', 'fuels?\"'],\n",
       " ['\"What', 'if', 'fossil', 'fuels', 'were', 'exhausted?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'quality',\n",
       "  'assistance',\n",
       "  'in',\n",
       "  'Sydney',\n",
       "  'for',\n",
       "  'any',\n",
       "  'property',\n",
       "  'transaction?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'very',\n",
       "  'smooth',\n",
       "  'and',\n",
       "  'uncomplicated',\n",
       "  'assistance',\n",
       "  'in',\n",
       "  'Sydney',\n",
       "  'for',\n",
       "  'any',\n",
       "  'property',\n",
       "  'transaction?\"'],\n",
       " ['\"What', 'has', 'restored', 'your', 'faith', 'in', 'humanity?\"'],\n",
       " ['\"What',\n",
       "  'experience',\n",
       "  'has',\n",
       "  'most',\n",
       "  'restored',\n",
       "  'your',\n",
       "  'faith',\n",
       "  'in',\n",
       "  'humanity?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'method', 'of', 'losing', 'weight?\"'],\n",
       " ['\"How', 'should', 'I', 'loose', 'weight?\"'],\n",
       " ['\"How', 'does', 'it', 'feel', 'to', 'be', 'a', 'pornstar?\"'],\n",
       " ['\"How', 'is', 'it', 'like', 'to', 'be', 'a', 'pornstar?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'good',\n",
       "  'websites',\n",
       "  'to',\n",
       "  'download',\n",
       "  'anime',\n",
       "  '(Japanese',\n",
       "  'with',\n",
       "  'English',\n",
       "  'subs)',\n",
       "  'from?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'website',\n",
       "  'to',\n",
       "  'download',\n",
       "  'anime',\n",
       "  'series?\"'],\n",
       " ['\"What',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'if',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'were',\n",
       "  'assassinated',\n",
       "  'after',\n",
       "  'the',\n",
       "  'election?\"'],\n",
       " ['\"What',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'if',\n",
       "  'both',\n",
       "  'Trump',\n",
       "  'and',\n",
       "  'Pence',\n",
       "  'were',\n",
       "  'assassinated',\n",
       "  'before',\n",
       "  'taking',\n",
       "  'office?\"'],\n",
       " ['\"What',\n",
       "  'could',\n",
       "  'be',\n",
       "  'the',\n",
       "  'reason',\n",
       "  'behind',\n",
       "  'Arnab',\n",
       "  'Goswami',\n",
       "  'quitting',\n",
       "  'Times',\n",
       "  'Now?\"'],\n",
       " ['\"Why', 'did', 'Arnab', 'Goswami', 'quitted', 'from', 'Times', 'Now?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'lose',\n",
       "  'weight',\n",
       "  'and',\n",
       "  'reduce',\n",
       "  'my',\n",
       "  'waist',\n",
       "  'quickly?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'lose',\n",
       "  'weight',\n",
       "  'fast',\n",
       "  'by',\n",
       "  'perfect',\n",
       "  'weight',\n",
       "  'Loss',\n",
       "  'plan?\"'],\n",
       " ['\"Is', 'there', 'advertising', 'on', 'Quora?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'ways', 'to', 'advertise', 'on', 'Quora?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'sentence',\n",
       "  'examples',\n",
       "  'using',\n",
       "  'idiomatic',\n",
       "  'expressions?\"'],\n",
       " ['\"What', 'are', 'some', 'sentence', 'examples', 'using', '\"\"inherent\"\"?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'personally',\n",
       "  'react',\n",
       "  'on',\n",
       "  'Modi',\n",
       "  'government',\n",
       "  'imposing',\n",
       "  'ban',\n",
       "  'on',\n",
       "  '500/1000',\n",
       "  'rs.',\n",
       "  'Notes?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'about',\n",
       "  'ban',\n",
       "  'on',\n",
       "  'Rs.',\n",
       "  '500',\n",
       "  'and',\n",
       "  'Rs.',\n",
       "  '1000',\n",
       "  'currency',\n",
       "  'notes?\"'],\n",
       " ['\"What', 'is', 'fiscal', 'monetary', 'policy?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'fiscal',\n",
       "  'and',\n",
       "  'monetary',\n",
       "  'policy?\"'],\n",
       " ['\"India:', 'What', 'are', 'things', 'that', 'make', 'Indians', 'sad?\"'],\n",
       " ['\"What', 'things', 'make', 'Indians', 'sad?\"'],\n",
       " ['\"If',\n",
       "  'I',\n",
       "  'unfriend',\n",
       "  'someone',\n",
       "  'on',\n",
       "  'Snapchat',\n",
       "  'can',\n",
       "  'I',\n",
       "  'still',\n",
       "  'see',\n",
       "  'saved',\n",
       "  'messages',\n",
       "  'that',\n",
       "  'we',\n",
       "  'saved?\"'],\n",
       " ['\"If',\n",
       "  'I',\n",
       "  'unfriend',\n",
       "  'someone',\n",
       "  'on',\n",
       "  'Snapchat',\n",
       "  'can',\n",
       "  'they',\n",
       "  'still',\n",
       "  'see',\n",
       "  'saved',\n",
       "  'messages',\n",
       "  'that',\n",
       "  'they',\n",
       "  'saved?\"'],\n",
       " ['\"Is',\n",
       "  'India',\n",
       "  'really',\n",
       "  'progressing',\n",
       "  'under',\n",
       "  'Narendra',\n",
       "  'Modi',\n",
       "  'government?\"'],\n",
       " ['\"How', 'is', 'India', 'changing', 'under', 'Modi', 'Government?\"'],\n",
       " ['\"What', 'are', 'some', 'of', 'the', 'best', 'life', '-changing', 'books?\"'],\n",
       " ['\"Daily',\n",
       "  'Life:',\n",
       "  'What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'self-improvement',\n",
       "  'books?\"'],\n",
       " ['\"Can',\n",
       "  'you',\n",
       "  'get',\n",
       "  'pregnant',\n",
       "  'the',\n",
       "  'day',\n",
       "  'before',\n",
       "  'your',\n",
       "  'period',\n",
       "  'starts?\"'],\n",
       " ['\"Can',\n",
       "  'you',\n",
       "  'get',\n",
       "  'pregnant',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'days',\n",
       "  'before',\n",
       "  'your',\n",
       "  'period?\"'],\n",
       " ['\"How',\n",
       "  'is',\n",
       "  'the',\n",
       "  'word',\n",
       "  \"'subcontinent'\",\n",
       "  'used',\n",
       "  'in',\n",
       "  'a',\n",
       "  'sentence?\"'],\n",
       " ['\"How', 'is', 'the', 'word', \"'quibble'\", 'used', 'in', 'a', 'sentence?\"'],\n",
       " ['\"Can',\n",
       "  'the',\n",
       "  'penis',\n",
       "  'size',\n",
       "  '(length',\n",
       "  'or',\n",
       "  'girth)',\n",
       "  'be',\n",
       "  'permanently',\n",
       "  'increased?\"'],\n",
       " ['\"My',\n",
       "  'penis',\n",
       "  'size',\n",
       "  'is',\n",
       "  '2.5\"\"',\n",
       "  'when',\n",
       "  'it',\n",
       "  'is',\n",
       "  'in',\n",
       "  'normal',\n",
       "  'state',\n",
       "  '&',\n",
       "  'my',\n",
       "  'age',\n",
       "  'is',\n",
       "  '26',\n",
       "  'now.',\n",
       "  'How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'increase',\n",
       "  'the',\n",
       "  'size',\n",
       "  'of',\n",
       "  'my',\n",
       "  'penis',\n",
       "  'without',\n",
       "  'surgery?\"'],\n",
       " ['\"Why',\n",
       "  'was',\n",
       "  'George',\n",
       "  'RR',\n",
       "  'Martin',\n",
       "  'critical',\n",
       "  'of',\n",
       "  'JK',\n",
       "  'Rowling',\n",
       "  'after',\n",
       "  'losing',\n",
       "  'the',\n",
       "  'Hugo',\n",
       "  'award?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'you',\n",
       "  'finish',\n",
       "  'the',\n",
       "  'story',\n",
       "  'if',\n",
       "  'you',\n",
       "  'were',\n",
       "  'George',\n",
       "  'RR',\n",
       "  'Martin?\"'],\n",
       " ['\"How', 'do', 'I', 'earn', '500', 'dollar', 'per', 'day', 'online?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'earn',\n",
       "  '3000',\n",
       "  'dollars',\n",
       "  'online',\n",
       "  'in',\n",
       "  '15',\n",
       "  'days?',\n",
       "  'Without',\n",
       "  'any',\n",
       "  'investment.\"'],\n",
       " ['\"Which',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'of',\n",
       "  'all',\n",
       "  'time?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'that',\n",
       "  'will',\n",
       "  'keep',\n",
       "  'you',\n",
       "  'up',\n",
       "  'at',\n",
       "  'night?\"'],\n",
       " ['\"How', 'should', 'I', 'be', 'a', 'millionaire?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'ways', 'to', 'become', 'a', 'millionaire?\"'],\n",
       " ['\"Does', 'bleach', 'kill', 'fungus?\"'],\n",
       " ['\"Will', 'bleach', 'kill', 'a', 'spider?\"'],\n",
       " ['\"How', 'do', 'I', 'add', 'an', 'image', 'to', 'my', 'question?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'add',\n",
       "  'an',\n",
       "  'image',\n",
       "  'to',\n",
       "  'a',\n",
       "  'question',\n",
       "  'or',\n",
       "  'a',\n",
       "  'post',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"What',\n",
       "  'does',\n",
       "  'it',\n",
       "  'mean',\n",
       "  'when',\n",
       "  'you',\n",
       "  'press',\n",
       "  'on',\n",
       "  \"someone's\",\n",
       "  'name',\n",
       "  'on',\n",
       "  'instagram',\n",
       "  'and',\n",
       "  'it',\n",
       "  'says',\n",
       "  'user',\n",
       "  'is',\n",
       "  'not',\n",
       "  'found?\"'],\n",
       " ['\"How', 'do', 'I', 'recover', 'a', 'hacked', 'instagram?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'memorize',\n",
       "  'or',\n",
       "  'remember',\n",
       "  'what',\n",
       "  'you',\n",
       "  'study/read?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'retain',\n",
       "  'what',\n",
       "  'l',\n",
       "  'have',\n",
       "  'read?\"'],\n",
       " ['\"What', 'is', 'the', 'biggest', 'state', 'in', 'the', 'world?\"'],\n",
       " ['\"Why', 'did', 'Germany', 'lose', 'WWII?\"'],\n",
       " ['\"Does', 'everything', 'happen', 'for', 'a', 'reason?\"'],\n",
       " ['\"Do', 'you', 'believe', 'everything', 'happens', 'for', 'a', 'reason?\"'],\n",
       " ['\"Why', 'does', 'yellow', 'mustard', 'relieve', 'a', 'burn?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'Dijon',\n",
       "  'mustard',\n",
       "  'and',\n",
       "  'other',\n",
       "  'mustards?\"'],\n",
       " ['\"How', 'do', 'I', 'create', 'a', 'proxy', 'site', 'online?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'free', 'proxy', 'servers?\"'],\n",
       " ['\"What',\n",
       "  'should',\n",
       "  'I',\n",
       "  'follow',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'myself',\n",
       "  'fit',\n",
       "  'without',\n",
       "  'going',\n",
       "  'to',\n",
       "  'gym?\"'],\n",
       " ['\"How',\n",
       "  'to',\n",
       "  'build',\n",
       "  'a',\n",
       "  'fit',\n",
       "  'and',\n",
       "  'strong',\n",
       "  'body',\n",
       "  'without',\n",
       "  'going',\n",
       "  'to',\n",
       "  'the',\n",
       "  'gym?\"'],\n",
       " ['\"Is', 'it', 'safe', 'to', 'give', 'your', 'puppy', 'baby', 'aspirin?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'give',\n",
       "  'my',\n",
       "  'dog',\n",
       "  'a',\n",
       "  'baby',\n",
       "  'aspirin',\n",
       "  'and',\n",
       "  'how',\n",
       "  'often',\n",
       "  'should',\n",
       "  'I',\n",
       "  'give',\n",
       "  'it',\n",
       "  'to',\n",
       "  'him?\"'],\n",
       " ['\"How', 'has', 'india', 'changed', 'under', 'Narendra', 'Modi?\"'],\n",
       " ['\"Can',\n",
       "  'a',\n",
       "  'person',\n",
       "  'increase',\n",
       "  'his/her',\n",
       "  'height',\n",
       "  'even',\n",
       "  'after',\n",
       "  '20?\"'],\n",
       " ['\"Are',\n",
       "  'there',\n",
       "  'any',\n",
       "  'chances',\n",
       "  'of',\n",
       "  'growing',\n",
       "  'height',\n",
       "  'at',\n",
       "  'the',\n",
       "  'age',\n",
       "  'of',\n",
       "  '20?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'natural',\n",
       "  'way',\n",
       "  'to',\n",
       "  'increase',\n",
       "  'height',\n",
       "  'at',\n",
       "  'age',\n",
       "  'of',\n",
       "  '20?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'way',\n",
       "  'to',\n",
       "  'increase',\n",
       "  'height',\n",
       "  'after',\n",
       "  'age',\n",
       "  'of',\n",
       "  '20?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'inprove',\n",
       "  'my',\n",
       "  'English',\n",
       "  'writing',\n",
       "  'ability?\"'],\n",
       " ['\"How', 'do', 'I', 'enhance', 'my', 'English', 'writing', 'skills?\"'],\n",
       " ['\"Why',\n",
       "  \"don't\",\n",
       "  'Quora',\n",
       "  'users',\n",
       "  'simply',\n",
       "  'use',\n",
       "  'Google',\n",
       "  'first',\n",
       "  'before',\n",
       "  'asking',\n",
       "  'a',\n",
       "  'question?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'use',\n",
       "  'Quora',\n",
       "  'when',\n",
       "  'a',\n",
       "  'dictionary',\n",
       "  'or',\n",
       "  'Google',\n",
       "  'could',\n",
       "  'be',\n",
       "  'used',\n",
       "  'instead?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'delete', 'a', 'virus?\"'],\n",
       " ['\"How', 'do', 'i', 'delete', 'an', 'computer', 'virus?\"'],\n",
       " ['\"How', 'can', 'I', 'publish', 'my', 'book', 'on', 'my', 'own?\"'],\n",
       " ['\"How', 'can', 'one', 'publish', 'a', 'book?\"'],\n",
       " ['\"How', 'can', 'I', 'report', 'a', 'bug', 'to', 'yahoo?\"'],\n",
       " ['\"How', 'can', 'I', 'report', 'a', 'bug', 'on', 'Facebook?\"'],\n",
       " ['\"What', 'can', 'I', 'do', 'to', 'save', 'the', 'world?\"'],\n",
       " ['\"Can', 'I', 'save', 'the', 'world?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'stop',\n",
       "  'a',\n",
       "  'Rottweiler/Pitbull',\n",
       "  'mix',\n",
       "  'from',\n",
       "  'humping',\n",
       "  'your',\n",
       "  'furniture?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'my',\n",
       "  'Miniature',\n",
       "  'Pinscher/Chihuahua',\n",
       "  'mix',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'humping',\n",
       "  'my',\n",
       "  'furniture?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'easiest',\n",
       "  'and',\n",
       "  'painless',\n",
       "  'way',\n",
       "  'to',\n",
       "  'commit',\n",
       "  'suicide?\"'],\n",
       " ['\"What', 'would', 'be', 'a', 'cool', 'way', 'to', 'commit', 'suicide?\"'],\n",
       " ['\"Can', 'I', 'block', 'a', 'topic', 'on', 'Quora?\"'],\n",
       " ['\"Can',\n",
       "  'you',\n",
       "  'block',\n",
       "  'a',\n",
       "  'topic',\n",
       "  'on',\n",
       "  'Quora',\n",
       "  'that',\n",
       "  'you',\n",
       "  'are',\n",
       "  'tired',\n",
       "  'of',\n",
       "  'reading',\n",
       "  'content',\n",
       "  'about?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'thing(s)',\n",
       "  'about',\n",
       "  'raising',\n",
       "  'children',\n",
       "  'in',\n",
       "  'Palestine?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'thing(s)',\n",
       "  'about',\n",
       "  'raising',\n",
       "  'children',\n",
       "  'in',\n",
       "  'Australia?\"'],\n",
       " ['\"Has',\n",
       "  'Ancient',\n",
       "  'History',\n",
       "  'been',\n",
       "  'scientifically',\n",
       "  'tested?',\n",
       "  'Is',\n",
       "  'it',\n",
       "  'all',\n",
       "  'real?',\n",
       "  'Did',\n",
       "  'it',\n",
       "  'happen',\n",
       "  'differently',\n",
       "  'than',\n",
       "  'we',\n",
       "  'were',\n",
       "  'told',\n",
       "  'it',\n",
       "  'did?',\n",
       "  'Did',\n",
       "  'it',\n",
       "  'even',\n",
       "  'happen',\n",
       "  'at',\n",
       "  'all?\"'],\n",
       " ['\"Has', 'Ancient', 'Japan', 'been', 'scientifically', 'tested?\"'],\n",
       " ['\"Does',\n",
       "  'a',\n",
       "  'superfluid',\n",
       "  'dark',\n",
       "  'matter',\n",
       "  'which',\n",
       "  'ripples',\n",
       "  'when',\n",
       "  'Galaxy',\n",
       "  'clusters',\n",
       "  'collide',\n",
       "  'and',\n",
       "  'waves',\n",
       "  'in',\n",
       "  'a',\n",
       "  'double',\n",
       "  'slit',\n",
       "  'experiment',\n",
       "  'relate',\n",
       "  'GR',\n",
       "  'and',\n",
       "  'QM?\"'],\n",
       " ['\"Does',\n",
       "  'dark',\n",
       "  'matter',\n",
       "  'ripple',\n",
       "  'when',\n",
       "  'galaxy',\n",
       "  'clusters',\n",
       "  'collide',\n",
       "  'and',\n",
       "  'wave',\n",
       "  'in',\n",
       "  'a',\n",
       "  'double',\n",
       "  'slit',\n",
       "  'experiment?\"'],\n",
       " ['\"What',\n",
       "  'can',\n",
       "  'I',\n",
       "  'do',\n",
       "  'to',\n",
       "  'earn',\n",
       "  'money',\n",
       "  'lot',\n",
       "  'without',\n",
       "  'working',\n",
       "  'hard?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'earn',\n",
       "  'money',\n",
       "  'in',\n",
       "  '2016',\n",
       "  'without',\n",
       "  'investment?\"'],\n",
       " ['\"Why',\n",
       "  'does',\n",
       "  'Quora',\n",
       "  'fail',\n",
       "  'to',\n",
       "  'recognize',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'success?\"'],\n",
       " ['\"Is', 'Quora', 'biased', 'against', 'Donald', 'Trump?\"'],\n",
       " ['\"What', 'does', 'blind', 'people', 'see', 'in', 'their', 'dreams?\"'],\n",
       " ['\"Do',\n",
       "  'people',\n",
       "  'who',\n",
       "  'are',\n",
       "  'blind',\n",
       "  'experience',\n",
       "  'the',\n",
       "  'same',\n",
       "  'dreams',\n",
       "  'as',\n",
       "  'those',\n",
       "  'who',\n",
       "  \"aren't\",\n",
       "  'blind?\"'],\n",
       " ['\"How', 'do', 'I', 'handle', 'exam', 'stress?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'cope',\n",
       "  'with',\n",
       "  'test-taking',\n",
       "  'anxiety?\"'],\n",
       " ['\"What', 'is', 'it', 'like', 'to', 'live', 'without', 'depression?\"'],\n",
       " ['\"How', 'do', 'I', 'become', 'mentally', 'strong?\"'],\n",
       " ['\"What', 'do', 'Chinese', 'people', 'think', 'about', 'Indians?\"'],\n",
       " ['\"What', 'do', 'the', 'Chinese', 'people', 'think', 'about', 'India?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Rhode',\n",
       "  'Island?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'North',\n",
       "  'Dakota?\"'],\n",
       " ['\"At',\n",
       "  'what',\n",
       "  'age',\n",
       "  'does',\n",
       "  'the',\n",
       "  'human',\n",
       "  'penis',\n",
       "  'reach',\n",
       "  'its',\n",
       "  'maximum',\n",
       "  'size',\n",
       "  'and',\n",
       "  'its',\n",
       "  'length',\n",
       "  'and',\n",
       "  'girth',\n",
       "  'stop',\n",
       "  'increasing?\"'],\n",
       " ['\"At',\n",
       "  'what',\n",
       "  'age',\n",
       "  'does',\n",
       "  'the',\n",
       "  'size',\n",
       "  'of',\n",
       "  'breasts',\n",
       "  'stop',\n",
       "  'increasing?\"'],\n",
       " ['\"Wil', 'there', 'be', 'a', 'war', 'between', 'India', 'and', 'Pakistan?\"'],\n",
       " ['\"Will',\n",
       "  'there',\n",
       "  'be',\n",
       "  'another',\n",
       "  'war',\n",
       "  'between',\n",
       "  'Pakistan',\n",
       "  'and',\n",
       "  'India?\"'],\n",
       " ['\"How', 'do', 'you', 'get', 'rid', 'of', 'a', 'addiction?\"'],\n",
       " ['\"How', 'should', 'I', 'get', 'rid', 'of', 'addiction?\"'],\n",
       " ['\"Can', 'Donald', 'Trump', 'keep', 'his', 'campaign', 'promises?\"'],\n",
       " ['\"Can', 'Donald', 'Trump', 'achieve', 'his', 'campaign', 'proposals?\"'],\n",
       " ['\"What', 'are', 'your', 'best', 'hangover', 'hacks?\"'],\n",
       " ['\"What', 'is', 'your', 'personal', 'hangover', 'cure?\"'],\n",
       " ['\"How', 'will', 'I', 'improve', 'my', 'spoken', 'English?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'ways', 'to', 'improve', 'English?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'good',\n",
       "  'gifts',\n",
       "  'for',\n",
       "  'a',\n",
       "  'foreign',\n",
       "  'visitor',\n",
       "  'to',\n",
       "  'bring',\n",
       "  'when',\n",
       "  \"they're\",\n",
       "  'invited',\n",
       "  'to',\n",
       "  \"someone's\",\n",
       "  'home',\n",
       "  'in',\n",
       "  'Chad',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'good',\n",
       "  'gifts',\n",
       "  'for',\n",
       "  'a',\n",
       "  'foreign',\n",
       "  'visitor',\n",
       "  'to',\n",
       "  'bring',\n",
       "  'when',\n",
       "  \"they're\",\n",
       "  'invited',\n",
       "  'to',\n",
       "  \"someone's\",\n",
       "  'home',\n",
       "  'in',\n",
       "  'Pakistan',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time?\"'],\n",
       " ['\"Does', 'masturbation', 'reduces', 'memory?\"'],\n",
       " ['\"Does', 'masturbation', 'cause', 'loss', 'of', 'memory?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'importance',\n",
       "  'of',\n",
       "  'education',\n",
       "  'to',\n",
       "  'the',\n",
       "  'United',\n",
       "  'States',\n",
       "  'and',\n",
       "  'how',\n",
       "  'are',\n",
       "  'their',\n",
       "  'views',\n",
       "  'of',\n",
       "  'education',\n",
       "  'different',\n",
       "  'from',\n",
       "  'Denmark\\'s?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'importance',\n",
       "  'of',\n",
       "  'education',\n",
       "  'to',\n",
       "  'the',\n",
       "  'United',\n",
       "  'States',\n",
       "  'and',\n",
       "  'how',\n",
       "  'are',\n",
       "  'their',\n",
       "  'views',\n",
       "  'of',\n",
       "  'education',\n",
       "  'different',\n",
       "  'from',\n",
       "  'Norway\\'s?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'Vector',\n",
       "  'Group?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'DSP',\n",
       "  'Group?\"'],\n",
       " ['\"How', 'can', 'I', 'land', 'a', 'job', 'at', 'Microsoft?\"'],\n",
       " ['\"How', 'can', 'I', 'get', 'a', 'job', 'in', 'Microsoft?\"'],\n",
       " ['\"Who', 'is', 'God', 'and', 'where', 'is', 'he?\"'],\n",
       " ['\"Where', 'is', 'God?\"'],\n",
       " ['\"Why', 'do', 'we', 'need', 'friends?\"'],\n",
       " ['\"Do', 'I', 'need', 'friends?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'selection',\n",
       "  'process',\n",
       "  'in',\n",
       "  'cricket',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Ranji',\n",
       "  'team?\"'],\n",
       " ['\"Can', 'anybody', 'play', 'for', 'any', 'ranji', 'team?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'career',\n",
       "  'options',\n",
       "  'after',\n",
       "  'electrical',\n",
       "  'and',\n",
       "  'electronics',\n",
       "  'engineering?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'career',\n",
       "  'options',\n",
       "  'after',\n",
       "  'B.tech',\n",
       "  'in',\n",
       "  'Electrical',\n",
       "  'engineering?\"'],\n",
       " ['\"Who',\n",
       "  'are',\n",
       "  'some',\n",
       "  'lesser',\n",
       "  'known',\n",
       "  'important',\n",
       "  'historical',\n",
       "  'figures',\n",
       "  'of',\n",
       "  'Jamaica',\n",
       "  'and',\n",
       "  'what',\n",
       "  'should',\n",
       "  'people',\n",
       "  'know',\n",
       "  'about',\n",
       "  'them?\"'],\n",
       " ['\"Who',\n",
       "  'are',\n",
       "  'some',\n",
       "  'lesser',\n",
       "  'known',\n",
       "  'important',\n",
       "  'historical',\n",
       "  'figures',\n",
       "  'of',\n",
       "  'India',\n",
       "  'and',\n",
       "  'what',\n",
       "  'should',\n",
       "  'people',\n",
       "  'know',\n",
       "  'about',\n",
       "  'them?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'lose',\n",
       "  'fat',\n",
       "  'without',\n",
       "  'doing',\n",
       "  'any',\n",
       "  'aerobic',\n",
       "  'physical',\n",
       "  'activity?\"'],\n",
       " ['\"How', 'can', 'you', 'lose', 'weight', 'without', 'doing', 'exercises?\"'],\n",
       " ['\"Have', 'you', 'ever', 'seen', 'ghost', 'in', 'your', 'real', 'life?\"'],\n",
       " ['\"Have',\n",
       "  'you',\n",
       "  'ever',\n",
       "  'seen',\n",
       "  'or',\n",
       "  'experienced',\n",
       "  'a',\n",
       "  'ghost',\n",
       "  'in',\n",
       "  '\"\"India\"\"?\"'],\n",
       " ['\"How', 'can', 'I', 'deal', 'with', 'hypocrites?\"'],\n",
       " ['\"How', 'do', 'I', 'deal', 'with', 'a', 'hypocrite?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'similarities',\n",
       "  'between',\n",
       "  'India',\n",
       "  'and',\n",
       "  'Pakistan?\"'],\n",
       " ['\"Who', 'is', \"Pakistan's\", 'biggest', 'enemy?\"'],\n",
       " ['\"How', 'can', 'I', 'see', 'who', 'viewed', 'my', 'Instagram', 'video?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'l',\n",
       "  'see',\n",
       "  'who',\n",
       "  'viewed',\n",
       "  'my',\n",
       "  'videos',\n",
       "  'on',\n",
       "  'Instagram?\"'],\n",
       " ['\"How', 'does', 'it', 'feel', 'to', 'be', 'pepper', 'sprayed?\"'],\n",
       " ['\"What', 'does', 'it', 'feel', 'like', 'to', 'be', 'pepper', 'sprayed?\"'],\n",
       " ['\"Which',\n",
       "  'building',\n",
       "  'has',\n",
       "  'the',\n",
       "  'best',\n",
       "  'architecture',\n",
       "  'in',\n",
       "  'Brazil?\"'],\n",
       " ['\"Which', 'building', 'has', 'the', 'best', 'architecture', 'in', 'China?\"'],\n",
       " ['\"Would', 'Modi', '\"\"dare\"\"', 'to', 'abolish', 'the', 'reservation?\"'],\n",
       " ['\"Will', 'PM', 'Modi', 'abolish', 'caste', 'based', 'reservations?\"'],\n",
       " ['\"What\\'s',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'mistake',\n",
       "  'you',\n",
       "  'have',\n",
       "  'made',\n",
       "  'in',\n",
       "  'your',\n",
       "  'life?\"'],\n",
       " ['\"Which',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'mistakes',\n",
       "  'you',\n",
       "  'have',\n",
       "  'made',\n",
       "  'in',\n",
       "  'your',\n",
       "  'love',\n",
       "  'life?\"'],\n",
       " ['\"Which', 'comment', 'do', 'girls', 'like', 'the', 'most?\"'],\n",
       " ['\"What', 'EU', 'country', 'has', 'the', 'most', 'Asian', 'girls?\"'],\n",
       " ['\"How', 'do', 'you', 'commit', 'suicide?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'and',\n",
       "  'surest',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'commit',\n",
       "  'suicide?\"'],\n",
       " ['\"What',\n",
       "  'can',\n",
       "  'I',\n",
       "  'do',\n",
       "  'to',\n",
       "  'clean',\n",
       "  'out',\n",
       "  'my',\n",
       "  'system',\n",
       "  'from',\n",
       "  'meth?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'meth', 'out', 'of', 'my', 'system', 'faster?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'on',\n",
       "  'Quora',\n",
       "  'that',\n",
       "  'could',\n",
       "  'easily',\n",
       "  'be',\n",
       "  'answered',\n",
       "  'by',\n",
       "  'using',\n",
       "  'a',\n",
       "  'search',\n",
       "  'engine?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'so',\n",
       "  'many',\n",
       "  'people',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'on',\n",
       "  'Quora',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'easily',\n",
       "  'answered',\n",
       "  'by',\n",
       "  'any',\n",
       "  'number',\n",
       "  'of',\n",
       "  'legitimate',\n",
       "  'sources',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Web?',\n",
       "  'Have',\n",
       "  'they',\n",
       "  'not',\n",
       "  'heard',\n",
       "  'of',\n",
       "  'Google',\n",
       "  'or',\n",
       "  'Bing?\"'],\n",
       " ['\"What', 'is', 'wrong', 'with', 'Indian', 'Education', 'System?\"'],\n",
       " ['\"Why', 'is', 'the', 'Indian', 'education', 'system', 'worthless?\"'],\n",
       " ['\"How', 'do', 'you', 'make', 'purple', 'food', 'coloring?\"'],\n",
       " ['\"How', 'do', 'you', 'mix', 'food', 'coloring', 'to', 'make', 'black?\"'],\n",
       " ['\"Is', 'a', 'law', 'degree', 'valuable?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'a',\n",
       "  'good',\n",
       "  'idea',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'law',\n",
       "  'school',\n",
       "  'if',\n",
       "  'you',\n",
       "  \"don't\",\n",
       "  'want',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'lawyer?\"'],\n",
       " ['\"How', 'does', 'one', 'excel', 'at', 'maths?\"'],\n",
       " ['\"How', 'can', 'I', 'excel', 'at', 'math?\"'],\n",
       " ['\"Why', 'do', 'Quorans', 'appear', 'to', 'favor', 'Hillary', 'Clinton?\"'],\n",
       " ['\"Is', 'Quora', 'biased', 'towards', 'Hillary', 'Clinton?\"'],\n",
       " ['\"Which', 'is', 'your', 'favourite', 'Tv', 'serial?\"'],\n",
       " ['\"What\\'s', 'your', 'favourite', 'TVprogram?\"'],\n",
       " ['\"Is', 'there', 'a', 'particular', 'way', 'physics', 'should', 'be?\"'],\n",
       " ['\"What', 'is', 'the', 'answer', 'to', 'this', 'MAT', 'question?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'institute',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'digital',\n",
       "  'marketing',\n",
       "  '(job',\n",
       "  'oriented)',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'digital',\n",
       "  'marketing',\n",
       "  'course',\n",
       "  'available',\n",
       "  'online',\n",
       "  'and',\n",
       "  'offline',\n",
       "  'in',\n",
       "  'India',\n",
       "  'and',\n",
       "  'Why?\"'],\n",
       " ['\"Are', 'downvotes', 'anonymous', 'on', 'Quora?\"'],\n",
       " ['\"How', 'do', 'you', 'vote', 'anonymously', 'on', 'Quora?\"'],\n",
       " ['\"Psychology',\n",
       "  'of',\n",
       "  'Everyday',\n",
       "  'Life:',\n",
       "  'What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'greatest',\n",
       "  'examples',\n",
       "  'of',\n",
       "  'presence',\n",
       "  'of',\n",
       "  'mind?\"'],\n",
       " ['\"What', 'are', 'the', 'examples', 'of', 'a', 'presence', 'of', 'mind?\"'],\n",
       " ['\"What', 'are', 'the', 'hardest', 'puzzles', 'asked', 'in', 'interview?\"'],\n",
       " ['\"What', 'are', 'the', 'standard', 'puzzles', 'asked', 'in', 'interviews?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'views',\n",
       "  'on',\n",
       "  'the',\n",
       "  'decision',\n",
       "  'of',\n",
       "  'Narendra',\n",
       "  'Modi',\n",
       "  'to',\n",
       "  'discontinue',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'currency',\n",
       "  'notes?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'views',\n",
       "  'on',\n",
       "  'PM',\n",
       "  \"Modi's\",\n",
       "  'decision',\n",
       "  'on',\n",
       "  'discontinuing',\n",
       "  '500',\n",
       "  '&',\n",
       "  '1000',\n",
       "  'rs',\n",
       "  'notes?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'learn', 'German', 'language?\"'],\n",
       " ['\"How', 'can', 'I', 'learn', 'German?\"'],\n",
       " ['\"Why',\n",
       "  \"don't\",\n",
       "  'Indian',\n",
       "  'government',\n",
       "  'promote',\n",
       "  'one',\n",
       "  'child',\n",
       "  'policy',\n",
       "  'like',\n",
       "  'China',\n",
       "  'did',\n",
       "  'to',\n",
       "  'control',\n",
       "  'the',\n",
       "  'population?\"'],\n",
       " ['\"Should', 'India', 'have', 'one', 'child', 'policy', 'or', 'not?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'hack',\n",
       "  'someone',\n",
       "  \"else's\",\n",
       "  'WhatsApp',\n",
       "  'account',\n",
       "  'from',\n",
       "  'a',\n",
       "  'different',\n",
       "  'place?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'open', 'a', 'dmat', 'account?\"'],\n",
       " ['\"Will', 'I', 'improve', 'my', 'memory', 'power?\"'],\n",
       " ['\"How', 'do', 'I', 'my', 'increase', 'memory', 'power?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'inpatient',\n",
       "  'drug',\n",
       "  'and',\n",
       "  'alcohol',\n",
       "  'rehab',\n",
       "  'center',\n",
       "  'near',\n",
       "  'Washington',\n",
       "  'County',\n",
       "  'FL?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'inpatient',\n",
       "  'drug',\n",
       "  'and',\n",
       "  'alcohol',\n",
       "  'rehab',\n",
       "  'center',\n",
       "  'near',\n",
       "  'Jefferson',\n",
       "  'County',\n",
       "  'FL?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'best',\n",
       "  'furniture',\n",
       "  'removal',\n",
       "  'or',\n",
       "  'storage',\n",
       "  'services',\n",
       "  'in',\n",
       "  'Central',\n",
       "  'Coast?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'most',\n",
       "  'reliable',\n",
       "  'moving',\n",
       "  'services',\n",
       "  'in',\n",
       "  'Central',\n",
       "  'Coast?\"'],\n",
       " ['\"What',\n",
       "  'type',\n",
       "  'of',\n",
       "  'government',\n",
       "  'does',\n",
       "  'Guatemala',\n",
       "  'have?',\n",
       "  'How',\n",
       "  'does',\n",
       "  'it',\n",
       "  'compare',\n",
       "  'to',\n",
       "  'the',\n",
       "  'one',\n",
       "  'in',\n",
       "  'Ireland?\"'],\n",
       " ['\"What',\n",
       "  'type',\n",
       "  'of',\n",
       "  'government',\n",
       "  'does',\n",
       "  'Guatemala',\n",
       "  'have?',\n",
       "  'How',\n",
       "  'does',\n",
       "  'it',\n",
       "  'compare',\n",
       "  'to',\n",
       "  'those',\n",
       "  'in',\n",
       "  'other',\n",
       "  'countries?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'miscellaneous',\n",
       "  'in',\n",
       "  'an',\n",
       "  'Android',\n",
       "  \"phone's\",\n",
       "  'storage?\"'],\n",
       " ['\"I',\n",
       "  'deleted',\n",
       "  'a',\n",
       "  'miscellaneous',\n",
       "  'file',\n",
       "  'from',\n",
       "  'my',\n",
       "  'Android',\n",
       "  'phone',\n",
       "  'Motorola',\n",
       "  'G2.',\n",
       "  'I',\n",
       "  'lost',\n",
       "  'my',\n",
       "  'personal',\n",
       "  'data.',\n",
       "  'I',\n",
       "  'want',\n",
       "  'them',\n",
       "  'back.',\n",
       "  'Can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'them',\n",
       "  'back?',\n",
       "  'What',\n",
       "  'should',\n",
       "  'I',\n",
       "  'do?\"'],\n",
       " ['\"What', 'will', 'happen', 'after', 'I', 'die?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'happen',\n",
       "  'after',\n",
       "  'we',\n",
       "  'die?',\n",
       "  'Does',\n",
       "  'nothing',\n",
       "  'happen?\"'],\n",
       " ['\"Does',\n",
       "  'charging',\n",
       "  'my',\n",
       "  'iPhone',\n",
       "  '6',\n",
       "  'overnight',\n",
       "  'destroy',\n",
       "  'my',\n",
       "  'battery?\"'],\n",
       " ['\"How',\n",
       "  'long',\n",
       "  'does',\n",
       "  'it',\n",
       "  'take',\n",
       "  'for',\n",
       "  'an',\n",
       "  'iPhone',\n",
       "  '6',\n",
       "  'Plus',\n",
       "  'to',\n",
       "  'charge?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'would',\n",
       "  'it',\n",
       "  'cost',\n",
       "  'to',\n",
       "  'build',\n",
       "  'a',\n",
       "  'website',\n",
       "  'like',\n",
       "  'Kazansummit.com?\"'],\n",
       " ['\"How', 'much', 'does', 'it', 'cost', 'to', 'build', 'a', 'website?\"'],\n",
       " ['\"How', 'can', 'I', 'realistically', 'make', 'money', 'online?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'various',\n",
       "  'ways',\n",
       "  'through',\n",
       "  'which',\n",
       "  'one',\n",
       "  'can',\n",
       "  'earn',\n",
       "  'money',\n",
       "  'online?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'we',\n",
       "  'need',\n",
       "  'to',\n",
       "  'use',\n",
       "  'Quora',\n",
       "  'when',\n",
       "  'we',\n",
       "  'have',\n",
       "  'Google',\n",
       "  'to',\n",
       "  'search',\n",
       "  'for',\n",
       "  'answers?\"'],\n",
       " ['\"Why',\n",
       "  'use',\n",
       "  'Quora',\n",
       "  'when',\n",
       "  'Google',\n",
       "  'answers',\n",
       "  'almost',\n",
       "  'everything?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'do',\n",
       "  'with',\n",
       "  'your',\n",
       "  'old',\n",
       "  'computer',\n",
       "  'once',\n",
       "  'you',\n",
       "  'buy',\n",
       "  'a',\n",
       "  'new',\n",
       "  'one?\"'],\n",
       " ['\"What', 'did', 'you', 'do', 'with', 'your', 'old', 'desktop', 'computer?\"'],\n",
       " ['\"I',\n",
       "  'am',\n",
       "  'in',\n",
       "  'first',\n",
       "  'year.',\n",
       "  'In',\n",
       "  'which',\n",
       "  'direction',\n",
       "  'should',\n",
       "  'I',\n",
       "  'start',\n",
       "  'my',\n",
       "  'preparation',\n",
       "  'to',\n",
       "  'clear',\n",
       "  'for',\n",
       "  'IAS',\n",
       "  'exam?\"'],\n",
       " ['\"How',\n",
       "  'should',\n",
       "  'I',\n",
       "  'start',\n",
       "  'the',\n",
       "  'preparation',\n",
       "  'of',\n",
       "  'IAS',\n",
       "  'exam',\n",
       "  'from',\n",
       "  'my',\n",
       "  'graduation',\n",
       "  'level?\"'],\n",
       " ['\"What',\n",
       "  'coding',\n",
       "  'language',\n",
       "  'should',\n",
       "  'I',\n",
       "  'learn',\n",
       "  'first',\n",
       "  'for',\n",
       "  'making',\n",
       "  'games?\"'],\n",
       " ['\"What',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'should',\n",
       "  'I',\n",
       "  'learn',\n",
       "  'if',\n",
       "  'I',\n",
       "  'want',\n",
       "  'to',\n",
       "  'create',\n",
       "  'games?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'discover',\n",
       "  'what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'minimum',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'sleep',\n",
       "  'I',\n",
       "  'need',\n",
       "  'to',\n",
       "  'function',\n",
       "  'well?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'absolute',\n",
       "  'bare',\n",
       "  'minimum',\n",
       "  'amount',\n",
       "  'of',\n",
       "  'sleep',\n",
       "  'you',\n",
       "  'need',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'stay',\n",
       "  'healthy?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'transfer',\n",
       "  'files',\n",
       "  'from',\n",
       "  'my',\n",
       "  'phone',\n",
       "  'to',\n",
       "  'PC',\n",
       "  'without',\n",
       "  'using',\n",
       "  'a',\n",
       "  'USB',\n",
       "  'cable?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'transfer',\n",
       "  'files',\n",
       "  'from',\n",
       "  'my',\n",
       "  'Android',\n",
       "  'phone',\n",
       "  'to',\n",
       "  'tablet',\n",
       "  'without',\n",
       "  'a',\n",
       "  'USB',\n",
       "  'cable?\"'],\n",
       " ['\"What', 'things', 'make', 'you', 'sad', 'about', 'people', 'in', 'India?\"'],\n",
       " ['\"Should', 'people', 'over', '95', 'not', 'be', 'allowed', 'to', 'vote?\"'],\n",
       " ['\"Should', 'people', 'over', '88', 'not', 'be', 'allowed', 'to', 'vote?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'book',\n",
       "  'for',\n",
       "  'beginners',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'java?\"'],\n",
       " ['\"Which', 'book', 'to', 'buy', 'to', 'learn', 'java?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'all',\n",
       "  'the',\n",
       "  'job',\n",
       "  'levels',\n",
       "  'in',\n",
       "  \"Amazon's\",\n",
       "  'technical',\n",
       "  'career',\n",
       "  'track?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'job',\n",
       "  'levels',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Apple',\n",
       "  'technical',\n",
       "  'career',\n",
       "  'track?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'difference',\n",
       "  'between',\n",
       "  '4g',\n",
       "  'LTE',\n",
       "  'and',\n",
       "  'VoLTE?',\n",
       "  'Which',\n",
       "  'phone',\n",
       "  'should',\n",
       "  'I',\n",
       "  'buy',\n",
       "  'one',\n",
       "  'with',\n",
       "  'VoLTE',\n",
       "  'or',\n",
       "  'only',\n",
       "  '4G.\"'],\n",
       " ['\"What', 'is', 'the', 'difference', 'between', 'LTE', 'and', 'VOLTE?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'find',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'Instagram',\n",
       "  'accounts',\n",
       "  'with',\n",
       "  'more',\n",
       "  'than',\n",
       "  '10000',\n",
       "  'followers?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'free', 'Instagram', 'followers', 'fast?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'differences',\n",
       "  'in',\n",
       "  'life',\n",
       "  'between',\n",
       "  'Chinese',\n",
       "  'and',\n",
       "  'western',\n",
       "  'cultures?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'major',\n",
       "  'differences',\n",
       "  'between',\n",
       "  'Chinese',\n",
       "  'culture',\n",
       "  'and',\n",
       "  'Western',\n",
       "  'cultures?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'expected',\n",
       "  'cutoff',\n",
       "  'for',\n",
       "  'KVPY',\n",
       "  '2016',\n",
       "  'SA',\n",
       "  '-stream',\n",
       "  '2016?\"'],\n",
       " ['\"How',\n",
       "  'many',\n",
       "  'marks',\n",
       "  'are',\n",
       "  'you',\n",
       "  'getting',\n",
       "  'in',\n",
       "  'KVPY',\n",
       "  '2016',\n",
       "  'SA',\n",
       "  'stream?',\n",
       "  '(According',\n",
       "  'to',\n",
       "  'the',\n",
       "  'answer',\n",
       "  'key)\"'],\n",
       " ['\"An',\n",
       "  'honest',\n",
       "  'question.',\n",
       "  'Is',\n",
       "  'Quora',\n",
       "  'biased',\n",
       "  'against',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'and',\n",
       "  'Pro',\n",
       "  'Clinton?\"'],\n",
       " ['\"Why', 'is', 'Quora', 'biased', 'agaist', 'Donald', 'Trump.?\"'],\n",
       " ['\"Why', 'do', 'I', 'want', 'to', 'become', 'a', 'lawyer?\"'],\n",
       " ['\"Why', 'do', 'people', 'want', 'to', 'become', 'lawyers?\"'],\n",
       " ['\"What', 'is', 'the', 'fastest', 'way', 'to', 'lose', 'weight', 'safely?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'loose', 'weight', 'quickly?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'ways',\n",
       "  'in',\n",
       "  'which',\n",
       "  'the',\n",
       "  'American',\n",
       "  'culture',\n",
       "  'is',\n",
       "  'different',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Chinese',\n",
       "  'culture?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'differences',\n",
       "  'between',\n",
       "  'Chinese',\n",
       "  'and',\n",
       "  'American',\n",
       "  'culture?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'solution',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Kashmir',\n",
       "  'conflict?\"'],\n",
       " ['\"What', 'is', 'solution', 'to', 'Kashmir', 'issue?\"'],\n",
       " ['\"How', 'do', 'I', 'introduce', 'myself', 'for', 'an', 'interview?\"'],\n",
       " ['\"How',\n",
       "  'should',\n",
       "  'I',\n",
       "  'respond',\n",
       "  'when',\n",
       "  'asked',\n",
       "  'to',\n",
       "  'introduce',\n",
       "  'myself?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'good',\n",
       "  'gifts',\n",
       "  'for',\n",
       "  'a',\n",
       "  'foreign',\n",
       "  'visitor',\n",
       "  'to',\n",
       "  'bring',\n",
       "  'when',\n",
       "  \"they're\",\n",
       "  'invited',\n",
       "  'to',\n",
       "  \"someone's\",\n",
       "  'home',\n",
       "  'in',\n",
       "  'Turkey',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'good',\n",
       "  'gifts',\n",
       "  'for',\n",
       "  'a',\n",
       "  'foreign',\n",
       "  'visitor',\n",
       "  'to',\n",
       "  'bring',\n",
       "  'when',\n",
       "  \"they're\",\n",
       "  'invited',\n",
       "  'to',\n",
       "  \"someone's\",\n",
       "  'home',\n",
       "  'in',\n",
       "  'Portugal',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time?\"'],\n",
       " ['\"How', 'is', 'the', 'word', \"'persuade'\", 'used', 'in', 'a', 'sentence?\"'],\n",
       " ['\"How', 'is', 'the', 'word', \"'gape'\", 'used', 'in', 'a', 'sentence?\"'],\n",
       " ['\"Could',\n",
       "  'Bruce',\n",
       "  'Lee',\n",
       "  'win',\n",
       "  'every',\n",
       "  'UFC',\n",
       "  'match',\n",
       "  'if',\n",
       "  'he',\n",
       "  'was',\n",
       "  'still',\n",
       "  'around',\n",
       "  'today',\n",
       "  'in',\n",
       "  'his',\n",
       "  'prime?\"'],\n",
       " ['\"Would',\n",
       "  'Bruce',\n",
       "  'Lee',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'hold',\n",
       "  'his',\n",
       "  'own',\n",
       "  'with',\n",
       "  'the',\n",
       "  'top',\n",
       "  'UFC',\n",
       "  'fighters',\n",
       "  'today?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'changing',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'end',\n",
       "  'the',\n",
       "  'black',\n",
       "  'money',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"How',\n",
       "  'will',\n",
       "  'the',\n",
       "  'new',\n",
       "  'currency',\n",
       "  'notes',\n",
       "  'of',\n",
       "  'denomination',\n",
       "  '500',\n",
       "  'and',\n",
       "  '2000',\n",
       "  'curb',\n",
       "  'black',\n",
       "  'money?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'decide',\n",
       "  'what',\n",
       "  'career',\n",
       "  'is',\n",
       "  'best',\n",
       "  'for',\n",
       "  'you?\"'],\n",
       " ['\"How', 'do', 'you', 'decide', 'on', 'the', 'career', 'you', 'want?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'lessons',\n",
       "  'you',\n",
       "  'have',\n",
       "  'learned',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corporate',\n",
       "  'world?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'main',\n",
       "  'lessons',\n",
       "  'a',\n",
       "  'fresh',\n",
       "  'out',\n",
       "  'of',\n",
       "  'college',\n",
       "  'employee',\n",
       "  'needs',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corporate',\n",
       "  'world?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'it',\n",
       "  'like',\n",
       "  'to',\n",
       "  'get',\n",
       "  'diagnosed',\n",
       "  'as',\n",
       "  'a',\n",
       "  'borderline',\n",
       "  'personality',\n",
       "  'disorder',\n",
       "  'patient?\"'],\n",
       " ['\"Abnormal',\n",
       "  'Psychology:',\n",
       "  'What',\n",
       "  'does',\n",
       "  'it',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'to',\n",
       "  'have',\n",
       "  'borderline',\n",
       "  'personality',\n",
       "  'disorder?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'survive',\n",
       "  'in',\n",
       "  'a',\n",
       "  'long',\n",
       "  'distance',\n",
       "  'relationship?\"'],\n",
       " ['\"Does', 'long', 'distance', 'relationships', 'actually', 'work?\"'],\n",
       " ['\"Where',\n",
       "  'do',\n",
       "  'I',\n",
       "  'find',\n",
       "  'a',\n",
       "  'simple',\n",
       "  'to',\n",
       "  'understand',\n",
       "  'solution',\n",
       "  'on',\n",
       "  'how',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'weight?\"'],\n",
       " ['\"How', 'can', 'I', 'lose', 'post', 'marriage', 'weight?\"'],\n",
       " ['\"Why', 'is', 'Hillary', 'Clinton', 'worse', 'than', 'Donald', 'Trump?\"'],\n",
       " ['\"Who',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'better',\n",
       "  'president',\n",
       "  '-',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'or',\n",
       "  'Donald',\n",
       "  'Trump?\"'],\n",
       " ['\"Why', 'do', 'some', 'people', 'think', 'the', 'Earth', 'is', 'flat?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'think',\n",
       "  'that',\n",
       "  'the',\n",
       "  'Earth',\n",
       "  'is',\n",
       "  'flat?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'web',\n",
       "  'scraping',\n",
       "  'and',\n",
       "  'is',\n",
       "  'Python',\n",
       "  'the',\n",
       "  'best',\n",
       "  'language',\n",
       "  'to',\n",
       "  'use',\n",
       "  'for',\n",
       "  'this?\"'],\n",
       " ['\"What', 'are', 'some', 'good', 'Python', 'web', 'scraping', 'tutorials?\"'],\n",
       " ['\"How', 'can', 'I', 'ask', 'great', 'questions', 'on', 'Quora?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'ask',\n",
       "  'a',\n",
       "  'really',\n",
       "  'great',\n",
       "  'question',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'career',\n",
       "  'options',\n",
       "  'after',\n",
       "  'completing',\n",
       "  'b.com?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'career',\n",
       "  'options',\n",
       "  'available',\n",
       "  'after',\n",
       "  'completing',\n",
       "  'a',\n",
       "  'B.Tech?\"'],\n",
       " ['\"Do', 'you', 'believe', 'in', 'horoscope?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'three',\n",
       "  'daily',\n",
       "  'practices',\n",
       "  'to',\n",
       "  'stay',\n",
       "  'young?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stay',\n",
       "  'consistent',\n",
       "  'in',\n",
       "  'things',\n",
       "  'I',\n",
       "  'do',\n",
       "  'daily?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'views',\n",
       "  'on',\n",
       "  'Modi',\n",
       "  'governments',\n",
       "  'decision',\n",
       "  'to',\n",
       "  'demonetize',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes?',\n",
       "  'How',\n",
       "  'will',\n",
       "  'this',\n",
       "  'affect',\n",
       "  'economy?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'effect',\n",
       "  'of',\n",
       "  'Modi',\n",
       "  \"Government's\",\n",
       "  'decision',\n",
       "  'of',\n",
       "  'invalidating',\n",
       "  'the',\n",
       "  'RS',\n",
       "  '500',\n",
       "  'and',\n",
       "  'RS',\n",
       "  '1000',\n",
       "  'notes?\"'],\n",
       " ['\"I',\n",
       "  'forgot',\n",
       "  'my',\n",
       "  'Facebook',\n",
       "  'password',\n",
       "  'and',\n",
       "  'email',\n",
       "  'password.',\n",
       "  'How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'log',\n",
       "  'into',\n",
       "  'Facebook?\"'],\n",
       " ['\"Can', 'you', 'log', 'in', 'to', 'Facebook', 'without', 'a', 'password?\"'],\n",
       " ['\"How', 'do', 'we', 'start', 'preparing', 'c', 'language?\"'],\n",
       " ['\"How', 'should', 'I', 'get', 'started', 'with', 'c', 'language?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'unexpected',\n",
       "  'things',\n",
       "  'first-time',\n",
       "  'visitors',\n",
       "  'to',\n",
       "  'Maldives',\n",
       "  'notice?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'unexpected',\n",
       "  'things',\n",
       "  'first-time',\n",
       "  'visitors',\n",
       "  'to',\n",
       "  'China',\n",
       "  'notice?\"'],\n",
       " ['\"Do', 'you', 'make', 'money', 'by', 'writing', 'answers', 'on', 'Quora?\"'],\n",
       " ['\"Do',\n",
       "  'you',\n",
       "  'make',\n",
       "  'money',\n",
       "  'by',\n",
       "  'answering',\n",
       "  'questions',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"How', 'do', 'I', 'use', 'Jio', 'sim', 'in', 'my', 'iPhone', '5s?\"'],\n",
       " ['\"Can', 'I', 'use', 'Jio', 'SIM', 'in', 'iPhone', '5s?\"'],\n",
       " ['\"How', 'do', 'I', 'enhance', 'my', 'English?\"'],\n",
       " ['\"How', 'can', 'I', 'continue', 'to', 'improve', 'my', 'English?\"'],\n",
       " ['\"Why', 'is', 'cricket', 'not', 'played', 'at', 'the', 'Olympics?\"'],\n",
       " ['\"Why', 'cricket', 'is', 'not', 'played', 'in', 'Olympics?\"'],\n",
       " ['\"What',\n",
       "  'should',\n",
       "  'I',\n",
       "  'do',\n",
       "  'to',\n",
       "  'get',\n",
       "  'an',\n",
       "  'intership',\n",
       "  'at',\n",
       "  'Google?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'internship', 'at', 'Google?\"'],\n",
       " ['\"Why', 'is', 'selling', 'weed', 'illegal?\"'],\n",
       " ['\"Why', 'is', 'weed', 'illegal?\"'],\n",
       " ['\"How', 'do', 'you', 'improve', 'your', 'programming', 'skills?\"'],\n",
       " ['\"How', 'can', 'i', 'improve', 'my', 'coding', 'skills?\"'],\n",
       " ['\"What', 'are', 'the', 'most', 'embarrassing', 'moments', 'in', 'life?\"'],\n",
       " ['\"What’s',\n",
       "  'the',\n",
       "  'most',\n",
       "  'embarrassing',\n",
       "  'moment',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life',\n",
       "  'so',\n",
       "  'far?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'free',\n",
       "  'online',\n",
       "  'test',\n",
       "  'that',\n",
       "  'I',\n",
       "  'can',\n",
       "  'take',\n",
       "  'to',\n",
       "  'measure',\n",
       "  'my',\n",
       "  'IQ',\n",
       "  'correctly?\"'],\n",
       " ['\"Are',\n",
       "  'there',\n",
       "  'any',\n",
       "  'free',\n",
       "  'online',\n",
       "  'iq',\n",
       "  'tests',\n",
       "  'that',\n",
       "  'are',\n",
       "  'accurate?\"'],\n",
       " ['\"Who', 'is', 'the', 'richest', 'man', 'in', 'Nigeria?\"'],\n",
       " ['\"Who', 'is', 'the', 'richest', 'man?\"'],\n",
       " ['\"Why',\n",
       "  'the',\n",
       "  'Modi',\n",
       "  'government',\n",
       "  'ban',\n",
       "  'the',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'notes?\"'],\n",
       " ['\"Banning',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'is',\n",
       "  'appreciated',\n",
       "  'but',\n",
       "  'why',\n",
       "  'is',\n",
       "  'the',\n",
       "  'government',\n",
       "  'bringing',\n",
       "  '500',\n",
       "  'and',\n",
       "  '2000',\n",
       "  'rupees',\n",
       "  'again',\n",
       "  'into',\n",
       "  'the',\n",
       "  'market?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'start',\n",
       "  'learning',\n",
       "  'python',\n",
       "  'web',\n",
       "  'programming',\n",
       "  'from',\n",
       "  'scratch',\n",
       "  'to',\n",
       "  'advanced?\"'],\n",
       " ['\"How', 'can', 'I', 'start', 'learning', \"Python's\", 'language?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'Citizens?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'Itron?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'start',\n",
       "  'learning',\n",
       "  'programming',\n",
       "  'language?',\n",
       "  'Which',\n",
       "  'one',\n",
       "  'to',\n",
       "  'start',\n",
       "  'with?\"'],\n",
       " ['\"Where', 'should', 'I', 'start', 'learning', 'to', 'program?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'positive',\n",
       "  'and',\n",
       "  'negative',\n",
       "  'effects',\n",
       "  'of',\n",
       "  'demonization',\n",
       "  'of',\n",
       "  '₹500/1000',\n",
       "  'notes',\n",
       "  'on',\n",
       "  'Indian',\n",
       "  'economy?\"'],\n",
       " ['\"How',\n",
       "  'will',\n",
       "  \"Modi's\",\n",
       "  'move',\n",
       "  'to',\n",
       "  'discontinue',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'notes',\n",
       "  'effect',\n",
       "  'the',\n",
       "  'economy?\"'],\n",
       " ['\"What', 'are', 'your', 'opinions', 'on', 'Brexit?\"'],\n",
       " ['\"What', 'is', 'your', 'opinion', 'on', 'brexit?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'Test',\n",
       "  'Series',\n",
       "  'SSC',\n",
       "  'CGL',\n",
       "  'Tier',\n",
       "  '2',\n",
       "  'exam?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'good',\n",
       "  'online',\n",
       "  'mock',\n",
       "  'test',\n",
       "  'series',\n",
       "  'for',\n",
       "  'SSC',\n",
       "  'CGL',\n",
       "  'Tier',\n",
       "  '2',\n",
       "  '2016?\"'],\n",
       " ['\"What', 'are', 'good', 'scary', 'movies?\"'],\n",
       " ['\"What\\'s', 'the', 'best', 'non-slasher', 'scary', 'movie?\"'],\n",
       " ['\"Is', 'there', 'any', 'life', 'in', 'Mars?\"'],\n",
       " ['\"Do', 'you', 'think', 'there', 'is', 'life', 'on', 'Mars?\"'],\n",
       " ['\"Are', 'there', 'real', 'life', 'karma', 'police?\"'],\n",
       " ['\"What\\'s',\n",
       "  'the',\n",
       "  'best',\n",
       "  'real',\n",
       "  'life',\n",
       "  'karma',\n",
       "  \"you've\",\n",
       "  'ever',\n",
       "  'seen/experienced?\"'],\n",
       " ['\"I',\n",
       "  'am',\n",
       "  'a',\n",
       "  'teenage',\n",
       "  'girl',\n",
       "  'and',\n",
       "  'my',\n",
       "  'face',\n",
       "  'has',\n",
       "  'dark',\n",
       "  'spots.',\n",
       "  'Which',\n",
       "  'face',\n",
       "  'wash',\n",
       "  'should',\n",
       "  'I',\n",
       "  'use?',\n",
       "  'How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'cure',\n",
       "  'the',\n",
       "  'spots?\"'],\n",
       " ['\"Which',\n",
       "  'cream',\n",
       "  'is',\n",
       "  'best',\n",
       "  'for',\n",
       "  'removing',\n",
       "  'dark',\n",
       "  'spots',\n",
       "  'in',\n",
       "  'the',\n",
       "  'face?\"'],\n",
       " ['\"What', 'is', 'the', 'best', '2016', 'film', 'released', 'so', 'far?\"'],\n",
       " ['\"Which', 'was', 'the', 'best', 'film', 'of', '2016?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'speak', 'English', 'well?\"'],\n",
       " ['\"How', 'can', 'I', 'improve', 'my', 'English', 'grammar?\"'],\n",
       " ['\"How', 'do', 'I', 'become', 'a', 'good', 'digital', 'marketer?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'digital',\n",
       "  'marketing?',\n",
       "  'What',\n",
       "  'is',\n",
       "  'good',\n",
       "  'Way',\n",
       "  'Learning',\n",
       "  'for',\n",
       "  'digital',\n",
       "  'Marketing?',\n",
       "  'Any',\n",
       "  'good',\n",
       "  'website.\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'prepare',\n",
       "  'for',\n",
       "  'UGC',\n",
       "  'NET',\n",
       "  'English',\n",
       "  'Literature',\n",
       "  'exam?\"'],\n",
       " ['\"How', 'do', 'I', 'prepare', 'for', 'UGC', 'NET', 'English?\"'],\n",
       " ['\"Why', 'is', 'Messi', 'better', 'than', 'Cristiano', 'Ronaldo?\"'],\n",
       " ['\"How', 'is', 'Ronaldo', 'better', 'than', 'Messi?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'demerits',\n",
       "  'of',\n",
       "  'having',\n",
       "  'a',\n",
       "  'physically',\n",
       "  'attractive',\n",
       "  'and',\n",
       "  'beautiful',\n",
       "  'wife?\"'],\n",
       " ['\"Which',\n",
       "  'Indian',\n",
       "  'cricketer',\n",
       "  'has',\n",
       "  'the',\n",
       "  'most',\n",
       "  'attractive',\n",
       "  'wife?\"'],\n",
       " ['\"Why',\n",
       "  'does',\n",
       "  'quora',\n",
       "  'mark',\n",
       "  'my',\n",
       "  'questions',\n",
       "  'as',\n",
       "  'needing',\n",
       "  'improvement?\"'],\n",
       " ['\"Why',\n",
       "  'does',\n",
       "  'Quora',\n",
       "  'keep',\n",
       "  'marking',\n",
       "  'my',\n",
       "  'questions',\n",
       "  'as',\n",
       "  '\"\"needing',\n",
       "  'improvement\"\"?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'caring',\n",
       "  'about',\n",
       "  'people',\n",
       "  'who',\n",
       "  'do',\n",
       "  'not',\n",
       "  'care',\n",
       "  'about',\n",
       "  'you?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'caring',\n",
       "  'for',\n",
       "  'people',\n",
       "  'who',\n",
       "  \"don't\",\n",
       "  'really',\n",
       "  'care',\n",
       "  'for',\n",
       "  'me?\"'],\n",
       " ['\"Was', 'Nandani', 'the', 'real', 'wife', 'of', 'Chandragupta', 'Maurya?\"'],\n",
       " ['\"Is', 'Justin', 'the', 'son', 'of', 'Chandragupta', 'Maurya?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'happen',\n",
       "  'to',\n",
       "  'America',\n",
       "  'now',\n",
       "  'that',\n",
       "  'Trump',\n",
       "  'is',\n",
       "  'president?\"'],\n",
       " ['\"What',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'to',\n",
       "  'this',\n",
       "  'country',\n",
       "  'if',\n",
       "  'Trump',\n",
       "  'were',\n",
       "  'elected',\n",
       "  'president?\"'],\n",
       " ['\"Does', 'Lipton', 'green', 'tea', 'Assist', 'in', 'weight', 'loss?\"'],\n",
       " ['\"Does', 'green', 'chai', 'tea', 'assist', 'with', 'weight', 'loss?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'a',\n",
       "  'German',\n",
       "  'Shepherd/Border',\n",
       "  'Collie',\n",
       "  'mix',\n",
       "  'puppy',\n",
       "  'from',\n",
       "  'chewing',\n",
       "  'my',\n",
       "  'shoes?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'my',\n",
       "  'Yorkie/Corgi',\n",
       "  'mix',\n",
       "  'from',\n",
       "  'chewing',\n",
       "  'my',\n",
       "  'shoes?\"'],\n",
       " ['\"I',\n",
       "  'accidentally',\n",
       "  'posted',\n",
       "  'a',\n",
       "  'vulgar',\n",
       "  'message',\n",
       "  'on',\n",
       "  'a',\n",
       "  'WhatsApp',\n",
       "  'group.',\n",
       "  'What',\n",
       "  'should',\n",
       "  'I',\n",
       "  'do?\"'],\n",
       " ['\"How', 'do', 'I', 'check', 'Whatsapp', 'messages', 'online?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'you',\n",
       "  'join',\n",
       "  'Quora?',\n",
       "  'What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'good',\n",
       "  'reasons',\n",
       "  'to',\n",
       "  'join?\"'],\n",
       " ['\"What',\n",
       "  'prompted',\n",
       "  'you',\n",
       "  'to',\n",
       "  'join',\n",
       "  'Quora',\n",
       "  'as',\n",
       "  'a',\n",
       "  'full-time',\n",
       "  'member',\n",
       "  'of',\n",
       "  'the',\n",
       "  'team?\"'],\n",
       " ['\"What', 'are', 'the', 'fastest', 'ways', 'to', 'increase', 'height?\"'],\n",
       " ['\"How', 'do', 'I', 'increase', 'my', 'height?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Minnesota?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Wisconsin?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'so',\n",
       "  'many',\n",
       "  'Americans',\n",
       "  'vote',\n",
       "  'for',\n",
       "  'Donald',\n",
       "  'Trump?\"'],\n",
       " ['\"Will', 'the', 'Americans', 'actually', 'vote', 'Donald', 'Trump?\"'],\n",
       " ['\"Why', 'is', 'India', 'medal-less', 'in', '2016', 'Rio', 'Olympics?\"'],\n",
       " ['\"How', 'many', 'medals', 'will', 'India', 'win', 'in', 'Rio', 'Olympics?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'start', 'up', 'to', 'start?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'start', 'up', 'idea?\"'],\n",
       " ['\"Can', 'you', 'make', 'money', 'on', 'tumblr?', 'How?\"'],\n",
       " ['\"How', 'can', 'I', 'make', 'money', 'with', 'tumblr?\"'],\n",
       " ['\"Why', 'do', 'the', 'stars', 'in', 'the', 'sky', 'flicker?\"'],\n",
       " ['\"Why', 'do', 'stars', 'flick?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'over', 'a', 'girl', 'that', 'I', 'like?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'over',\n",
       "  'a',\n",
       "  'girl',\n",
       "  'who',\n",
       "  \"doesn't\",\n",
       "  'like',\n",
       "  'me?\"'],\n",
       " ['\"Is',\n",
       "  'world',\n",
       "  'wrestling',\n",
       "  \"entertainment's\",\n",
       "  'fights',\n",
       "  'real',\n",
       "  'or',\n",
       "  'fake?\"'],\n",
       " ['\"Is', 'pro', 'wrestling', 'fake?\"'],\n",
       " ['\"What', 'is', 'the', 'wildest', 'thing', 'you', 'ever', 'done?\"'],\n",
       " ['\"What', 'is', 'the', 'wildest', 'thing', 'you', 'have', 'done?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'suggestions',\n",
       "  'or',\n",
       "  'ideas',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'English',\n",
       "  'writing',\n",
       "  'skills?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'writing',\n",
       "  'skills',\n",
       "  'in',\n",
       "  'English.?\"'],\n",
       " ['\"How',\n",
       "  'come',\n",
       "  'the',\n",
       "  'media',\n",
       "  'is',\n",
       "  'quiet',\n",
       "  'about',\n",
       "  'Donald',\n",
       "  \"Trump's\",\n",
       "  'child',\n",
       "  'rape',\n",
       "  'case?\"'],\n",
       " ['\"Why',\n",
       "  \"isn't\",\n",
       "  'Donald',\n",
       "  \"Trump's\",\n",
       "  'rape',\n",
       "  'case',\n",
       "  'a',\n",
       "  'bigger',\n",
       "  'story?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'ways', 'to', 'lose', 'weight?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'weight?',\n",
       "  'What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'diet',\n",
       "  'plan?\"'],\n",
       " ['\"How', 'do', 'I', 'self-train', 'myself', 'in', 'marathon?\"'],\n",
       " ['\"How', 'should', 'one', 'prepare', 'for', 'a', 'marathon?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'know',\n",
       "  'know',\n",
       "  'whether',\n",
       "  'my',\n",
       "  'gf',\n",
       "  'is',\n",
       "  'cheating',\n",
       "  'me',\n",
       "  'or',\n",
       "  'not?\"'],\n",
       " ['\"How',\n",
       "  'will',\n",
       "  'you',\n",
       "  'know',\n",
       "  'if',\n",
       "  'your',\n",
       "  'partner',\n",
       "  'was',\n",
       "  'or',\n",
       "  'still',\n",
       "  'cheating',\n",
       "  'on',\n",
       "  'you?\"'],\n",
       " ['\"How', 'do', 'I', 'access', 'the', 'deep', 'dark', 'web?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'to', 'the', 'dark', 'web?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'job', 'for', 'a', '12', 'year', 'old?\"'],\n",
       " ['\"What', 'are', 'great', 'jobs', 'for', '17', 'year', 'olds?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'resources',\n",
       "  'online',\n",
       "  'to',\n",
       "  'self-study',\n",
       "  'German?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'free',\n",
       "  'Online',\n",
       "  'Resources',\n",
       "  'for',\n",
       "  'learning',\n",
       "  'German?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'build',\n",
       "  'a',\n",
       "  'perpetual',\n",
       "  'motion',\n",
       "  'machine?\"'],\n",
       " ['\"Is', 'a', 'perpetual', 'motion', 'machine', 'theoretically', 'possible?\"'],\n",
       " ['\"How', 'you', 'ever', 'been', 'raped?\"'],\n",
       " ['\"Have', 'you', 'been', 'raped?\"'],\n",
       " ['\"Why', 'did', 'Arnab', 'Goswami', 'quit', 'TIMES', 'NOW?\"'],\n",
       " ['\"Is', 'Arnab', 'Goswami', 'quitting', 'from', 'Times', 'now?\"'],\n",
       " ['\"Has',\n",
       "  'the',\n",
       "  '50AE',\n",
       "  'Desert',\n",
       "  'eagle',\n",
       "  'a',\n",
       "  'almost',\n",
       "  '99%',\n",
       "  'kill',\n",
       "  'chance',\n",
       "  'with',\n",
       "  'a',\n",
       "  'headshot',\n",
       "  'at',\n",
       "  'close',\n",
       "  'range?\"'],\n",
       " ['\"Can',\n",
       "  'I',\n",
       "  'survive',\n",
       "  'A',\n",
       "  '50',\n",
       "  'AE',\n",
       "  'Desert',\n",
       "  'eagle',\n",
       "  'hit',\n",
       "  'in',\n",
       "  'The',\n",
       "  'head',\n",
       "  'at',\n",
       "  'close',\n",
       "  'range?\"'],\n",
       " ['\"What', 'are', 'ways', 'to', 'make', 'money', 'online', 'at', 'home?\"'],\n",
       " ['\"Can', 'I', 'make', 'money', 'online?\"'],\n",
       " ['\"Can',\n",
       "  'we',\n",
       "  'change',\n",
       "  'email',\n",
       "  'id',\n",
       "  'registered',\n",
       "  'with',\n",
       "  'Aadhar',\n",
       "  'card?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'register',\n",
       "  'my',\n",
       "  'email',\n",
       "  'ID',\n",
       "  'and',\n",
       "  'phone',\n",
       "  'number',\n",
       "  'in',\n",
       "  'an',\n",
       "  'Aadhar',\n",
       "  'card',\n",
       "  '-',\n",
       "  'UID',\n",
       "  'online?\"'],\n",
       " ['\"Why',\n",
       "  \"can't\",\n",
       "  'I',\n",
       "  'load',\n",
       "  'money',\n",
       "  'on',\n",
       "  'Paytm',\n",
       "  'using',\n",
       "  'SBI',\n",
       "  'Card',\n",
       "  'or',\n",
       "  'Internet',\n",
       "  'banking?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'transfer',\n",
       "  'money',\n",
       "  'from',\n",
       "  'an',\n",
       "  'SBI',\n",
       "  'account',\n",
       "  'to',\n",
       "  'another',\n",
       "  'bank',\n",
       "  'account',\n",
       "  'using',\n",
       "  'a',\n",
       "  'debit',\n",
       "  'card?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'letter',\n",
       "  'format',\n",
       "  'for',\n",
       "  'writing',\n",
       "  'a',\n",
       "  'letter',\n",
       "  'to',\n",
       "  'bank',\n",
       "  'manager',\n",
       "  'asking',\n",
       "  'him',\n",
       "  'issuing',\n",
       "  'a',\n",
       "  'demand',\n",
       "  'draft',\n",
       "  'of',\n",
       "  '98000',\n",
       "  'in',\n",
       "  'favour',\n",
       "  'of',\n",
       "  'xxxyyyzzz',\n",
       "  'college',\n",
       "  'for',\n",
       "  'the',\n",
       "  'semester',\n",
       "  'fee',\n",
       "  '(education',\n",
       "  'loan',\n",
       "  'is',\n",
       "  'already',\n",
       "  'senctioned)?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'application',\n",
       "  'format',\n",
       "  'for',\n",
       "  'a',\n",
       "  'letter',\n",
       "  'bank',\n",
       "  'manager',\n",
       "  'for',\n",
       "  'an',\n",
       "  'education',\n",
       "  'loan',\n",
       "  'demand',\n",
       "  'draft?\"'],\n",
       " ['\"Which', 'is', 'your', 'favourite', 'film', 'in', '2016?\"'],\n",
       " ['\"Which', 'movies', 'are', 'the', 'best', 'in', '2016?\"'],\n",
       " ['\"How',\n",
       "  'is',\n",
       "  'everyone',\n",
       "  'earning',\n",
       "  'millions',\n",
       "  'from',\n",
       "  'home',\n",
       "  'by',\n",
       "  'just',\n",
       "  'using',\n",
       "  'Uber',\n",
       "  'app',\n",
       "  'in',\n",
       "  'mobile?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'people',\n",
       "  'earn',\n",
       "  'billions',\n",
       "  'from',\n",
       "  'home',\n",
       "  'using',\n",
       "  'simple',\n",
       "  'Uber',\n",
       "  'app',\n",
       "  'hack?\"'],\n",
       " ['\"When',\n",
       "  'was',\n",
       "  'the',\n",
       "  'last',\n",
       "  'time',\n",
       "  'you',\n",
       "  'felt',\n",
       "  'true',\n",
       "  'happiness?',\n",
       "  'And',\n",
       "  'for',\n",
       "  'what',\n",
       "  'reason?\"'],\n",
       " ['\"When',\n",
       "  'was',\n",
       "  'the',\n",
       "  'last',\n",
       "  'time',\n",
       "  'you',\n",
       "  'felt',\n",
       "  'happy?',\n",
       "  'Because',\n",
       "  'I',\n",
       "  \"can't\",\n",
       "  'remeber\"'],\n",
       " ['\"What', 'should', 'I', 'do', 'to', 'crack', 'Gsoc', '\\'16?\"'],\n",
       " ['\"How', 'should', 'I', 'prepare', 'for', 'GSoc', '2017?\"'],\n",
       " ['\"How', 'do', 'I', 'contact', 'a', 'real', 'hacker?\"'],\n",
       " ['\"Where', 'can', 'I', 'find', 'a', 'professional', 'hacker?\"'],\n",
       " ['\"How', 'do', 'I', 'overcome', 'my', 'anger', 'problem?\"'],\n",
       " ['\"What', 'should', 'I', 'do', 'to', 'overcome', 'my', 'anger?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'earn',\n",
       "  'money',\n",
       "  'online',\n",
       "  'without',\n",
       "  'investing?',\n",
       "  'Are',\n",
       "  'there',\n",
       "  'some',\n",
       "  'authentic',\n",
       "  'jobs',\n",
       "  'online?\"'],\n",
       " ['\"How', 'do', 'I', 'make', 'money', 'online', 'without', 'investment?\"'],\n",
       " ['\"How', 'hard', 'has', 'puberty', 'hit', 'you?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'way',\n",
       "  'of',\n",
       "  'getting',\n",
       "  'full',\n",
       "  'puberty',\n",
       "  'growth',\n",
       "  'after',\n",
       "  '20s?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'movies',\n",
       "  'ever',\n",
       "  'irrespective',\n",
       "  'of',\n",
       "  'genre?\"'],\n",
       " ['\"What', 'are', 'the', 'five', 'best', 'movies', 'of', 'all', 'time?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  '‘Edgware’',\n",
       "  'and',\n",
       "  'how',\n",
       "  'does',\n",
       "  'the',\n",
       "  'lifestyle',\n",
       "  'compare',\n",
       "  'to',\n",
       "  'the',\n",
       "  'London',\n",
       "  'Borough',\n",
       "  'of',\n",
       "  'Brent?\"'],\n",
       " ['\"How',\n",
       "  'does',\n",
       "  'Edgware',\n",
       "  'compare',\n",
       "  'with',\n",
       "  'other',\n",
       "  'neighbourhoods',\n",
       "  'in',\n",
       "  'London?\"'],\n",
       " ['\"Can', 'World', 'War', '3', 'ever', 'take', 'place?\"'],\n",
       " ['\"How', 'soon', 'is', 'world', 'war', 'III?\"'],\n",
       " ['\"What', 'is', 'your', 'New', \"Year's\", 'resolutions', 'for', '2017?\"'],\n",
       " ['\"What\\'s', 'your', 'resolutions', 'for', '2017?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'reduce',\n",
       "  'tummy',\n",
       "  'fat',\n",
       "  'through',\n",
       "  'a',\n",
       "  'diet',\n",
       "  'without',\n",
       "  'yoga',\n",
       "  'or',\n",
       "  'exercise?\"'],\n",
       " ['\"How', 'do', 'I', 'reduce', 'my', 'tummy', 'fat?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'inpatient',\n",
       "  'drug',\n",
       "  'and',\n",
       "  'alcohol',\n",
       "  'rehab',\n",
       "  'center',\n",
       "  'near',\n",
       "  'Wheeler',\n",
       "  'County',\n",
       "  'GA?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'inpatient',\n",
       "  'drug',\n",
       "  'and',\n",
       "  'alcohol',\n",
       "  'rehab',\n",
       "  'center',\n",
       "  'near',\n",
       "  'Lamar',\n",
       "  'County',\n",
       "  'GA?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'the',\n",
       "  'guy',\n",
       "  'I',\n",
       "  'like',\n",
       "  'to',\n",
       "  'like',\n",
       "  'me',\n",
       "  'again?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'a',\n",
       "  'guy',\n",
       "  'I',\n",
       "  'like',\n",
       "  'to',\n",
       "  'think',\n",
       "  'about',\n",
       "  'me?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'C++', 'books?\"'],\n",
       " ['\"What', 'are', 'the', 'top', 'C++', 'books?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'first?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'programming',\n",
       "  'languages',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'today?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'thing(s)',\n",
       "  'about',\n",
       "  'raising',\n",
       "  'children',\n",
       "  'in',\n",
       "  'Singapore?\"'],\n",
       " ['\"What', 'is', 'the', 'QuickBooks', 'support', 'phone', 'number?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'QuickBooks',\n",
       "  'proadvisor',\n",
       "  'tech',\n",
       "  'support',\n",
       "  'number?\"'],\n",
       " ['\"How', 'do', 'you', 'carrier', 'unlock', 'an', 'iPhone?\"'],\n",
       " ['\"How', 'can', 'an', 'iPhone', 'be', 'carrier', 'unlocked?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'reset',\n",
       "  'my',\n",
       "  'Gmail',\n",
       "  'password',\n",
       "  'when',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'remember',\n",
       "  'my',\n",
       "  'recovery',\n",
       "  'information?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'reset',\n",
       "  'my',\n",
       "  'Gmail',\n",
       "  'password',\n",
       "  'when',\n",
       "  'I',\n",
       "  \"don't\",\n",
       "  'remember',\n",
       "  'my',\n",
       "  'recovery',\n",
       "  'information?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'funniest',\n",
       "  'moments',\n",
       "  'in',\n",
       "  '2016',\n",
       "  'Rio',\n",
       "  'Olympics?\"'],\n",
       " ['\"What',\n",
       "  'heartbreaking',\n",
       "  'moments',\n",
       "  'from',\n",
       "  'Rio',\n",
       "  'Olympics',\n",
       "  '2016',\n",
       "  'will',\n",
       "  'be',\n",
       "  'remembered',\n",
       "  'forever?\"'],\n",
       " ['\"How', 'does', 'one', 'move', 'on?\"'],\n",
       " ['\"How', 'can', 'one', 'move', 'on', 'from', 'the', 'past?\"'],\n",
       " ['\"How', 'can', 'I', 'get', 'traffic', 'on', 'website?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'drive',\n",
       "  'traffic',\n",
       "  'to',\n",
       "  'a',\n",
       "  'website?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'restore',\n",
       "  'WhatsApp',\n",
       "  'chats',\n",
       "  'from',\n",
       "  'Google',\n",
       "  'Drive',\n",
       "  'backup',\n",
       "  'on',\n",
       "  'an',\n",
       "  'iPhone?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'retrieve',\n",
       "  'old',\n",
       "  'WhatsApp',\n",
       "  'chat',\n",
       "  'backups',\n",
       "  'from',\n",
       "  'Google',\n",
       "  'Drive',\n",
       "  'without',\n",
       "  'losing',\n",
       "  'the',\n",
       "  'new',\n",
       "  'chats',\n",
       "  'on',\n",
       "  'my',\n",
       "  'iPhone?',\n",
       "  'Or',\n",
       "  'how',\n",
       "  'do',\n",
       "  'I',\n",
       "  'merge',\n",
       "  'iCloud',\n",
       "  'and',\n",
       "  'Drive',\n",
       "  'backups?\"'],\n",
       " ['\"How', 'could', 'I', 'gain', 'weight', 'in', 'a', 'healthy', 'way?\"'],\n",
       " ['\"How', 'to', 'gain', 'weight', '?\"'],\n",
       " ['\"What',\n",
       "  'does',\n",
       "  'a',\n",
       "  'Trump',\n",
       "  'presidency',\n",
       "  'mean',\n",
       "  'for',\n",
       "  'Indian',\n",
       "  'students',\n",
       "  'looking',\n",
       "  'to',\n",
       "  'pursue',\n",
       "  \"Master's\",\n",
       "  'degrees',\n",
       "  'in',\n",
       "  'USA?\"'],\n",
       " ['\"What',\n",
       "  'does',\n",
       "  'Donald',\n",
       "  \"Trump's\",\n",
       "  'win',\n",
       "  'mean',\n",
       "  'for',\n",
       "  'Indian',\n",
       "  'students',\n",
       "  'in',\n",
       "  'USA?\"'],\n",
       " ['\"How', 'can', 'I', 'lose', 'weight', 'at', 'age', '55?\"'],\n",
       " ['\"How', 'do', 'I', 'lose', '5', 'kg', 'weight', 'within', '7', 'days?\"'],\n",
       " ['\"What', 'are', 'the', 'craziest', 'facts', 'about', 'Microsoft?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'mind',\n",
       "  'blowing',\n",
       "  'facts',\n",
       "  'about',\n",
       "  'Google?\"'],\n",
       " ['\"What', 'are', 'the', 'greatest', 'action', 'movies?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'action', 'movies?\"'],\n",
       " ['\"Why', 'did', 'you', 'convert', 'to', 'islam?\"'],\n",
       " ['\"Why', 'have', 'you', 'accepted', 'Islam?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'moment', 'in', 'our', 'life?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'moment',\n",
       "  'in',\n",
       "  'your',\n",
       "  'life',\n",
       "  'you',\n",
       "  'can',\n",
       "  'think',\n",
       "  'of?\"'],\n",
       " ['\"Can', 'you', 'be', 'a', 'Muslim', 'and', 'gay?\"'],\n",
       " ['\"What', 'should', 'I', 'do', 'if', 'my', 'sibling', 'left', 'Islam?\"'],\n",
       " ['\"Daniel',\n",
       "  'Ek:',\n",
       "  'Why',\n",
       "  \"isn't\",\n",
       "  'Spotify',\n",
       "  'coming',\n",
       "  'to',\n",
       "  'India',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'the',\n",
       "  'huge',\n",
       "  'internet',\n",
       "  'consumption',\n",
       "  'India',\n",
       "  'has?\"'],\n",
       " ['\"Daniel',\n",
       "  'Ek:',\n",
       "  'When',\n",
       "  'will',\n",
       "  'you',\n",
       "  'launch',\n",
       "  'Spotify',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"Is', 'there', 'life', 'on', 'other', 'planets?\"'],\n",
       " ['\"What', 'are', 'some', 'signs', 'of', 'life', 'on', 'other', 'planets?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'tips',\n",
       "  'for',\n",
       "  'clearing',\n",
       "  'Google',\n",
       "  'Summer',\n",
       "  'of',\n",
       "  'Code?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'prepare',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Google',\n",
       "  'Summer',\n",
       "  'of',\n",
       "  'Code',\n",
       "  '(GSoC)?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'U.S.',\n",
       "  'including',\n",
       "  'it’s',\n",
       "  'territories',\n",
       "  'and',\n",
       "  'possessions?',\n",
       "  '2\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Montana?\"'],\n",
       " ['\"How',\n",
       "  'many',\n",
       "  'days',\n",
       "  'does',\n",
       "  'it',\n",
       "  'take',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'fresh',\n",
       "  'passport',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"How',\n",
       "  'many',\n",
       "  'days',\n",
       "  'to',\n",
       "  'get',\n",
       "  'passport',\n",
       "  'for',\n",
       "  'kids',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What',\n",
       "  'was',\n",
       "  'the',\n",
       "  'best',\n",
       "  'day',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life?',\n",
       "  '(Excluding',\n",
       "  'family',\n",
       "  'things',\n",
       "  'like',\n",
       "  'births)\"'],\n",
       " ['\"What',\n",
       "  'was',\n",
       "  'the',\n",
       "  'best',\n",
       "  'day',\n",
       "  'of',\n",
       "  'your',\n",
       "  'life?',\n",
       "  'What',\n",
       "  'happened?\"'],\n",
       " ['\"What', 'are', 'some', 'interesting', 'facts', 'about', 'Bill', 'Gates?\"'],\n",
       " ['\"What', 'are', 'some', 'unknown', 'facts', 'about', 'Bill', 'Gates?\"'],\n",
       " ['\"What', 'is', 'firefoxpatch.exe?\"'],\n",
       " ['\"What', 'does', 'Javac.exe', 'do?\"'],\n",
       " ['\"How', 'do', 'I', 'upload', 'profile', 'pic', 'in', 'Quora?\"'],\n",
       " ['\"How', 'do', 'I', 'change', 'profile', 'picture', 'in', 'Quora?\"'],\n",
       " ['\"How', 'should', 'you', 'start', 'learning', 'programming?\"'],\n",
       " ['\"How', 'do', 'you', 'get', 'started', 'learning', 'programming?\"'],\n",
       " ['\"How', 'do', 'I', 'know', 'if', 'I', 'have', 'met', '\"\"The', 'One\"\"?\"'],\n",
       " ['\"How', 'do', 'I', 'know', 'if', \"I've\", 'met', 'my', 'life', 'partner?\"'],\n",
       " ['\"How', 'do', 'I', 'prepare', 'for', 'TOEFL', 'exam?\"'],\n",
       " ['\"How', 'can', 'I', 'study', 'for', 'TOEFL', 'exam?\"'],\n",
       " ['\"How', 'do', 'i', 'know', 'if', 'someone', 'loves', 'me?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'know',\n",
       "  'if',\n",
       "  'he',\n",
       "  'is',\n",
       "  'indeed',\n",
       "  'in',\n",
       "  'love',\n",
       "  'with',\n",
       "  'me?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'one',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'fear',\n",
       "  'of',\n",
       "  'having',\n",
       "  'anxiety?\"'],\n",
       " ['\"How', 'we', 'get', 'rid', 'of', 'fear?\"'],\n",
       " ['\"What',\n",
       "  'country',\n",
       "  'has',\n",
       "  'the',\n",
       "  'highest',\n",
       "  'average',\n",
       "  'virginity',\n",
       "  'losing',\n",
       "  'age?\"'],\n",
       " ['\"Does',\n",
       "  'masturbation',\n",
       "  'cause',\n",
       "  'you',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'your',\n",
       "  'virginity?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'way',\n",
       "  'I',\n",
       "  'can',\n",
       "  'track',\n",
       "  'a',\n",
       "  'lost',\n",
       "  'iPhone',\n",
       "  '4s',\n",
       "  'using',\n",
       "  'imei',\n",
       "  'code?\"'],\n",
       " ['\"How', 'do', 'I', 'find', 'my', 'lost', 'mobile', 'using', 'IMEI?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'permanently',\n",
       "  'change',\n",
       "  'the',\n",
       "  'color',\n",
       "  'of',\n",
       "  'the',\n",
       "  'eye?\"'],\n",
       " ['\"Is', 'it', 'possible', 'to', 'change', 'eye', 'colour', 'naturally?\"'],\n",
       " ['\"What', 'is', 'a', 'good', 'way', 'to', 'learn', 'the', 'violin?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'violin?',\n",
       "  'Is',\n",
       "  'it',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Internet',\n",
       "  '(any',\n",
       "  'source)',\n",
       "  'or',\n",
       "  'through',\n",
       "  'coaching?\"'],\n",
       " ['\"What',\n",
       "  'was',\n",
       "  'the',\n",
       "  'hardest',\n",
       "  'job',\n",
       "  'interview',\n",
       "  'question',\n",
       "  'that',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'asked?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'worst',\n",
       "  'question',\n",
       "  'you',\n",
       "  'have',\n",
       "  'ever',\n",
       "  'been',\n",
       "  'asked',\n",
       "  'in',\n",
       "  'a',\n",
       "  'job',\n",
       "  'interview?\"'],\n",
       " ['\"How', 'can', 'I', 'do', 'gay', 'sex?\"'],\n",
       " ['\"What', 'does', 'gay', 'sex', 'feel', 'like?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'advantages',\n",
       "  'and',\n",
       "  'disadvantages',\n",
       "  'of',\n",
       "  'eating',\n",
       "  'pizza?\"'],\n",
       " ['\"How',\n",
       "  'safe',\n",
       "  'is',\n",
       "  'it',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'raw',\n",
       "  'eggs?',\n",
       "  'Does',\n",
       "  'it',\n",
       "  'benefit',\n",
       "  'your',\n",
       "  'health',\n",
       "  'in',\n",
       "  'any',\n",
       "  'way?\"'],\n",
       " ['\"How',\n",
       "  'effective',\n",
       "  'is',\n",
       "  'scrapping',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes?',\n",
       "  'Will',\n",
       "  'it',\n",
       "  'reduce',\n",
       "  'black',\n",
       "  'money?\"'],\n",
       " ['\"How',\n",
       "  'is',\n",
       "  'discontinuing',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'note',\n",
       "  'going',\n",
       "  'to',\n",
       "  'put',\n",
       "  'a',\n",
       "  'hold',\n",
       "  'on',\n",
       "  'black',\n",
       "  'money',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'demonetizing',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'and',\n",
       "  'introducing',\n",
       "  'new',\n",
       "  '2000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'help',\n",
       "  'curb',\n",
       "  'black',\n",
       "  'money',\n",
       "  'and',\n",
       "  'corruption?\"'],\n",
       " ['\"How',\n",
       "  'Rs.2000',\n",
       "  'currency',\n",
       "  'notes',\n",
       "  'will',\n",
       "  'reduce',\n",
       "  'black',\n",
       "  'money',\n",
       "  'market',\n",
       "  'or',\n",
       "  'corruption?\"'],\n",
       " ['\"Has',\n",
       "  'anyone',\n",
       "  'had',\n",
       "  'an',\n",
       "  'encounter',\n",
       "  'with',\n",
       "  'a',\n",
       "  'ghost',\n",
       "  'in',\n",
       "  'real',\n",
       "  'life?\"'],\n",
       " ['\"Has', 'anyone', 'ever', 'seen', 'a', 'ghost', 'in', 'real', 'life?\"'],\n",
       " ['\"If',\n",
       "  \"'empty'\",\n",
       "  'space',\n",
       "  'is',\n",
       "  'a',\n",
       "  'superfluid',\n",
       "  'which',\n",
       "  'is',\n",
       "  'displaced',\n",
       "  'by',\n",
       "  'matter',\n",
       "  'does',\n",
       "  'this',\n",
       "  'relate',\n",
       "  'GR',\n",
       "  'and',\n",
       "  'QM?\"'],\n",
       " ['\"I',\n",
       "  'have',\n",
       "  'cellulite',\n",
       "  'on',\n",
       "  'my',\n",
       "  'legs',\n",
       "  'which',\n",
       "  'makes',\n",
       "  'me',\n",
       "  'look',\n",
       "  'fat.',\n",
       "  'How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'it?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'cellulite',\n",
       "  'on',\n",
       "  'my',\n",
       "  'thighs',\n",
       "  'and',\n",
       "  'butt?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'over', 'the', 'fear', 'of', 'everything?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'over', 'my', 'fears?\"'],\n",
       " ['\"Can',\n",
       "  'i',\n",
       "  'get',\n",
       "  'pregnant',\n",
       "  'the',\n",
       "  'day',\n",
       "  'after',\n",
       "  'my',\n",
       "  'period',\n",
       "  'ends?\"'],\n",
       " ['\"Can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'pregnant',\n",
       "  '14',\n",
       "  'days',\n",
       "  'after',\n",
       "  'my',\n",
       "  'period',\n",
       "  'started?\"'],\n",
       " ['\"Why', 'is', 'nobody', 'answering', 'my', 'questions', 'in', 'Quora?\"'],\n",
       " ['\"Why', 'are', 'my', 'questions', 'not', 'answered', 'on', 'Quora?\"'],\n",
       " ['\"Should', 'India', 'declare', 'war', 'against', 'Pakistan?\"'],\n",
       " ['\"Should', 'India', 'go', 'through', 'war', 'with', 'Pakistan?\"'],\n",
       " ['\"I',\n",
       "  'want',\n",
       "  'to',\n",
       "  'know',\n",
       "  'people',\n",
       "  'checking',\n",
       "  'on',\n",
       "  'my',\n",
       "  'WhatsApp',\n",
       "  'profile?\"'],\n",
       " ['\"Do',\n",
       "  'we',\n",
       "  'have',\n",
       "  'any',\n",
       "  'trick',\n",
       "  'to',\n",
       "  'know',\n",
       "  'who',\n",
       "  'see',\n",
       "  'your',\n",
       "  'whatsapp',\n",
       "  'profile?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'effective',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'social',\n",
       "  'anxiety?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ways',\n",
       "  'for',\n",
       "  'dealing',\n",
       "  'with',\n",
       "  'social',\n",
       "  'anxiety?\"'],\n",
       " ['\"How', 'do', 'I', 'lose', 'weight', 'ayurvedically?\"'],\n",
       " ['\"How', 'do', 'I', 'actually', 'lose', 'weight?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'find',\n",
       "  'my',\n",
       "  'lost',\n",
       "  'phone',\n",
       "  'using',\n",
       "  'imei',\n",
       "  'number',\n",
       "  'without',\n",
       "  'going',\n",
       "  'to',\n",
       "  'police?\"'],\n",
       " ['\"How', 'do', 'I', 'use', 'the', 'desktop', 'version', 'of', 'Instagram?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'upload',\n",
       "  'pictures',\n",
       "  'from',\n",
       "  'your',\n",
       "  'PC',\n",
       "  'to',\n",
       "  'your',\n",
       "  'Instagram?\"'],\n",
       " ['\"Which', 'is', 'the', 'best', 'QuickBooks', 'error', 'support', 'number?\"'],\n",
       " ['\"How', 'can', 'I', 'reduce', 'my', 'Belly', 'and', 'tummy', 'fat?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'reduce',\n",
       "  'my',\n",
       "  'belly',\n",
       "  'fat',\n",
       "  'through',\n",
       "  'a',\n",
       "  'diet?\"'],\n",
       " ['\"Christianity:',\n",
       "  'What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'Roman',\n",
       "  'Catholics',\n",
       "  'and',\n",
       "  'Protestants?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'a',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'a',\n",
       "  'Catholic',\n",
       "  'and',\n",
       "  'a',\n",
       "  'Roman',\n",
       "  'Catholic?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'pregnant', 'just', 'after', 'my', 'periods?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'get',\n",
       "  'pregnant',\n",
       "  'if',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'is',\n",
       "  'trying',\n",
       "  'just',\n",
       "  '4',\n",
       "  'days',\n",
       "  'before',\n",
       "  'her',\n",
       "  'menstrual',\n",
       "  'cycle?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'over', 'my', 'fear', 'of', 'driving?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'the',\n",
       "  'fear',\n",
       "  'to',\n",
       "  'drive',\n",
       "  'car?\"'],\n",
       " ['\"How', 'do', 'you', 'overcome', 'fear?\"'],\n",
       " ['\"How', 'do', 'I', 'overcome', 'fear', 'of', 'coding?\"'],\n",
       " ['\"Will',\n",
       "  'Rs.',\n",
       "  '2000',\n",
       "  'currency',\n",
       "  'note',\n",
       "  'really',\n",
       "  'come',\n",
       "  'with',\n",
       "  'A',\n",
       "  'GPS',\n",
       "  'chip?',\n",
       "  'Or',\n",
       "  'it',\n",
       "  'is',\n",
       "  'just',\n",
       "  'a',\n",
       "  'rumor?\"'],\n",
       " ['\"Will',\n",
       "  'the',\n",
       "  'new',\n",
       "  'Rs',\n",
       "  '2000',\n",
       "  'notes',\n",
       "  'carry',\n",
       "  'a',\n",
       "  'nano',\n",
       "  'GPS',\n",
       "  'chip?',\n",
       "  'Will',\n",
       "  'it',\n",
       "  'really',\n",
       "  'Help?\"'],\n",
       " ['\"How', 'is', 'it', 'like', 'to', 'have', 'a', 'huge', 'penis?\"'],\n",
       " ['\"What', 'is', 'it', 'like', 'to', 'have', 'a', 'huge', 'penis?\"'],\n",
       " ['\"How', 'can', 'we', 'improve', 'our', 'handwriting?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'ways',\n",
       "  'in',\n",
       "  'which',\n",
       "  'I',\n",
       "  'can',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'handwriting?\"'],\n",
       " ['\"Is', 'Illuminati', 'a', 'real', 'theory?\"'],\n",
       " ['\"Do', 'the', 'Illuminati', 'really', 'exist?\"'],\n",
       " ['\"How', 'should', 'I', 'prepare', 'for', 'a', 'job', 'interview?\"'],\n",
       " ['\"How', 'can', 'I', 'prepare', 'for', 'a', 'job', 'interview?\"'],\n",
       " ['\"Which', 'are', 'the', 'best', 'movies', 'ever?\"'],\n",
       " ['\"What', 'is', 'the', \"world's\", 'best', 'movie?\"'],\n",
       " ['\"How', 'does', 'circleofmoms.com', 'make', 'money?\"'],\n",
       " ['\"How', 'does', 'buddybuild.com', 'make', 'money?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'stop',\n",
       "  'a',\n",
       "  'Terrier/Border',\n",
       "  'Collie',\n",
       "  'mix',\n",
       "  'from',\n",
       "  'destroying',\n",
       "  'your',\n",
       "  'furniture?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'my',\n",
       "  'dog',\n",
       "  'from',\n",
       "  'humping',\n",
       "  'my',\n",
       "  'furniture?\"'],\n",
       " ['\"Does',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'actually',\n",
       "  'want',\n",
       "  'to',\n",
       "  'be',\n",
       "  'the',\n",
       "  'president?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'run',\n",
       "  'for',\n",
       "  'presidency',\n",
       "  'only',\n",
       "  'now?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'make',\n",
       "  'money',\n",
       "  'online',\n",
       "  'with',\n",
       "  'free',\n",
       "  'of',\n",
       "  'cost?\"'],\n",
       " ['\"How', 'do', 'I', 'to', 'make', 'money', 'online?\"'],\n",
       " ['\"How', 'can', 'improve', 'my', 'English', 'speaking?\"'],\n",
       " ['\"What', 'are', 'some', 'ways', 'to', 'improve', 'English?\"'],\n",
       " ['\"Who', 'create', 'Quora?\"'],\n",
       " ['\"Who', 'has', 'invented', 'Quora?\"'],\n",
       " ['\"How',\n",
       "  'will',\n",
       "  'be',\n",
       "  'my',\n",
       "  'career',\n",
       "  'if',\n",
       "  'I',\n",
       "  'chose',\n",
       "  'MBA',\n",
       "  'after',\n",
       "  'b.tech',\n",
       "  'in',\n",
       "  'mechanical',\n",
       "  'engineering?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'all',\n",
       "  'the',\n",
       "  'available',\n",
       "  'options',\n",
       "  'for',\n",
       "  'government',\n",
       "  'jobs',\n",
       "  'after',\n",
       "  'completing',\n",
       "  'B.Tech',\n",
       "  'in',\n",
       "  'mechanical',\n",
       "  'engineering?\"'],\n",
       " ['\"Can', 'anyone', 'hack', 'my', 'mobile', 'through', 'WhatsApp?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'hack',\n",
       "  \"someone's\",\n",
       "  'WhatsApp',\n",
       "  'account',\n",
       "  'if',\n",
       "  'I',\n",
       "  'just',\n",
       "  'have',\n",
       "  'his',\n",
       "  'WhatsApp',\n",
       "  'account',\n",
       "  'number?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'QuickBooks',\n",
       "  'data',\n",
       "  'recovery',\n",
       "  'support',\n",
       "  'number',\n",
       "  'in',\n",
       "  'New',\n",
       "  'York?\"'],\n",
       " ['\"What', 'is', 'the', 'QuickBooks', 'Hosting', 'Support', 'Number?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'the',\n",
       "  'best',\n",
       "  'grades/better',\n",
       "  'grades',\n",
       "  'at',\n",
       "  'school?\"'],\n",
       " ['\"What',\n",
       "  'can',\n",
       "  'I',\n",
       "  'do',\n",
       "  'to',\n",
       "  'get',\n",
       "  'better',\n",
       "  'grades',\n",
       "  'next',\n",
       "  'quarter?\"'],\n",
       " ['\"What', 'is', 'the', 'biggest', 'regret', 'you', 'have?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'your',\n",
       "  'biggest',\n",
       "  'regret',\n",
       "  'for',\n",
       "  'not',\n",
       "  'doing',\n",
       "  'something?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'a', 'thin', 'waist?\"'],\n",
       " ['\"How', 'do', 'you', 'get', 'a', 'smaller', 'waist?\"'],\n",
       " ['\"Which', 'phone', 'is', 'best', 'to', 'buy', 'under', '15000?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'smartphones', 'under', '15000?\"'],\n",
       " ['\"How', 'do', 'I', 'retrieve', 'deleted', 'Snapchat', 'messages?\"'],\n",
       " ['\"How', 'do', 'you', 'delete', 'messages', 'on', 'Snapchat?\"'],\n",
       " ['\"Who',\n",
       "  'will',\n",
       "  'be',\n",
       "  'a',\n",
       "  'better',\n",
       "  'president',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'or',\n",
       "  'Hillary',\n",
       "  'Clinton?\"'],\n",
       " ['\"Why',\n",
       "  'would',\n",
       "  'Hillary',\n",
       "  'Clinton',\n",
       "  'be',\n",
       "  'a',\n",
       "  'better',\n",
       "  'president',\n",
       "  'than',\n",
       "  'Donald',\n",
       "  'Trump?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'over', 'my', 'first', 'break', 'up?\"'],\n",
       " ['\"Dating',\n",
       "  'and',\n",
       "  'Relationships:',\n",
       "  'How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'over',\n",
       "  'a',\n",
       "  'break',\n",
       "  'up?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'full',\n",
       "  'implications',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Indian',\n",
       "  \"government's\",\n",
       "  'decision',\n",
       "  'to',\n",
       "  'withdraw',\n",
       "  'all',\n",
       "  'the',\n",
       "  'old',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'from',\n",
       "  'circulation?\"'],\n",
       " ['\"Do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'Indian',\n",
       "  'governments',\n",
       "  'decision',\n",
       "  'to',\n",
       "  'demonetize',\n",
       "  'the',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'note',\n",
       "  'is',\n",
       "  'a',\n",
       "  'sudden',\n",
       "  'action?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'proof',\n",
       "  'of',\n",
       "  'the',\n",
       "  'existence',\n",
       "  'of',\n",
       "  'aliens?',\n",
       "  'Has',\n",
       "  'anyone',\n",
       "  'seen',\n",
       "  'aliens?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'any',\n",
       "  'scientific',\n",
       "  'proof',\n",
       "  'for',\n",
       "  'the',\n",
       "  'existence',\n",
       "  'of',\n",
       "  'aliens?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'corporate',\n",
       "  'culture',\n",
       "  'like',\n",
       "  'at',\n",
       "  'CMS',\n",
       "  'Energy?',\n",
       "  'How',\n",
       "  'is',\n",
       "  'the',\n",
       "  'culture',\n",
       "  'different',\n",
       "  'than',\n",
       "  'other',\n",
       "  'companies?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'corporate',\n",
       "  'culture',\n",
       "  'like',\n",
       "  'at',\n",
       "  'Atmos',\n",
       "  'Energy?',\n",
       "  'How',\n",
       "  'is',\n",
       "  'the',\n",
       "  'culture',\n",
       "  'different',\n",
       "  'than',\n",
       "  'other',\n",
       "  'companies?\"'],\n",
       " ['\"What', 'are', 'the', 'ways', 'to', 'commit', 'suicide?\"'],\n",
       " ['\"What\\'s',\n",
       "  'the',\n",
       "  'quickest',\n",
       "  'and',\n",
       "  'most',\n",
       "  'painless',\n",
       "  'way',\n",
       "  'to',\n",
       "  'commit',\n",
       "  'suicide?\"'],\n",
       " ['\"What', 'are', 'some', 'facts', 'about', 'Rajdeep', 'Sardesai?\"'],\n",
       " ['\"Why', 'do', 'some', 'people', 'hate', 'Rajdeep', 'Sardesai?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'favorite',\n",
       "  'movies',\n",
       "  'of',\n",
       "  'all',\n",
       "  'time?',\n",
       "  'It',\n",
       "  'could',\n",
       "  'be',\n",
       "  'of',\n",
       "  'any',\n",
       "  'genre.\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'must',\n",
       "  'watch',\n",
       "  'movies',\n",
       "  'to',\n",
       "  'see',\n",
       "  'before',\n",
       "  'you',\n",
       "  'die?\"'],\n",
       " ['\"Why',\n",
       "  'were',\n",
       "  'the',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'demonetized?\"'],\n",
       " ['\"India\\'s',\n",
       "  'Prime',\n",
       "  'Minister',\n",
       "  'removed',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'rupee',\n",
       "  'notes',\n",
       "  'from',\n",
       "  'circulation.',\n",
       "  'Is',\n",
       "  'this',\n",
       "  'a',\n",
       "  'good',\n",
       "  'way',\n",
       "  'to',\n",
       "  'curb',\n",
       "  'the',\n",
       "  'spread',\n",
       "  'of',\n",
       "  'black',\n",
       "  'money?\"'],\n",
       " ['\"How',\n",
       "  'many',\n",
       "  'U.S.',\n",
       "  'Forever',\n",
       "  'stamps',\n",
       "  'do',\n",
       "  'I',\n",
       "  'need',\n",
       "  'to',\n",
       "  'send',\n",
       "  'a',\n",
       "  'letter',\n",
       "  'sized',\n",
       "  'envelope',\n",
       "  'of',\n",
       "  'normal',\n",
       "  'letter',\n",
       "  'thickness',\n",
       "  'and',\n",
       "  'weight',\n",
       "  'to',\n",
       "  'the',\n",
       "  'UK?\"'],\n",
       " ['\"How',\n",
       "  'many',\n",
       "  'stamps',\n",
       "  'does',\n",
       "  'it',\n",
       "  'take',\n",
       "  'to',\n",
       "  'mail',\n",
       "  'a',\n",
       "  'letter',\n",
       "  'from',\n",
       "  'Iowa',\n",
       "  'to',\n",
       "  'Norway?\"'],\n",
       " ['\"How', 'do', 'I', 'place', 'my', 'first', 'question', 'on', 'Quora?\"'],\n",
       " ['\"How', 'do', 'I', 'post', 'a', 'question', 'in', 'quora?\"'],\n",
       " ['\"What', 'are', 'some', 'ways', 'to', 'get', 'a', 'small', 'waist?\"'],\n",
       " ['\"How', 'can', 'I', 'study', 'to', 'get', 'better', 'grades?\"'],\n",
       " ['\"How', 'can', 'one', 'get', 'better', 'grades?\"'],\n",
       " ['\"How', 'do', 'I', 'post', 'a', 'question', 'here?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'a', 'question', 'posted?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'internship', 'at', 'Google', '?\"'],\n",
       " ['\"How', 'do', 'I', 'do', 'internship', 'at', 'google?\"'],\n",
       " ['\"Why', \"doesn't\", 'Quora', 'allow', 'the', 'use', 'of', 'emoticons?\"'],\n",
       " ['\"Why', \"doesn't\", 'Quora', 'update', 'to', 'support', 'emojis?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'enable',\n",
       "  'hardware',\n",
       "  'acceleration',\n",
       "  'in',\n",
       "  'Windows',\n",
       "  '10?\"'],\n",
       " ['\"Windows',\n",
       "  '10',\n",
       "  'issues:',\n",
       "  'wifi',\n",
       "  'is',\n",
       "  'shown',\n",
       "  'as',\n",
       "  'disabled',\n",
       "  'and',\n",
       "  'does',\n",
       "  'not',\n",
       "  'enable',\n",
       "  '.',\n",
       "  'How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'fix',\n",
       "  'this?\"'],\n",
       " ['\"Why', 'India', 'fails', 'to', 'get', 'medals', 'in', 'Olympics?\"'],\n",
       " ['\"Why',\n",
       "  'is',\n",
       "  'India',\n",
       "  'failing',\n",
       "  'so',\n",
       "  'miserably',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Rio',\n",
       "  'Olympics?\"'],\n",
       " ['\"Why', 'is', 'manaphy', 'whining?\"'],\n",
       " ['\"Why', 'is', 'Manaphy', 'complaining?\"'],\n",
       " ['\"Is', 'platinum', 'metal', 'very', 'reactive?\"'],\n",
       " ['\"What', 'are', 'the', 'least', 'reactive', 'metals?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ways',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'writing',\n",
       "  'skills?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'way',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'writing',\n",
       "  'skills?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'more', 'views', 'on', 'youtube', 'video?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'more',\n",
       "  'traffic',\n",
       "  'to',\n",
       "  'my',\n",
       "  'YouTube',\n",
       "  'videos?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'money',\n",
       "  'does',\n",
       "  'the',\n",
       "  'average',\n",
       "  'American',\n",
       "  'make',\n",
       "  'in',\n",
       "  'their',\n",
       "  'lifetime?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'has',\n",
       "  'your',\n",
       "  'salary',\n",
       "  'increased',\n",
       "  'from',\n",
       "  'your',\n",
       "  'first',\n",
       "  'job',\n",
       "  'to',\n",
       "  'today?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'great',\n",
       "  'range',\n",
       "  'of',\n",
       "  'flavours',\n",
       "  'for',\n",
       "  'cupcakes',\n",
       "  'at',\n",
       "  'Gold',\n",
       "  'Coast?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'get',\n",
       "  'good',\n",
       "  'quality',\n",
       "  'cupcakes',\n",
       "  'and',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'different',\n",
       "  'flavor',\n",
       "  'in',\n",
       "  'Gold',\n",
       "  'Coast?\"'],\n",
       " ['\"Which',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'books',\n",
       "  'or',\n",
       "  'learning',\n",
       "  'resources',\n",
       "  'to',\n",
       "  'prepare',\n",
       "  'for',\n",
       "  'the',\n",
       "  'GRE?\"'],\n",
       " ['\"Which',\n",
       "  'books',\n",
       "  'and',\n",
       "  'online',\n",
       "  'sites',\n",
       "  'are',\n",
       "  'best',\n",
       "  'for',\n",
       "  'the',\n",
       "  'GRE',\n",
       "  'and',\n",
       "  'TOEFL',\n",
       "  'preparation?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'Xoma?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'things',\n",
       "  'new',\n",
       "  'employees',\n",
       "  'should',\n",
       "  'know',\n",
       "  'going',\n",
       "  'into',\n",
       "  'their',\n",
       "  'first',\n",
       "  'day',\n",
       "  'at',\n",
       "  'AT&T?\"'],\n",
       " ['\"Can',\n",
       "  'you',\n",
       "  'really',\n",
       "  'see',\n",
       "  'who',\n",
       "  'viewed',\n",
       "  'your',\n",
       "  'pics',\n",
       "  'or',\n",
       "  'profile',\n",
       "  'on',\n",
       "  'instagram?\"'],\n",
       " ['\"Is',\n",
       "  'there',\n",
       "  'app',\n",
       "  'to',\n",
       "  'see',\n",
       "  'who',\n",
       "  'viewed',\n",
       "  'your',\n",
       "  'pictures',\n",
       "  'on',\n",
       "  'Instagram?\"'],\n",
       " ['\"How', 'can', 'we', 'get', 'the', 'best', 'grades', 'in', 'school?\"'],\n",
       " ['\"How', 'can', 'I', 'get', 'the', 'best', 'grades', 'at', 'school?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'ideas',\n",
       "  'of',\n",
       "  'a',\n",
       "  'new',\n",
       "  'business',\n",
       "  'with',\n",
       "  'low',\n",
       "  'investment',\n",
       "  'to',\n",
       "  'start',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'good',\n",
       "  'business',\n",
       "  'ideas',\n",
       "  'with',\n",
       "  'low',\n",
       "  'investment',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What', 'were', 'your', 'impressions', 'of', 'visiting', 'China?\"'],\n",
       " ['\"What', 'is', 'your', 'impression', 'of', 'China?\"'],\n",
       " ['\"How', 'could', 'I', 'improve', 'my', 'writing', 'skill?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'writing',\n",
       "  'skills',\n",
       "  'to',\n",
       "  'write',\n",
       "  'advanced',\n",
       "  'composition?\"'],\n",
       " ['\"Who',\n",
       "  'is',\n",
       "  'going',\n",
       "  'to',\n",
       "  'win',\n",
       "  'the',\n",
       "  '2016',\n",
       "  'presidential',\n",
       "  'election?\"'],\n",
       " ['\"Who', 'will', 'win', 'the', 'Election?', 'Trump', 'or', 'Clinton?\"'],\n",
       " ['\"What', 'can', 'I', 'eat', 'every', 'day', 'to', 'be', 'more', 'healthy?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'healthy',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'a',\n",
       "  'whole',\n",
       "  'avocado',\n",
       "  'every',\n",
       "  'day?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'need',\n",
       "  'for',\n",
       "  'Quora',\n",
       "  'as',\n",
       "  'I',\n",
       "  'already',\n",
       "  'had',\n",
       "  'Google',\n",
       "  'to',\n",
       "  'answer?\"'],\n",
       " ['\"I', 'miss', 'my', 'ex?', 'What', 'should', 'I', 'do?\"'],\n",
       " ['\"I',\n",
       "  'miss',\n",
       "  'my',\n",
       "  'ex',\n",
       "  'so',\n",
       "  'much',\n",
       "  'how',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'thinking',\n",
       "  'about',\n",
       "  'it?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'find',\n",
       "  'suggestions',\n",
       "  'for',\n",
       "  'planning',\n",
       "  'a',\n",
       "  'trip',\n",
       "  'to',\n",
       "  'Goa',\n",
       "  'for',\n",
       "  '3',\n",
       "  'days?\"'],\n",
       " ['\"Can',\n",
       "  'you',\n",
       "  'suggest',\n",
       "  'how',\n",
       "  'to',\n",
       "  'plan',\n",
       "  'Goa',\n",
       "  'trip',\n",
       "  'for',\n",
       "  '4',\n",
       "  'days?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'so',\n",
       "  'many',\n",
       "  'people',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'on',\n",
       "  'Quora',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'found',\n",
       "  'in',\n",
       "  'a',\n",
       "  'Google',\n",
       "  'search?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'on',\n",
       "  'QUORA',\n",
       "  'ask',\n",
       "  'questions',\n",
       "  'that',\n",
       "  'they',\n",
       "  'can',\n",
       "  'easily',\n",
       "  'findout',\n",
       "  'on',\n",
       "  'Google?\"'],\n",
       " ['\"What', 'is', 'main', 'cause', 'of', 'cancer?\"'],\n",
       " ['\"What', 'is', 'the', 'cause', 'of', 'cancer?\"'],\n",
       " ['\"What', 'are', 'some', 'amazing', 'pictures?\"'],\n",
       " ['\"Amazing', 'pictures', 'of', 'earth?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'smoking',\n",
       "  'cigarettes',\n",
       "  'once',\n",
       "  'and',\n",
       "  'for',\n",
       "  'all?\"'],\n",
       " ['\"How', 'do', 'I', 'stop', 'smoking', 'cigarettes?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'install',\n",
       "  'and',\n",
       "  'start',\n",
       "  'up',\n",
       "  'C',\n",
       "  'programming',\n",
       "  'language?\"'],\n",
       " ['\"How',\n",
       "  'and',\n",
       "  'where',\n",
       "  'do',\n",
       "  'I',\n",
       "  'start',\n",
       "  'learning',\n",
       "  'C',\n",
       "  'programming?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'truth',\n",
       "  'behind',\n",
       "  'the',\n",
       "  'death',\n",
       "  'of',\n",
       "  'Subhas',\n",
       "  'Chandra',\n",
       "  'Bose?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'truth',\n",
       "  'is',\n",
       "  'there',\n",
       "  'in',\n",
       "  'the',\n",
       "  'TOI',\n",
       "  'report',\n",
       "  'claiming',\n",
       "  'that',\n",
       "  'Netaji',\n",
       "  'Subhas',\n",
       "  'Chandra',\n",
       "  'Bose',\n",
       "  'wanted',\n",
       "  'ruthless',\n",
       "  'dictatorship',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"Is',\n",
       "  'the',\n",
       "  'new',\n",
       "  'Macbook',\n",
       "  'Pro',\n",
       "  '2016',\n",
       "  'an',\n",
       "  'over',\n",
       "  'priced',\n",
       "  'disappointment?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'of',\n",
       "  'the',\n",
       "  'new',\n",
       "  'MacBook',\n",
       "  'Pro',\n",
       "  'Apple',\n",
       "  'announced',\n",
       "  'in',\n",
       "  '2016?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'time',\n",
       "  'do',\n",
       "  'you',\n",
       "  'spend',\n",
       "  'checking',\n",
       "  'if',\n",
       "  'someone',\n",
       "  'already',\n",
       "  'asked',\n",
       "  'your',\n",
       "  'question',\n",
       "  'before',\n",
       "  'posting',\n",
       "  'it',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"Quora:', 'How', 'do', 'you', 'post', 'a', 'question', 'on', 'Quora?\"'],\n",
       " ['\"Plea',\n",
       "  'Bargaining:',\n",
       "  'Is',\n",
       "  'pleading',\n",
       "  'guilty',\n",
       "  'considered',\n",
       "  'being',\n",
       "  'convicted?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'plead',\n",
       "  'not',\n",
       "  'guilty',\n",
       "  'when',\n",
       "  'I',\n",
       "  'get',\n",
       "  'two',\n",
       "  'driving',\n",
       "  'tickets?\"'],\n",
       " ['\"How', 'can', 'i', 'promote', 'my', 'first', 'android', 'app?\"'],\n",
       " ['\"How', 'can', 'I', 'promote', 'my', 'Android', 'app?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'repercussions',\n",
       "  'of',\n",
       "  'banning',\n",
       "  'Rs',\n",
       "  '500',\n",
       "  'and',\n",
       "  'Rs',\n",
       "  '1000',\n",
       "  'notes',\n",
       "  'on',\n",
       "  'Indian',\n",
       "  'economy?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'views',\n",
       "  'on',\n",
       "  'India',\n",
       "  'banning',\n",
       "  '500',\n",
       "  'and',\n",
       "  '1000',\n",
       "  'notes?',\n",
       "  'In',\n",
       "  'what',\n",
       "  'way',\n",
       "  'it',\n",
       "  'will',\n",
       "  'affect',\n",
       "  'Indian',\n",
       "  'economy?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'App',\n",
       "  '(Application)',\n",
       "  'and',\n",
       "  'Software?\"'],\n",
       " ['\"What', 'is', 'the', 'difference', 'between', 'app', 'and', 'software?\"'],\n",
       " ['\"When',\n",
       "  'will',\n",
       "  'Apple',\n",
       "  'release',\n",
       "  'the',\n",
       "  'new',\n",
       "  'MacBook',\n",
       "  'Pro',\n",
       "  'in',\n",
       "  '2016?\"'],\n",
       " ['\"When',\n",
       "  'will',\n",
       "  'Apple',\n",
       "  'launch',\n",
       "  'new',\n",
       "  'MacBook',\n",
       "  'Pro?',\n",
       "  'Is',\n",
       "  'it',\n",
       "  'in',\n",
       "  '2016?\"'],\n",
       " ['\"Is',\n",
       "  'dark/vacuum',\n",
       "  'energy',\n",
       "  'infinite',\n",
       "  'because',\n",
       "  'the',\n",
       "  'expansion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'universe',\n",
       "  'is',\n",
       "  'infinite',\n",
       "  'and',\n",
       "  'more',\n",
       "  'and',\n",
       "  'more',\n",
       "  'of',\n",
       "  'it',\n",
       "  'is',\n",
       "  'created',\n",
       "  'as',\n",
       "  'the',\n",
       "  'universe',\n",
       "  'expands?\"'],\n",
       " ['\"If',\n",
       "  'universe',\n",
       "  'is',\n",
       "  'expanding',\n",
       "  'without',\n",
       "  'a',\n",
       "  'limit',\n",
       "  'and',\n",
       "  'dark',\n",
       "  'and',\n",
       "  'vacuum',\n",
       "  'energy',\n",
       "  'are',\n",
       "  'created',\n",
       "  'as',\n",
       "  'it',\n",
       "  'expands?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'lies',\n",
       "  'that',\n",
       "  'society',\n",
       "  'tells',\n",
       "  'us?\"'],\n",
       " ['\"Why', 'do', 'I', 'have', 'to', 'lie', 'so', 'much?\"'],\n",
       " ['\"How', 'can', 'I', 'master', 'Java', 'in', 'one', 'month?\"'],\n",
       " ['\"How',\n",
       "  'much',\n",
       "  'Java',\n",
       "  'do',\n",
       "  'you',\n",
       "  'need',\n",
       "  'to',\n",
       "  'know',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'job?\"'],\n",
       " ['\"What', 'do', 'the', 'Pakistanis', 'think', 'about', 'India?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'the',\n",
       "  'people',\n",
       "  'from',\n",
       "  'Pakistan',\n",
       "  'think',\n",
       "  'about',\n",
       "  'Indians?\"'],\n",
       " ['\"What',\n",
       "  'can',\n",
       "  'a',\n",
       "  'Tinder',\n",
       "  'scammer',\n",
       "  'do',\n",
       "  'with',\n",
       "  'my',\n",
       "  'phone',\n",
       "  'number?\"'],\n",
       " ['\"Can', 'you', 'find', 'someone', 'on', 'Tinder?\"'],\n",
       " ['\"Why',\n",
       "  'do',\n",
       "  'some',\n",
       "  'people',\n",
       "  'believe',\n",
       "  'everything',\n",
       "  'has',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'for',\n",
       "  'a',\n",
       "  'reason?\"'],\n",
       " ['\"How',\n",
       "  'vulnerable',\n",
       "  'are',\n",
       "  'paratroopers',\n",
       "  'on',\n",
       "  'their',\n",
       "  'descent?',\n",
       "  'If',\n",
       "  'they',\n",
       "  'are',\n",
       "  'fired',\n",
       "  'at',\n",
       "  'can',\n",
       "  'they',\n",
       "  'fire',\n",
       "  'back?\"'],\n",
       " ['\"How', 'friendly', 'are', 'friendly', 'fires?\"'],\n",
       " ['\"Why',\n",
       "  'is',\n",
       "  'Manaphy',\n",
       "  'angsty',\n",
       "  'in',\n",
       "  'Pokemon',\n",
       "  'Ranger',\n",
       "  'and',\n",
       "  'The',\n",
       "  'Temple',\n",
       "  'Of',\n",
       "  'The',\n",
       "  'Sea?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'Manaphy',\n",
       "  'had',\n",
       "  'a',\n",
       "  'panic',\n",
       "  'attack',\n",
       "  'in',\n",
       "  'Pokémon',\n",
       "  'ranger',\n",
       "  'and',\n",
       "  'the',\n",
       "  'temple',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sea?\"'],\n",
       " ['\"When',\n",
       "  'is',\n",
       "  'the',\n",
       "  'new',\n",
       "  'Apple',\n",
       "  'Macbook',\n",
       "  'Pro',\n",
       "  'coming?',\n",
       "  'In',\n",
       "  '2016?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'typical',\n",
       "  'career',\n",
       "  'path',\n",
       "  'for',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'becomes',\n",
       "  'a',\n",
       "  'professor',\n",
       "  'in',\n",
       "  'education?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'typical',\n",
       "  'career',\n",
       "  'path',\n",
       "  'for',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'becomes',\n",
       "  'a',\n",
       "  'professor',\n",
       "  'in',\n",
       "  'statistics?\"'],\n",
       " ['\"My',\n",
       "  'son',\n",
       "  'plays',\n",
       "  'Minecraft',\n",
       "  'on',\n",
       "  'the',\n",
       "  'computer.',\n",
       "  'The',\n",
       "  'other',\n",
       "  'day',\n",
       "  'he',\n",
       "  'was',\n",
       "  'playing',\n",
       "  'online',\n",
       "  'and',\n",
       "  'someone',\n",
       "  'asked',\n",
       "  'him',\n",
       "  '\"\"d',\n",
       "  'or',\n",
       "  't.\"\"',\n",
       "  'What',\n",
       "  'does',\n",
       "  'this',\n",
       "  'mean?\"'],\n",
       " ['\"Can', 'I', 'play', 'Minecraft', 'for', 'free?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'formula',\n",
       "  'for',\n",
       "  'determining',\n",
       "  'density',\n",
       "  'of',\n",
       "  'a',\n",
       "  'mixed',\n",
       "  'sample?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'you',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'weight',\n",
       "  'density',\n",
       "  'formula?',\n",
       "  'How',\n",
       "  'is',\n",
       "  'it',\n",
       "  'used?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'books',\n",
       "  'on',\n",
       "  'algorithms',\n",
       "  'and',\n",
       "  'data',\n",
       "  'structures?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'books', 'for', 'algorithm?\"'],\n",
       " ['\"How', 'can', 'you', 'lose', 'weight', 'quickly?\"'],\n",
       " ['\"How', 'do', 'I', 'lose', 'weight', 'fast?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'venture',\n",
       "  'capital',\n",
       "  'firms',\n",
       "  'that',\n",
       "  'focus',\n",
       "  'on',\n",
       "  'early',\n",
       "  'stage',\n",
       "  'investments',\n",
       "  'in',\n",
       "  'Italy?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'venture',\n",
       "  'capital',\n",
       "  'firms',\n",
       "  'that',\n",
       "  'focus',\n",
       "  'on',\n",
       "  'early',\n",
       "  'stage',\n",
       "  'investments',\n",
       "  'in',\n",
       "  'Denmark?\"'],\n",
       " ['\"What', 'should', 'I', 'do', 'to', 'improve', 'my', 'tennis?\"'],\n",
       " ['\"How', 'can', 'I', 'get', 'better', 'at', 'tennis?\"'],\n",
       " ['\"How', 'can', 'we', 'learn', 'faster?\"'],\n",
       " ['\"How', 'can', 'I', 'learn', 'at', 'a', 'higher', 'speed?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'concentrate',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'when',\n",
       "  'I',\n",
       "  'sit',\n",
       "  'to',\n",
       "  'study?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'tips',\n",
       "  'to',\n",
       "  'concentrate',\n",
       "  'when',\n",
       "  'reading',\n",
       "  'or',\n",
       "  'at',\n",
       "  'work?',\n",
       "  'I',\n",
       "  'find',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'wanders',\n",
       "  'and',\n",
       "  'I',\n",
       "  'know',\n",
       "  'I',\n",
       "  'am',\n",
       "  'not',\n",
       "  'being',\n",
       "  'as',\n",
       "  'productive',\n",
       "  'as',\n",
       "  'I',\n",
       "  'should',\n",
       "  'be.\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'my',\n",
       "  'dachshund',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'chewing',\n",
       "  'furniture?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'stop',\n",
       "  'my',\n",
       "  'puppy',\n",
       "  'from',\n",
       "  'chewing',\n",
       "  'my',\n",
       "  'shoes?\"'],\n",
       " ['\"What', 'have', 'you', 'learnt', 'from', 'your', 'life', 'until', 'now?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'Some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'life',\n",
       "  'lesson',\n",
       "  'that',\n",
       "  'you',\n",
       "  'ever',\n",
       "  'learnt',\n",
       "  'in',\n",
       "  'your',\n",
       "  'life?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'you',\n",
       "  'explain',\n",
       "  'quantum',\n",
       "  'mechanics',\n",
       "  'to',\n",
       "  'a',\n",
       "  '12',\n",
       "  'year',\n",
       "  'old?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'you',\n",
       "  'explain',\n",
       "  'quantum',\n",
       "  'mechanics',\n",
       "  'to',\n",
       "  'a',\n",
       "  '12',\n",
       "  'year',\n",
       "  'old?\"'],\n",
       " ['\"How', 'do', 'you', 'gain', 'followers', 'on', 'Twitter?\"'],\n",
       " ['\"How', 'do', 'I', 'gain', 'more', 'twitter', 'followers', 'fast?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'I',\n",
       "  'stratify',\n",
       "  'my',\n",
       "  'preparations',\n",
       "  'for',\n",
       "  'the',\n",
       "  'CAT',\n",
       "  '2017?\"'],\n",
       " ['\"How', 'can', 'I', 'prepare', 'for', 'CAT', '2017?\"'],\n",
       " ['\"What', 'is', 'the', 'exact', 'meaning', 'of', 'love?\"'],\n",
       " ['\"What', 'is', 'real', 'meaning', 'of', 'love?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'foods',\n",
       "  'that',\n",
       "  'start',\n",
       "  'with',\n",
       "  'the',\n",
       "  'letter',\n",
       "  'I?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'five',\n",
       "  'letter',\n",
       "  'words',\n",
       "  'with',\n",
       "  '\"\"a\"\"',\n",
       "  'as',\n",
       "  'the',\n",
       "  'third',\n",
       "  'letter?\"'],\n",
       " ['\"Is', 'Zee', 'news', 'a', 'BJP', 'owned', 'channel?\"'],\n",
       " ['\"Why', 'is', 'ZEE', 'News', 'so', 'anti-AAP?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'your',\n",
       "  'creative',\n",
       "  'New',\n",
       "  \"Year's\",\n",
       "  'resolution',\n",
       "  'for',\n",
       "  '2017?\"'],\n",
       " ['\"What', 'is', 'your', 'New', 'Year', 'Resolution', 'for', '2017?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'a', 'slim', 'face?\"'],\n",
       " ['\"How', 'can', 'I', 'slim', 'my', 'face', 'naturally?\"'],\n",
       " ['\"Is',\n",
       "  'the',\n",
       "  'force',\n",
       "  'of',\n",
       "  'gravity',\n",
       "  'a',\n",
       "  'scalar',\n",
       "  'or',\n",
       "  'a',\n",
       "  'vector?\"'],\n",
       " ['\"Is',\n",
       "  'time',\n",
       "  'a',\n",
       "  'scalar',\n",
       "  'quantity',\n",
       "  'or',\n",
       "  'vector',\n",
       "  'quantity?',\n",
       "  'Why?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'resolutions',\n",
       "  'you',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'take',\n",
       "  'for',\n",
       "  'the',\n",
       "  'upcoming',\n",
       "  'New',\n",
       "  'year',\n",
       "  '2017?\"'],\n",
       " ['\"What\\'s', 'are', 'your', 'resolutions', 'for', '2017?\"'],\n",
       " ['\"Which',\n",
       "  'subjects',\n",
       "  'are',\n",
       "  'important',\n",
       "  'to',\n",
       "  'become',\n",
       "  'a',\n",
       "  'chartered',\n",
       "  'accountant?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'become',\n",
       "  'a',\n",
       "  'Chartered',\n",
       "  'Accountant',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"How', 'is', 'Bhagavad', 'Geeta', 'relevant', 'in', 'real', 'life?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'you',\n",
       "  'relate',\n",
       "  'the',\n",
       "  'Mahabharat',\n",
       "  'and',\n",
       "  'Bhagvad',\n",
       "  'Geeta',\n",
       "  'to',\n",
       "  'your',\n",
       "  'life?\"'],\n",
       " ['\"Can', 'you', 'wear', 'sterling', 'silver', 'in', 'water?\"'],\n",
       " ['\"How', 'do', 'you', 'identify', 'real', '925', 'sterling', 'silver?\"'],\n",
       " ['\"Which',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'open',\n",
       "  'source',\n",
       "  'projects',\n",
       "  'one',\n",
       "  'should',\n",
       "  'initially',\n",
       "  'start',\n",
       "  'contributing',\n",
       "  'to?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'easy',\n",
       "  'and',\n",
       "  'fun',\n",
       "  'open',\n",
       "  'source',\n",
       "  'or',\n",
       "  'free',\n",
       "  'software',\n",
       "  'projects',\n",
       "  'to',\n",
       "  'work',\n",
       "  'on?\"'],\n",
       " ['\"How', 'would', 'one', 'use', '“sort', 'of”', 'in', 'a', 'sentence?\"'],\n",
       " ['\"How', 'do', 'I', 'use', '\"\"lest\"\"', 'in', 'a', 'sentence?\"'],\n",
       " ['\"What', 'app', 'for', 'music', 'without', 'wifi', 'for', 'iPod?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'listen',\n",
       "  'to',\n",
       "  'music',\n",
       "  'offline',\n",
       "  'without',\n",
       "  'it',\n",
       "  'stopping',\n",
       "  'when',\n",
       "  'I',\n",
       "  'turn',\n",
       "  'off',\n",
       "  'the',\n",
       "  'device',\n",
       "  'or',\n",
       "  'open',\n",
       "  'up',\n",
       "  'another',\n",
       "  'app',\n",
       "  'or',\n",
       "  'tab?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'working',\n",
       "  'netcut',\n",
       "  'defender',\n",
       "  'android',\n",
       "  'applications',\n",
       "  'without',\n",
       "  'root?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'hide',\n",
       "  'apps',\n",
       "  'on',\n",
       "  'an',\n",
       "  'Android',\n",
       "  'phone',\n",
       "  'without',\n",
       "  'rooting',\n",
       "  'in',\n",
       "  'any',\n",
       "  'version',\n",
       "  'of',\n",
       "  'your',\n",
       "  'Android?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'U.S.',\n",
       "  'including',\n",
       "  'it’s',\n",
       "  'territories',\n",
       "  'and',\n",
       "  'possessions?',\n",
       "  '3\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Maryland?\"'],\n",
       " ['\"What', 'are', 'some', 'of', 'the', 'best', 'places', 'to', 'visit?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'places', 'to', 'visit', 'in', 'banglore?\"'],\n",
       " ['\"Which', 'is', 'the', 'best', 'online', 'IQ', 'test', 'available?\"'],\n",
       " ['\"Where',\n",
       "  'can',\n",
       "  'I',\n",
       "  'find',\n",
       "  'a',\n",
       "  'good',\n",
       "  'and',\n",
       "  'free',\n",
       "  'IQ',\n",
       "  'online',\n",
       "  'test?\"'],\n",
       " ['\"What', 'is', 'primary', 'purpose', 'of', 'life?\"'],\n",
       " ['\"What', 'purpose', 'do', 'you', 'find', 'in', 'life?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'true',\n",
       "  'that',\n",
       "  'India',\n",
       "  'is',\n",
       "  'faking',\n",
       "  'a',\n",
       "  'surgical',\n",
       "  'strike?\"'],\n",
       " ['\"Has', 'India', 'ever', 'held', 'a', 'surgical', 'strike?\"'],\n",
       " ['\"Why',\n",
       "  'are',\n",
       "  'so',\n",
       "  'many',\n",
       "  'people',\n",
       "  'obsessed',\n",
       "  'with',\n",
       "  'the',\n",
       "  'flat',\n",
       "  'Earth',\n",
       "  'theory?\"'],\n",
       " ['\"Why', 'is', 'there', 'the', 'flat', 'Earth', 'debate?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'find',\n",
       "  'out',\n",
       "  \"who's\",\n",
       "  'stealing',\n",
       "  'my',\n",
       "  'lunch',\n",
       "  'from',\n",
       "  'the',\n",
       "  'office',\n",
       "  'fridge?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'work',\n",
       "  'out',\n",
       "  'to',\n",
       "  'do',\n",
       "  'in',\n",
       "  'an',\n",
       "  'average',\n",
       "  'office',\n",
       "  'during',\n",
       "  'the',\n",
       "  'lunch',\n",
       "  'hour?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'quickly',\n",
       "  'and',\n",
       "  'efficiently',\n",
       "  'learn',\n",
       "  'a',\n",
       "  'new',\n",
       "  'language?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'method', 'to', 'learn', 'language?\"'],\n",
       " ['\"How', 'did', 'Hitler', 'come', 'to', 'power?\"'],\n",
       " ['\"How', 'close', 'did', 'Hitler', 'come', 'to', 'victory', 'in', 'WW2?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'horror', 'movies?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'best',\n",
       "  'online',\n",
       "  'test',\n",
       "  'series',\n",
       "  'for',\n",
       "  'bank',\n",
       "  'exams?\"'],\n",
       " ['\"Which',\n",
       "  'one',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'online',\n",
       "  'test',\n",
       "  'series',\n",
       "  'for',\n",
       "  'bank',\n",
       "  'exams?\"'],\n",
       " ['\"What', 'are', '5', 'of', 'your', 'favorite', 'songs?\"'],\n",
       " ['\"What', 'is', 'your', 'favorite', 'song', 'of', '2016?\"'],\n",
       " ['\"What', 'is', 'the', 'purpose', 'of', 'hymen?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'point',\n",
       "  'of',\n",
       "  'living',\n",
       "  'if',\n",
       "  'we',\n",
       "  'are',\n",
       "  'going',\n",
       "  'to',\n",
       "  'die',\n",
       "  'and',\n",
       "  'not',\n",
       "  'remember',\n",
       "  'anything?\"'],\n",
       " ['\"Why',\n",
       "  'is',\n",
       "  'india',\n",
       "  'still',\n",
       "  'a',\n",
       "  'developing',\n",
       "  'country..Why',\n",
       "  \"aren't\",\n",
       "  'reforms',\n",
       "  'framed',\n",
       "  'so',\n",
       "  'as',\n",
       "  'bring',\n",
       "  'the',\n",
       "  'country',\n",
       "  'on',\n",
       "  'a',\n",
       "  'fast',\n",
       "  'track?\"'],\n",
       " ['\"Why',\n",
       "  'ia',\n",
       "  'India',\n",
       "  'still',\n",
       "  'considered',\n",
       "  'as',\n",
       "  'a',\n",
       "  'developing',\n",
       "  'country?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'American',\n",
       "  'people',\n",
       "  'elect',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'as',\n",
       "  'their',\n",
       "  'president?\"'],\n",
       " ['\"How',\n",
       "  'did',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'got',\n",
       "  'elected',\n",
       "  'when',\n",
       "  'there',\n",
       "  'are',\n",
       "  'so',\n",
       "  'many',\n",
       "  'people',\n",
       "  'against',\n",
       "  'him?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'you',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'Lewis',\n",
       "  'structure',\n",
       "  'for',\n",
       "  'O2?\"'],\n",
       " ['\"How', 'is', 'the', 'Lewis', 'structure', 'for', 'cyanide', 'determined?\"'],\n",
       " ['\"Why',\n",
       "  'is',\n",
       "  '1',\n",
       "  'GB',\n",
       "  'of',\n",
       "  'RAM',\n",
       "  'enough',\n",
       "  'for',\n",
       "  'an',\n",
       "  'iPhone',\n",
       "  'or',\n",
       "  'Windows',\n",
       "  'phone',\n",
       "  'but',\n",
       "  'not',\n",
       "  'an',\n",
       "  'Android?\"'],\n",
       " ['\"Why',\n",
       "  'is',\n",
       "  'the',\n",
       "  \"iPhone's\",\n",
       "  '1',\n",
       "  'GB',\n",
       "  'RAM',\n",
       "  'touted',\n",
       "  'to',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'compete',\n",
       "  'with',\n",
       "  'more',\n",
       "  'than',\n",
       "  '2',\n",
       "  'GB',\n",
       "  'RAM',\n",
       "  'of',\n",
       "  'Android',\n",
       "  'phones?\"'],\n",
       " ['\"Could',\n",
       "  'time',\n",
       "  'travel',\n",
       "  'be',\n",
       "  'a',\n",
       "  'real',\n",
       "  'thing?',\n",
       "  'Could',\n",
       "  'it',\n",
       "  'be',\n",
       "  'scientifically',\n",
       "  'explained?\"'],\n",
       " ['\"Is', 'it', 'possible', 'to', 'travel', 'time', 'with', 'real', 'life?\"'],\n",
       " ['\"How', 'profitable', 'are', 'TV', 'channels', 'in', 'the', 'US?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'start',\n",
       "  'my',\n",
       "  'own',\n",
       "  'TV',\n",
       "  'channel',\n",
       "  'in',\n",
       "  'the',\n",
       "  'US?\"'],\n",
       " ['\"Should', 'hamsters', 'eat', 'popcorn?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'hamsters',\n",
       "  'like',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'besides',\n",
       "  'vegetables',\n",
       "  'and',\n",
       "  'hamster',\n",
       "  'food?\"'],\n",
       " ['\"How', 'do', 'i', 'get', 'my', 'ex', 'back?', 'Read', 'details\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'my', 'ex', 'back?\"'],\n",
       " ['\"Is',\n",
       "  'the',\n",
       "  'superfluid',\n",
       "  'dark',\n",
       "  'matter',\n",
       "  'what',\n",
       "  'waves',\n",
       "  'in',\n",
       "  'a',\n",
       "  'double',\n",
       "  'slit',\n",
       "  'experiment?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'you',\n",
       "  'would',\n",
       "  'never',\n",
       "  'change',\n",
       "  'about',\n",
       "  'yourself?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'that',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'you',\n",
       "  'can',\n",
       "  'never',\n",
       "  'change',\n",
       "  'about',\n",
       "  'yourself?\"'],\n",
       " ['\"How', 'can', 'you', 'recover', 'your', 'Gmail', 'password?\"'],\n",
       " ['\"How', 'can', 'I', 'recover', 'my', 'Gmail', \"account's\", 'password?\"'],\n",
       " ['\"What', 'makes', 'anal', 'sex', 'pleasurable', 'to', 'some', 'people?\"'],\n",
       " ['\"Is', 'anal', 'sex', 'enjoyable?\"'],\n",
       " ['\"What', 'are', 'best', 'job', 'portals', 'in', 'india?\"'],\n",
       " ['\"What', 'are', 'the', 'top', 'job', 'portals', 'in', 'India?\"'],\n",
       " ['\"How', 'do', 'I', 'delete', 'a', 'Quora', 'question?\"'],\n",
       " ['\"Why', \"can't\", 'I', 'delete', 'my', 'own', 'Quora', 'questions?\"'],\n",
       " ['\"What', 'is', 'Icloud', 'Customer', 'Service', 'Phone', 'Number', '?\"'],\n",
       " ['\"How', 'can', 'I', 'contact', 'Facebook?\"'],\n",
       " ['\"Who',\n",
       "  'will',\n",
       "  'win',\n",
       "  'in',\n",
       "  'a',\n",
       "  'war',\n",
       "  'between',\n",
       "  'India',\n",
       "  'and',\n",
       "  'Pakistan?\"'],\n",
       " ['\"If',\n",
       "  'war',\n",
       "  'happens',\n",
       "  'between',\n",
       "  'India',\n",
       "  'and',\n",
       "  'Pakistan',\n",
       "  'who',\n",
       "  'will',\n",
       "  'win?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'stop',\n",
       "  'a',\n",
       "  'English',\n",
       "  'Bulldog/Pitbull',\n",
       "  'mix',\n",
       "  'puppy',\n",
       "  'from',\n",
       "  'biting',\n",
       "  'my',\n",
       "  'shoes?\"'],\n",
       " ['\"What',\n",
       "  'universities',\n",
       "  'does',\n",
       "  'B&G',\n",
       "  'Foods',\n",
       "  'recruit',\n",
       "  'new',\n",
       "  'grads',\n",
       "  'from?',\n",
       "  'What',\n",
       "  'majors',\n",
       "  'are',\n",
       "  'they',\n",
       "  'looking',\n",
       "  'for?\"'],\n",
       " ['\"What',\n",
       "  'universities',\n",
       "  'does',\n",
       "  'ConAgra',\n",
       "  'Foods',\n",
       "  'recruit',\n",
       "  'new',\n",
       "  'grads',\n",
       "  'from?',\n",
       "  'What',\n",
       "  'majors',\n",
       "  'are',\n",
       "  'they',\n",
       "  'looking',\n",
       "  'for?\"'],\n",
       " ['\"How', 'do', 'I', 'earn', 'from', 'Quora?\"'],\n",
       " ['\"How', 'can', 'you', 'earn', 'a', 'living', 'on', 'Quora?\"'],\n",
       " ['\"Is',\n",
       "  'it',\n",
       "  'physically',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'travel',\n",
       "  'back',\n",
       "  'in',\n",
       "  'time?\"'],\n",
       " ['\"Why', 'would', 'Donald', 'Trump', 'make', 'a', 'good', 'president?\"'],\n",
       " ['\"Will', 'Trump', 'will', 'make', 'a', 'great', 'POTUS?\"'],\n",
       " ['\"How', 'can', 'green', 'tea', 'help', 'you', 'reduce', 'belly', 'fat?\"'],\n",
       " ['\"How', 'Green', 'tea', 'is', 'useful', 'for', 'reducing', 'fat?\"'],\n",
       " ['\"What', 'are', 'your', 'new', 'year', 'resolutions', 'for', '2017?\"'],\n",
       " ['\"What', 'is', 'your', 'New', 'Year’s', 'Resolution(s)', 'for', '2017?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'Indian',\n",
       "  'government',\n",
       "  'scrap',\n",
       "  'Rs',\n",
       "  '1000',\n",
       "  'and',\n",
       "  '500',\n",
       "  'note',\n",
       "  'and',\n",
       "  'instead',\n",
       "  'is',\n",
       "  'introducing',\n",
       "  'Rs',\n",
       "  '2000',\n",
       "  'note?\"'],\n",
       " ['\"Did',\n",
       "  'the',\n",
       "  'Indian',\n",
       "  'government',\n",
       "  'ban',\n",
       "  'the',\n",
       "  '500',\n",
       "  'Rs',\n",
       "  '&',\n",
       "  '1000',\n",
       "  'rupees',\n",
       "  'notes?\"'],\n",
       " ['\"How', 'can', 'I', 'know', 'if', 'I', 'made', 'the', 'right', 'decision?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'know',\n",
       "  'that',\n",
       "  'you',\n",
       "  'have',\n",
       "  'made',\n",
       "  'the',\n",
       "  'right',\n",
       "  'decision?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'add',\n",
       "  'question',\n",
       "  'details',\n",
       "  'beyond',\n",
       "  'the',\n",
       "  'usual',\n",
       "  'character',\n",
       "  'limit?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'to',\n",
       "  'use',\n",
       "  'more',\n",
       "  'than',\n",
       "  '300',\n",
       "  'characters',\n",
       "  'when',\n",
       "  'asking',\n",
       "  'questions',\n",
       "  'on',\n",
       "  'Quora?\"'],\n",
       " ['\"Why', \"can't\", 'planets', 'be', 'different', 'shapes?\"'],\n",
       " ['\"Why', 'are', 'planets', 'spherical?\"'],\n",
       " ['\"Is',\n",
       "  'the',\n",
       "  'demonetisation',\n",
       "  'policy',\n",
       "  'of',\n",
       "  'Modi',\n",
       "  'a',\n",
       "  'good',\n",
       "  'move',\n",
       "  'to',\n",
       "  'curb',\n",
       "  'black',\n",
       "  'money',\n",
       "  'and',\n",
       "  'corruption?\"'],\n",
       " ['\"Is',\n",
       "  'money',\n",
       "  'demonetisation',\n",
       "  'really',\n",
       "  'working',\n",
       "  'for',\n",
       "  'black',\n",
       "  'money?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Illinois?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'precautions',\n",
       "  'on',\n",
       "  'handling',\n",
       "  'shotguns',\n",
       "  'proposed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'NRA',\n",
       "  'in',\n",
       "  'Washington?\"'],\n",
       " ['\"Do', 'girls', 'like', 'nerdy', 'boys?\"'],\n",
       " ['\"Do', 'girls', 'like', 'geeky', 'and', 'nerdy', 'guys?\"'],\n",
       " ['\"What', 'are', 'the', 'best', 'phone', 'under', '10000', 'rupees?\"'],\n",
       " ['\"Which', 'is', 'best', 'smartphone', 'under', '10k?\"'],\n",
       " ['\"Why',\n",
       "  'does',\n",
       "  'it',\n",
       "  'seem',\n",
       "  'that',\n",
       "  'the',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'persons',\n",
       "  'using',\n",
       "  'Quora',\n",
       "  'are',\n",
       "  'politically',\n",
       "  'liberal?\"'],\n",
       " ['\"Why', 'do', 'people', 'think', 'Quora', 'is', 'so', 'liberal?\"'],\n",
       " ['\"Is', 'the', 'theory', 'of', 'evolution', 'not', 'falsifiable?\"'],\n",
       " ['\"What', 'is', 'the', 'theory', 'of', 'evolution?\"'],\n",
       " ['\"What', 'is', 'your', 'impression', 'on', 'China?\"'],\n",
       " ['\"Why', 'is', 'the', 'Thames', 'pronounced', 'the', 'way', 'it', 'is?\"'],\n",
       " ['\"What', 'is', 'the', 'best', 'way', 'to', 'pronounce', 'a', 'death?\"'],\n",
       " ['\"What', 'do', 'you', 'do', 'when', 'you', 'feel', 'depressed?\"'],\n",
       " ['\"What', 'can', 'I', 'do', 'when', 'I', 'feel', 'depressed?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'steps',\n",
       "  'to',\n",
       "  'create',\n",
       "  'a',\n",
       "  'page',\n",
       "  'in',\n",
       "  'Hybris',\n",
       "  'via',\n",
       "  'HMC?\"'],\n",
       " ['\"What', 'is', 'SAP', 'hybris?\"'],\n",
       " ['\"Why’s', 'watching', 'and', 'playing', 'snooker', 'different?\"'],\n",
       " ['\"Why\\'s',\n",
       "  'watching',\n",
       "  'snooker',\n",
       "  'always',\n",
       "  'different',\n",
       "  'to',\n",
       "  'playing',\n",
       "  'it',\n",
       "  'myself?\"'],\n",
       " ['\"What', 'is', 'the', 'value', 'of', 'human', 'life?\"'],\n",
       " ['\"What’s', 'the', 'value', 'of', 'your', 'life?\"'],\n",
       " ['\"How', 'can', 'I', 'become', 'a', 'race', 'car', 'or', 'F1', 'driver?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'become',\n",
       "  'an',\n",
       "  'F1',\n",
       "  'driver',\n",
       "  'when',\n",
       "  'you',\n",
       "  'have',\n",
       "  'limited',\n",
       "  'resources',\n",
       "  'and',\n",
       "  'no',\n",
       "  'racing',\n",
       "  'history?\"'],\n",
       " ['\"What', 'are', 'some', 'reasons', 'an', 'iPhone', 'might', 'not', 'ring?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'some',\n",
       "  'common',\n",
       "  'reasons',\n",
       "  'that',\n",
       "  'your',\n",
       "  'iPhone',\n",
       "  'will',\n",
       "  'not',\n",
       "  'ring?\"'],\n",
       " ['\"What',\n",
       "  'workout',\n",
       "  'clothes',\n",
       "  'did',\n",
       "  'guys',\n",
       "  'wear',\n",
       "  'in',\n",
       "  'the',\n",
       "  'summer',\n",
       "  'back',\n",
       "  'in',\n",
       "  'the',\n",
       "  'year',\n",
       "  '1990?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'I',\n",
       "  'dress',\n",
       "  'like',\n",
       "  'a',\n",
       "  'workout',\n",
       "  'guy',\n",
       "  'from',\n",
       "  'the',\n",
       "  'year',\n",
       "  '1990?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'trick',\n",
       "  'to',\n",
       "  'maintaining',\n",
       "  'long',\n",
       "  'distance',\n",
       "  'relationships?\"'],\n",
       " ['\"How', 'are', 'long', 'distance', 'relationships', 'maintained?\"'],\n",
       " ['\"Why', 'MS', 'Dhoni', 'leave', 'captaincy', 'of', 'ODI', '&', 'T-20?\"'],\n",
       " ['\"Why',\n",
       "  'did',\n",
       "  'M.S.Dhoni',\n",
       "  'left',\n",
       "  'captaincy',\n",
       "  'from',\n",
       "  'ODI',\n",
       "  '&',\n",
       "  'T20?\"'],\n",
       " ['\"Which',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'laptop',\n",
       "  'to',\n",
       "  'purchase',\n",
       "  'under',\n",
       "  'INR',\n",
       "  '60K?\"'],\n",
       " ['\"Which', 'is', 'the', 'best', 'laptop', 'for', '60k?\"'],\n",
       " ['\"How', 'can', 'I', 'hone', 'my', 'writing', 'skills?\"'],\n",
       " ['\"How', 'do', 'I', 'improve', 'writing', 'skills.?\"'],\n",
       " ['\"What', 'causes', 'sleep', 'paralysis?\"'],\n",
       " ['\"How', 'do', 'you', 'cause', 'sleep', 'paralysis?\"'],\n",
       " ['\"What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'best',\n",
       "  'ever',\n",
       "  'books',\n",
       "  'that',\n",
       "  'everyone',\n",
       "  'should',\n",
       "  'read',\n",
       "  'in',\n",
       "  'his/her',\n",
       "  'lifetime?\"'],\n",
       " ['\"What', 'are', 'the', 'most', 'important', 'books', 'ever', 'written?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'communication',\n",
       "  'skill',\n",
       "  'and',\n",
       "  'English',\n",
       "  'proficiency?\"'],\n",
       " ['\"How',\n",
       "  'can',\n",
       "  'I',\n",
       "  'improve',\n",
       "  'my',\n",
       "  'communication',\n",
       "  'skills',\n",
       "  'in',\n",
       "  'English?\"'],\n",
       " ['\"Why', 'is', 'the', 'number', '13', 'considered', 'unlucky?\"'],\n",
       " ['\"Why', 'is', '13', 'considered', 'an', 'unlucky', 'number?\"'],\n",
       " ['\"What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'computer',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'first?\"'],\n",
       " ['\"How', 'can', 'I', 'prepare', 'for', 'the', 'IBPS', 'PO', '2017?\"'],\n",
       " ['\"How', 'do', 'I', 'prepare', 'for', 'IBPS', 'PO', '2017?\"'],\n",
       " ['\"How', 'can', 'one', 'find', 'his/her', 'life', 'purpose?\"'],\n",
       " ['\"How', 'do', 'you', 'find', 'your', \"life's\", 'purpose?\"'],\n",
       " ['\"India:',\n",
       "  'What',\n",
       "  'are',\n",
       "  'your',\n",
       "  'views',\n",
       "  'on',\n",
       "  'caste',\n",
       "  'based',\n",
       "  'reservation',\n",
       "  'system',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What',\n",
       "  'do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'of',\n",
       "  'reservation',\n",
       "  'system',\n",
       "  'in',\n",
       "  'India?\"'],\n",
       " ['\"What',\n",
       "  'will',\n",
       "  'be',\n",
       "  'the',\n",
       "  'effect',\n",
       "  'of',\n",
       "  'Donald',\n",
       "  'Trump',\n",
       "  'becoming',\n",
       "  'the',\n",
       "  'president',\n",
       "  'of',\n",
       "  'US',\n",
       "  'on',\n",
       "  'India?\"'],\n",
       " ['\"How', 'will', 'Donald', 'Trump', 'benefit', 'India?\"'],\n",
       " ['\"How', 'can', 'I', 'hack', 'Facebook?\"'],\n",
       " ['\"Has', 'Facebook', 'ever', 'been', 'hacked?\"'],\n",
       " ['\"How', 'do', 'I', 'know', 'that', 'I', 'am', 'a', 'psychopath?\"'],\n",
       " ['\"How', 'do', 'you', 'know', 'if', 'you', 'are', 'a', 'psychopath?\"'],\n",
       " ['\"How', 'much', 'does', 'YouTube', 'pay', 'per', 'subscriber?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'you',\n",
       "  'check',\n",
       "  'how',\n",
       "  'many',\n",
       "  'subscribers',\n",
       "  'you',\n",
       "  'have',\n",
       "  'on',\n",
       "  'YouTube?',\n",
       "  'What',\n",
       "  'other',\n",
       "  'information',\n",
       "  'can',\n",
       "  'you',\n",
       "  'find?\"'],\n",
       " ['\"How', 'IS', 'TO', 'get', 'into', 'MIT?\"'],\n",
       " ['\"What', 'are', 'the', 'requirements', 'for', 'selection', 'into', 'MIT?\"'],\n",
       " ['\"In',\n",
       "  'the',\n",
       "  'summer',\n",
       "  'how',\n",
       "  'would',\n",
       "  'I',\n",
       "  'dress',\n",
       "  'like',\n",
       "  'a',\n",
       "  'workout',\n",
       "  'guy',\n",
       "  'from',\n",
       "  'the',\n",
       "  'year',\n",
       "  '1990?\"'],\n",
       " ['\"How',\n",
       "  'would',\n",
       "  'I',\n",
       "  'dress',\n",
       "  'like',\n",
       "  'a',\n",
       "  'workout',\n",
       "  'guru',\n",
       "  'from',\n",
       "  'the',\n",
       "  'year',\n",
       "  '1990',\n",
       "  'in',\n",
       "  'the',\n",
       "  'summer?\"'],\n",
       " ['\"How',\n",
       "  'is',\n",
       "  'a',\n",
       "  'zero',\n",
       "  'gravity',\n",
       "  'environment',\n",
       "  'created',\n",
       "  'for',\n",
       "  'astronaut',\n",
       "  'training',\n",
       "  'on',\n",
       "  'earth?\"'],\n",
       " ['\"Robert',\n",
       "  'Frost:',\n",
       "  'How',\n",
       "  'do',\n",
       "  'astronauts',\n",
       "  'practise',\n",
       "  'working',\n",
       "  'in',\n",
       "  'zero',\n",
       "  'gravity',\n",
       "  'on',\n",
       "  'Earth?\"'],\n",
       " ['\"How', 'can', 'i', 'recover', 'facebook', 'password?\"'],\n",
       " ['\"How', 'can', 'I', 'retrieve', 'my', 'Facebook', 'password?\"'],\n",
       " ['\"Is', 'porn', 'banned', 'in', 'any', 'country?\"'],\n",
       " ['\"Should', 'porn', 'be', 'banned?\"'],\n",
       " ['\"How', 'can', 'I', 'increase', 'the', 'traffic', 'to', 'a', 'website?\"'],\n",
       " ['\"How', 'do', 'I', 'get', 'more', 'traffic', 'on', 'my', 'website?\"'],\n",
       " ['\"Is', 'there', 'a', 'way', 'to', 'get', 'rid', 'of', 'acne', 'naturally?\"'],\n",
       " ['\"How',\n",
       "  'do',\n",
       "  'I',\n",
       "  'get',\n",
       "  'rid',\n",
       "  'of',\n",
       "  'severe',\n",
       "  'chronic',\n",
       "  'acne',\n",
       "  'naturally?\"'],\n",
       " ['\"How', 'do', 'I', 'recover', 'deleted', 'browser', 'history?\"'],\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.split() for d in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-0ece51290388>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#document_list,titles=load_data(\"\",\"articles.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprepare_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#model=create_gensim_lsa_model(clean_text,number_of_topics,words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-5693e1f62c1b>\u001b[0m in \u001b[0;36mprepare_corpus\u001b[0;34m(doc_clean)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdoc_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_clean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         logger.info(\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# LSA Model\n",
    "number_of_topics=7\n",
    "words=10\n",
    "#document_list,titles=load_data(\"\",\"articles.txt\")\n",
    "prepare_text=prepare_corpus(corpus)\n",
    "#model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "tfidf = TfidfModel(corpus_gensim)\n",
    "corpus_tfidf = tfidf[corpus_gensim]\n",
    "lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_bow = dictionary.doc2bow(corpus[0].lower().split())   \n",
    "vec_lsi = lsi[vec_bow]\n",
    "[x[1] for x in vec_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.155680427536988,\n",
       " 1.3062715770448448,\n",
       " 0.10709776703347901,\n",
       " -0.4335887472481544,\n",
       " 0.5855573046178008]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[1] for x in vec_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "coherence_values = []\n",
    "model_list = []\n",
    "start,stop,step=2,30,1\n",
    "for num_topics in range(start, stop, step):\n",
    "    # generate LSA model\n",
    "    print(num_topics)\n",
    "    model = LsiModel(corpus_tfidf, num_topics=num_topics, id2word = dictionary)  # train model\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=[d.split() for d in corpus], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a454bfa90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5674cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "#model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,stop, start, step)\n",
    "# Show graph\n",
    "x = range(start, stop, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSA_features():\n",
    "    dictionary = corpora.Dictionary([d.split() for d in corpus])\n",
    "    corpus_gensim = [dictionary.doc2bow(doc.split()) for doc in corpus]   \n",
    "    tfidf = TfidfModel(corpus_gensim)\n",
    "    corpus_tfidf = tfidf[corpus_gensim]\n",
    "    lsi = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    counter=0\n",
    "    lsa_similarity_training=[]\n",
    "    for i in range (len(pairs_train)):\n",
    "        q1 = pairs_train[i][0]\n",
    "        q2 = pairs_train[i][1]\n",
    "\n",
    "        vec_bow1 = dictionary.doc2bow(questions[q1].lower().split())   \n",
    "        vec_lsi1 = lsi[vec_bow1]\n",
    "        vec_bow2 = dictionary.doc2bow(questions[q2].lower().split())   \n",
    "        vec_lsi2 = lsi[vec_bow2]\n",
    "        \n",
    "        if not [x[1] for x in vec_lsi1] or not [x[1] for x in vec_lsi2]:\n",
    "            cos_sim = 0\n",
    "        else: \n",
    "            cos_sim = cosine_similarity([[x[1] for x in vec_lsi1]],[[x[1] for x in vec_lsi2]])\n",
    "        lsa_similarity_training.append([cos_sim])\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print(counter, \"training examples processsed\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    counter=0\n",
    "    lsa_similarity_testing=[]\n",
    "    for i in range (len(pairs_test)):\n",
    "        q1 = pairs_test[i][0]\n",
    "        q2 = pairs_test[i][1]\n",
    "        \n",
    "        vec_bow1 = dictionary.doc2bow(questions[q1].lower().split())   \n",
    "        vec_lsi1 = lsi[vec_bow1]\n",
    "        vec_bow2 = dictionary.doc2bow(questions[q2].lower().split())   \n",
    "        vec_lsi2 = lsi[vec_bow2]\n",
    "        if not [x[1] for x in vec_lsi1] or not [x[1] for x in vec_lsi2]:\n",
    "            cos_sim = 0\n",
    "        else: \n",
    "            cos_sim = cosine_similarity([[x[1] for x in vec_lsi1]],[[x[1] for x in vec_lsi2]])\n",
    "        lsa_similarity_testing.append([cos_sim])\n",
    "        counter += 1\n",
    "        if counter % 1000 == True:\n",
    "            print(counter, \"testing examples processsed\")\n",
    "    \n",
    "    return(np.array(lsa_similarity_training),np.array(lsa_similarity_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "1001 training examples processsed\n",
      "2001 training examples processsed\n",
      "3001 training examples processsed\n",
      "4001 training examples processsed\n",
      "5001 training examples processsed\n",
      "6001 training examples processsed\n",
      "7001 training examples processsed\n",
      "8001 training examples processsed\n",
      "9001 training examples processsed\n",
      "10001 training examples processsed\n",
      "11001 training examples processsed\n",
      "12001 training examples processsed\n",
      "13001 training examples processsed\n",
      "14001 training examples processsed\n",
      "15001 training examples processsed\n",
      "16001 training examples processsed\n",
      "17001 training examples processsed\n",
      "18001 training examples processsed\n",
      "19001 training examples processsed\n",
      "20001 training examples processsed\n",
      "21001 training examples processsed\n",
      "22001 training examples processsed\n",
      "23001 training examples processsed\n",
      "24001 training examples processsed\n",
      "25001 training examples processsed\n",
      "26001 training examples processsed\n",
      "27001 training examples processsed\n",
      "28001 training examples processsed\n",
      "29001 training examples processsed\n",
      "30001 training examples processsed\n",
      "31001 training examples processsed\n",
      "32001 training examples processsed\n",
      "33001 training examples processsed\n",
      "34001 training examples processsed\n",
      "35001 training examples processsed\n",
      "36001 training examples processsed\n",
      "37001 training examples processsed\n",
      "38001 training examples processsed\n",
      "39001 training examples processsed\n",
      "40001 training examples processsed\n",
      "41001 training examples processsed\n",
      "42001 training examples processsed\n",
      "43001 training examples processsed\n",
      "44001 training examples processsed\n",
      "45001 training examples processsed\n",
      "46001 training examples processsed\n",
      "47001 training examples processsed\n",
      "48001 training examples processsed\n",
      "49001 training examples processsed\n",
      "50001 training examples processsed\n",
      "51001 training examples processsed\n",
      "52001 training examples processsed\n",
      "53001 training examples processsed\n",
      "54001 training examples processsed\n",
      "55001 training examples processsed\n",
      "56001 training examples processsed\n",
      "57001 training examples processsed\n",
      "58001 training examples processsed\n",
      "59001 training examples processsed\n",
      "60001 training examples processsed\n",
      "61001 training examples processsed\n",
      "62001 training examples processsed\n",
      "63001 training examples processsed\n",
      "64001 training examples processsed\n",
      "65001 training examples processsed\n",
      "66001 training examples processsed\n",
      "67001 training examples processsed\n",
      "68001 training examples processsed\n",
      "69001 training examples processsed\n",
      "70001 training examples processsed\n",
      "71001 training examples processsed\n",
      "72001 training examples processsed\n",
      "73001 training examples processsed\n",
      "74001 training examples processsed\n",
      "75001 training examples processsed\n",
      "76001 training examples processsed\n",
      "77001 training examples processsed\n",
      "78001 training examples processsed\n",
      "79001 training examples processsed\n",
      "80001 training examples processsed\n",
      "1 testing examples processsed\n",
      "1001 testing examples processsed\n",
      "2001 testing examples processsed\n",
      "3001 testing examples processsed\n",
      "4001 testing examples processsed\n",
      "5001 testing examples processsed\n",
      "6001 testing examples processsed\n",
      "7001 testing examples processsed\n",
      "8001 testing examples processsed\n",
      "9001 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "11001 testing examples processsed\n",
      "12001 testing examples processsed\n",
      "13001 testing examples processsed\n",
      "14001 testing examples processsed\n",
      "15001 testing examples processsed\n",
      "16001 testing examples processsed\n",
      "17001 testing examples processsed\n",
      "18001 testing examples processsed\n",
      "19001 testing examples processsed\n",
      "20001 testing examples processsed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[array([[0.91524612]])],\n",
       "        [array([[0.65974795]])],\n",
       "        [array([[0.93475091]])],\n",
       "        ...,\n",
       "        [array([[0.99450931]])],\n",
       "        [array([[0.9610865]])],\n",
       "        [array([[0.96134636]])]], dtype=object),\n",
       " array([[array([[0.93314743]])],\n",
       "        [array([[0.99957739]])],\n",
       "        [array([[0.98480479]])],\n",
       "        ...,\n",
       "        [array([[0.97308892]])],\n",
       "        [array([[0.84378918]])],\n",
       "        [array([[0.95149506]])]], dtype=object))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the glove library\n",
    "from glove import Corpus, Glove\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(lines, window=10)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=5, learning_rate=0.05)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')\n",
    "\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# magic trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig =  pd.DataFrame({'question1':pairs_train[:,0],'question2':pairs_train[:,1]})\n",
    "test_orig =  pd.DataFrame({'question1':pairs_test[:,0],'question2':pairs_test[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80100, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = pd.concat([train_orig[['question1', 'question2']], \\\n",
    "        test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "ques.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_raw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-d68b05264639>\u001b[0m in \u001b[0;36mq1_q2_intersect\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mq1_q2_intersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: ('only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices', 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-3b347780d0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_q2_intersect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_q2_intersect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6012\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6013\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6014\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_raw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# TODO: mixed type case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-d68b05264639>\u001b[0m in \u001b[0;36mq1_q2_intersect\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mq1_q2_intersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_orig.apply(q1_q2_intersect, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orig =  pd.read_csv('train2.csv', header=0)\n",
    "test_orig =  pd.read_csv('test2.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_q2_intersect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>264621</td>\n",
       "      <td>285217</td>\n",
       "      <td>\"What is your New Year Resolution?\"</td>\n",
       "      <td>\"What Is your New year resolutions in 2017?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37774</td>\n",
       "      <td>331401</td>\n",
       "      <td>\"What are some alternatives ways to lose 30 po...</td>\n",
       "      <td>\"How can I lose 30 pounds in 1 month? What are...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>199268</td>\n",
       "      <td>44306</td>\n",
       "      <td>\"Which is the best programming language for a ...</td>\n",
       "      <td>\"What programming language I should learn first?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>315658</td>\n",
       "      <td>291313</td>\n",
       "      <td>\"How do I get VoLTE symbol in Redmi Note 3?\"</td>\n",
       "      <td>\"Does Coolpad Note 3 supports VoLTE?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>70704</td>\n",
       "      <td>328389</td>\n",
       "      <td>\"Why do we have nightmares? What is the cause ...</td>\n",
       "      <td>\"Why do we get Nightmares?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>211481</td>\n",
       "      <td>53379</td>\n",
       "      <td>\"When will Apple release the new MacBook Pro i...</td>\n",
       "      <td>\"When is the new Apple Macbook Pro coming? In ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>21222</td>\n",
       "      <td>38260</td>\n",
       "      <td>\"Is it healthy to eat chapati every day?\"</td>\n",
       "      <td>\"Is it healthy to eat one chicken every day?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>330104</td>\n",
       "      <td>331067</td>\n",
       "      <td>\"What is the difference between applications (...</td>\n",
       "      <td>\"What is the difference between an app and a s...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9872</td>\n",
       "      <td>325409</td>\n",
       "      <td>\"What are the best movie scenes ever? What mak...</td>\n",
       "      <td>\"What are the best scenes in movie history?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>404109</td>\n",
       "      <td>104740</td>\n",
       "      <td>\"Will Donald Trump’s win in the elections have...</td>\n",
       "      <td>\"How would Trump presidency affect Indian stud...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>210756</td>\n",
       "      <td>156056</td>\n",
       "      <td>\"How long does meth say in your urine?\"</td>\n",
       "      <td>\"If I smoked meth for a month how long for a U...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>366514</td>\n",
       "      <td>406111</td>\n",
       "      <td>\"How should one learn languages?\"</td>\n",
       "      <td>\"What is the best way to learn a language by y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>387859</td>\n",
       "      <td>407819</td>\n",
       "      <td>\"What do you think of the decision by the Indi...</td>\n",
       "      <td>\"How do you see the PM Modi’s move of banning ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>157373</td>\n",
       "      <td>230270</td>\n",
       "      <td>\"How do you know if you've fallen for someone?\"</td>\n",
       "      <td>\"How do you know if you're unconditionally in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>261969</td>\n",
       "      <td>71621</td>\n",
       "      <td>\"Which is the best phone under 15000 Rs.?\"</td>\n",
       "      <td>\"Which is the best phone below 15000?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>356739</td>\n",
       "      <td>125786</td>\n",
       "      <td>\"What are the environmental impacts of the tou...</td>\n",
       "      <td>\"What are the environmental impacts of the tou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>93544</td>\n",
       "      <td>188394</td>\n",
       "      <td>\"How can I increase my English fluency?\"</td>\n",
       "      <td>\"How do I speak English like celebrities?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>220277</td>\n",
       "      <td>443038</td>\n",
       "      <td>\"How do i lose weight?\"</td>\n",
       "      <td>\"I'm overweight. How can I begin to lose weight?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>408987</td>\n",
       "      <td>439179</td>\n",
       "      <td>\"What would be the good things of a Trump pres...</td>\n",
       "      <td>\"In what ways would a Donald Trump presidency ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>174913</td>\n",
       "      <td>210973</td>\n",
       "      <td>\"Is there any hypothesis that the big bang occ...</td>\n",
       "      <td>\"What existed before the Big Bang?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>344927</td>\n",
       "      <td>265291</td>\n",
       "      <td>\"How do I add photos to my questions on Quora?\"</td>\n",
       "      <td>\"How do I add pictures to my answers on Quora?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>297461</td>\n",
       "      <td>302388</td>\n",
       "      <td>\"What does an Oxford University degree look li...</td>\n",
       "      <td>\"How hard is it to be accepted into Cambridge ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>245183</td>\n",
       "      <td>172828</td>\n",
       "      <td>\"How long does it take a woman to get pregnant...</td>\n",
       "      <td>\"What are the common first signs of pregnancy?...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>812</td>\n",
       "      <td>63620</td>\n",
       "      <td>\"What are the strongest majors in terms of job...</td>\n",
       "      <td>\"What are the strongest majors in terms of job...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>387362</td>\n",
       "      <td>17976</td>\n",
       "      <td>\"How can I increase traffic very soon on my bl...</td>\n",
       "      <td>\"How can I increase the traffic on a site?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>330048</td>\n",
       "      <td>336588</td>\n",
       "      <td>\"Why can't Akshay Kumar make it as big as Salm...</td>\n",
       "      <td>\"What is Shah Rukh Khan's best movie till date...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>113526</td>\n",
       "      <td>217353</td>\n",
       "      <td>\"How many human beings have ever lived?\"</td>\n",
       "      <td>\"How many years do you think humanity will sti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>37135</td>\n",
       "      <td>189615</td>\n",
       "      <td>\"What is the best way to suicide?\"</td>\n",
       "      <td>\"How commit suicide?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>101589</td>\n",
       "      <td>3929</td>\n",
       "      <td>\"Where can I get wonderful floor tiles company...</td>\n",
       "      <td>\"Where can I found wide variety of ceramic til...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>385474</td>\n",
       "      <td>245231</td>\n",
       "      <td>\"What valid proof can confirm/deny the surgica...</td>\n",
       "      <td>\"Has India provided any proof of the surgical ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76065</th>\n",
       "      <td>76065</td>\n",
       "      <td>437949</td>\n",
       "      <td>7730</td>\n",
       "      <td>\"Does skipping increase height?\"</td>\n",
       "      <td>\"Can skipping increase my height?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76066</th>\n",
       "      <td>76066</td>\n",
       "      <td>79198</td>\n",
       "      <td>265959</td>\n",
       "      <td>\"Is it possible to lose 20 pounds in one month?\"</td>\n",
       "      <td>\"How do you lose 20 pounds in two months?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76067</th>\n",
       "      <td>76067</td>\n",
       "      <td>28566</td>\n",
       "      <td>219004</td>\n",
       "      <td>\"What are the indications that a guy is intere...</td>\n",
       "      <td>\"What makes a woman or girl unattractive?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76068</th>\n",
       "      <td>76068</td>\n",
       "      <td>41904</td>\n",
       "      <td>302387</td>\n",
       "      <td>\"What are the best resource to learn web devel...</td>\n",
       "      <td>\"What are the best online web development cour...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76069</th>\n",
       "      <td>76069</td>\n",
       "      <td>110966</td>\n",
       "      <td>305964</td>\n",
       "      <td>\"What are some good Android app project ideas?\"</td>\n",
       "      <td>\"What are some good Android app ideas (no game...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76070</th>\n",
       "      <td>76070</td>\n",
       "      <td>330043</td>\n",
       "      <td>132550</td>\n",
       "      <td>\"What are some good hotels in Nainital?\"</td>\n",
       "      <td>\"What are some of the best hotel in Nainital?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76071</th>\n",
       "      <td>76071</td>\n",
       "      <td>31792</td>\n",
       "      <td>311137</td>\n",
       "      <td>\"How did Donald Trump win the presidency?\"</td>\n",
       "      <td>\"Why did Donald Trump win the 2016 American el...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76072</th>\n",
       "      <td>76072</td>\n",
       "      <td>167690</td>\n",
       "      <td>434892</td>\n",
       "      <td>\"Will GST change Indian economy?\"</td>\n",
       "      <td>\"Why is GST amendment a gamechanger for Indian...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76073</th>\n",
       "      <td>76073</td>\n",
       "      <td>183100</td>\n",
       "      <td>158183</td>\n",
       "      <td>\"What is the best way to increase traffic for ...</td>\n",
       "      <td>\"What is the best way to get free traffic to m...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76074</th>\n",
       "      <td>76074</td>\n",
       "      <td>180001</td>\n",
       "      <td>298452</td>\n",
       "      <td>\"Why is Saltwater Taffy candy imported in Hong...</td>\n",
       "      <td>\"Why is Saltwater taffy candy imported in Germ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76075</th>\n",
       "      <td>76075</td>\n",
       "      <td>50110</td>\n",
       "      <td>111793</td>\n",
       "      <td>\"Which are the best books or learning resource...</td>\n",
       "      <td>\"What is the best book to study for the GRE?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76076</th>\n",
       "      <td>76076</td>\n",
       "      <td>75006</td>\n",
       "      <td>27149</td>\n",
       "      <td>\"How is it to fall in love?\"</td>\n",
       "      <td>\"How is it to fall in love.?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76077</th>\n",
       "      <td>76077</td>\n",
       "      <td>56628</td>\n",
       "      <td>29322</td>\n",
       "      <td>\"Will Littlefinger take the Iron Throne?\"</td>\n",
       "      <td>\"Will Littlefinger sit on the Iron Throne?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76078</th>\n",
       "      <td>76078</td>\n",
       "      <td>193426</td>\n",
       "      <td>68460</td>\n",
       "      <td>\"How do you call someone if they have blocked ...</td>\n",
       "      <td>\"How do I call a number that blocked me?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76079</th>\n",
       "      <td>76079</td>\n",
       "      <td>172664</td>\n",
       "      <td>88636</td>\n",
       "      <td>\"Are there any mathematical proofs to the exis...</td>\n",
       "      <td>\"Is there any proof of the existence of aliens...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76080</th>\n",
       "      <td>76080</td>\n",
       "      <td>430078</td>\n",
       "      <td>10278</td>\n",
       "      <td>\"How many keywords are there in the COBOL prog...</td>\n",
       "      <td>\"How many keywords are there in CRYPTOL Progra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76081</th>\n",
       "      <td>76081</td>\n",
       "      <td>428744</td>\n",
       "      <td>206359</td>\n",
       "      <td>\"Do we suffer a jerk when earth stops rotating...</td>\n",
       "      <td>\"What will happen if the earth stops rotating?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76082</th>\n",
       "      <td>76082</td>\n",
       "      <td>200349</td>\n",
       "      <td>99427</td>\n",
       "      <td>\"What are the best ways to lose weight fast?\"</td>\n",
       "      <td>\"What is the best way to loose weight quickly?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76083</th>\n",
       "      <td>76083</td>\n",
       "      <td>176466</td>\n",
       "      <td>291428</td>\n",
       "      <td>\"Why does everyone like dogs so much?\"</td>\n",
       "      <td>\"Why do some people love dogs so much?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76084</th>\n",
       "      <td>76084</td>\n",
       "      <td>58935</td>\n",
       "      <td>311402</td>\n",
       "      <td>\"What do you think is important in life?\"</td>\n",
       "      <td>\"What is simply the most important in life?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76085</th>\n",
       "      <td>76085</td>\n",
       "      <td>320239</td>\n",
       "      <td>59780</td>\n",
       "      <td>\"Do medical deemed universities follow caste-b...</td>\n",
       "      <td>\"Does India need reservations? Why?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76086</th>\n",
       "      <td>76086</td>\n",
       "      <td>344409</td>\n",
       "      <td>85904</td>\n",
       "      <td>\"How do I add content on Quora?\"</td>\n",
       "      <td>\"How can I anonymously add content on Quora?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76087</th>\n",
       "      <td>76087</td>\n",
       "      <td>357768</td>\n",
       "      <td>413654</td>\n",
       "      <td>\"I'm a chubby girl and my face looks swollen. ...</td>\n",
       "      <td>\"My face is fat. How do I slim down my moon fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76088</th>\n",
       "      <td>76088</td>\n",
       "      <td>420259</td>\n",
       "      <td>116804</td>\n",
       "      <td>\"How do I control anxiety?\"</td>\n",
       "      <td>\"How do I control my anxiety?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76089</th>\n",
       "      <td>76089</td>\n",
       "      <td>432579</td>\n",
       "      <td>41427</td>\n",
       "      <td>\"How do I get over a girl that I like?\"</td>\n",
       "      <td>\"How do you get a girl to like you?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76090</th>\n",
       "      <td>76090</td>\n",
       "      <td>357830</td>\n",
       "      <td>368012</td>\n",
       "      <td>\"Which are the best workout songs?\"</td>\n",
       "      <td>\"What are the some of the best songs for the g...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76091</th>\n",
       "      <td>76091</td>\n",
       "      <td>169140</td>\n",
       "      <td>196083</td>\n",
       "      <td>\"What's the possibility of planet Earth runnin...</td>\n",
       "      <td>\"Will we ever run out of pure drinking water?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76092</th>\n",
       "      <td>76092</td>\n",
       "      <td>190489</td>\n",
       "      <td>324591</td>\n",
       "      <td>\"What traffic laws in Australia are particular...</td>\n",
       "      <td>\"What traffic laws in Spain are particularly h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76093</th>\n",
       "      <td>76093</td>\n",
       "      <td>274647</td>\n",
       "      <td>345606</td>\n",
       "      <td>\"Should I hire a technical co-founder for my s...</td>\n",
       "      <td>\"How do I find good technical co-founders?\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76094</th>\n",
       "      <td>76094</td>\n",
       "      <td>266795</td>\n",
       "      <td>261371</td>\n",
       "      <td>\"How do I improve my pronunciation in English?\"</td>\n",
       "      <td>\"What are the best ways to improve English?\"</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76095 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    qid1    qid2  \\\n",
       "0          0  264621  285217   \n",
       "1          1   37774  331401   \n",
       "2          2  199268   44306   \n",
       "3          3  315658  291313   \n",
       "4          4   70704  328389   \n",
       "5          5  211481   53379   \n",
       "6          6   21222   38260   \n",
       "7          7  330104  331067   \n",
       "8          8    9872  325409   \n",
       "9          9  404109  104740   \n",
       "10        10  210756  156056   \n",
       "11        11  366514  406111   \n",
       "12        12  387859  407819   \n",
       "13        13  157373  230270   \n",
       "14        14  261969   71621   \n",
       "15        15  356739  125786   \n",
       "16        16   93544  188394   \n",
       "17        17  220277  443038   \n",
       "18        18  408987  439179   \n",
       "19        19  174913  210973   \n",
       "20        20  344927  265291   \n",
       "21        21  297461  302388   \n",
       "22        22  245183  172828   \n",
       "23        23     812   63620   \n",
       "24        24  387362   17976   \n",
       "25        25  330048  336588   \n",
       "26        26  113526  217353   \n",
       "27        27   37135  189615   \n",
       "28        28  101589    3929   \n",
       "29        29  385474  245231   \n",
       "...      ...     ...     ...   \n",
       "76065  76065  437949    7730   \n",
       "76066  76066   79198  265959   \n",
       "76067  76067   28566  219004   \n",
       "76068  76068   41904  302387   \n",
       "76069  76069  110966  305964   \n",
       "76070  76070  330043  132550   \n",
       "76071  76071   31792  311137   \n",
       "76072  76072  167690  434892   \n",
       "76073  76073  183100  158183   \n",
       "76074  76074  180001  298452   \n",
       "76075  76075   50110  111793   \n",
       "76076  76076   75006   27149   \n",
       "76077  76077   56628   29322   \n",
       "76078  76078  193426   68460   \n",
       "76079  76079  172664   88636   \n",
       "76080  76080  430078   10278   \n",
       "76081  76081  428744  206359   \n",
       "76082  76082  200349   99427   \n",
       "76083  76083  176466  291428   \n",
       "76084  76084   58935  311402   \n",
       "76085  76085  320239   59780   \n",
       "76086  76086  344409   85904   \n",
       "76087  76087  357768  413654   \n",
       "76088  76088  420259  116804   \n",
       "76089  76089  432579   41427   \n",
       "76090  76090  357830  368012   \n",
       "76091  76091  169140  196083   \n",
       "76092  76092  190489  324591   \n",
       "76093  76093  274647  345606   \n",
       "76094  76094  266795  261371   \n",
       "\n",
       "                                               question1  \\\n",
       "0                    \"What is your New Year Resolution?\"   \n",
       "1      \"What are some alternatives ways to lose 30 po...   \n",
       "2      \"Which is the best programming language for a ...   \n",
       "3           \"How do I get VoLTE symbol in Redmi Note 3?\"   \n",
       "4      \"Why do we have nightmares? What is the cause ...   \n",
       "5      \"When will Apple release the new MacBook Pro i...   \n",
       "6              \"Is it healthy to eat chapati every day?\"   \n",
       "7      \"What is the difference between applications (...   \n",
       "8      \"What are the best movie scenes ever? What mak...   \n",
       "9      \"Will Donald Trump’s win in the elections have...   \n",
       "10               \"How long does meth say in your urine?\"   \n",
       "11                     \"How should one learn languages?\"   \n",
       "12     \"What do you think of the decision by the Indi...   \n",
       "13       \"How do you know if you've fallen for someone?\"   \n",
       "14            \"Which is the best phone under 15000 Rs.?\"   \n",
       "15     \"What are the environmental impacts of the tou...   \n",
       "16              \"How can I increase my English fluency?\"   \n",
       "17                               \"How do i lose weight?\"   \n",
       "18     \"What would be the good things of a Trump pres...   \n",
       "19     \"Is there any hypothesis that the big bang occ...   \n",
       "20       \"How do I add photos to my questions on Quora?\"   \n",
       "21     \"What does an Oxford University degree look li...   \n",
       "22     \"How long does it take a woman to get pregnant...   \n",
       "23     \"What are the strongest majors in terms of job...   \n",
       "24     \"How can I increase traffic very soon on my bl...   \n",
       "25     \"Why can't Akshay Kumar make it as big as Salm...   \n",
       "26              \"How many human beings have ever lived?\"   \n",
       "27                    \"What is the best way to suicide?\"   \n",
       "28     \"Where can I get wonderful floor tiles company...   \n",
       "29     \"What valid proof can confirm/deny the surgica...   \n",
       "...                                                  ...   \n",
       "76065                   \"Does skipping increase height?\"   \n",
       "76066   \"Is it possible to lose 20 pounds in one month?\"   \n",
       "76067  \"What are the indications that a guy is intere...   \n",
       "76068  \"What are the best resource to learn web devel...   \n",
       "76069    \"What are some good Android app project ideas?\"   \n",
       "76070           \"What are some good hotels in Nainital?\"   \n",
       "76071         \"How did Donald Trump win the presidency?\"   \n",
       "76072                  \"Will GST change Indian economy?\"   \n",
       "76073  \"What is the best way to increase traffic for ...   \n",
       "76074  \"Why is Saltwater Taffy candy imported in Hong...   \n",
       "76075  \"Which are the best books or learning resource...   \n",
       "76076                       \"How is it to fall in love?\"   \n",
       "76077          \"Will Littlefinger take the Iron Throne?\"   \n",
       "76078  \"How do you call someone if they have blocked ...   \n",
       "76079  \"Are there any mathematical proofs to the exis...   \n",
       "76080  \"How many keywords are there in the COBOL prog...   \n",
       "76081  \"Do we suffer a jerk when earth stops rotating...   \n",
       "76082      \"What are the best ways to lose weight fast?\"   \n",
       "76083             \"Why does everyone like dogs so much?\"   \n",
       "76084          \"What do you think is important in life?\"   \n",
       "76085  \"Do medical deemed universities follow caste-b...   \n",
       "76086                   \"How do I add content on Quora?\"   \n",
       "76087  \"I'm a chubby girl and my face looks swollen. ...   \n",
       "76088                        \"How do I control anxiety?\"   \n",
       "76089            \"How do I get over a girl that I like?\"   \n",
       "76090                \"Which are the best workout songs?\"   \n",
       "76091  \"What's the possibility of planet Earth runnin...   \n",
       "76092  \"What traffic laws in Australia are particular...   \n",
       "76093  \"Should I hire a technical co-founder for my s...   \n",
       "76094    \"How do I improve my pronunciation in English?\"   \n",
       "\n",
       "                                               question2  is_duplicate  \\\n",
       "0           \"What Is your New year resolutions in 2017?\"             1   \n",
       "1      \"How can I lose 30 pounds in 1 month? What are...             1   \n",
       "2      \"What programming language I should learn first?\"             1   \n",
       "3                  \"Does Coolpad Note 3 supports VoLTE?\"             0   \n",
       "4                            \"Why do we get Nightmares?\"             1   \n",
       "5      \"When is the new Apple Macbook Pro coming? In ...             1   \n",
       "6          \"Is it healthy to eat one chicken every day?\"             0   \n",
       "7      \"What is the difference between an app and a s...             1   \n",
       "8           \"What are the best scenes in movie history?\"             1   \n",
       "9      \"How would Trump presidency affect Indian stud...             1   \n",
       "10     \"If I smoked meth for a month how long for a U...             1   \n",
       "11     \"What is the best way to learn a language by y...             0   \n",
       "12     \"How do you see the PM Modi’s move of banning ...             1   \n",
       "13     \"How do you know if you're unconditionally in ...             1   \n",
       "14                \"Which is the best phone below 15000?\"             1   \n",
       "15     \"What are the environmental impacts of the tou...             0   \n",
       "16            \"How do I speak English like celebrities?\"             1   \n",
       "17     \"I'm overweight. How can I begin to lose weight?\"             1   \n",
       "18     \"In what ways would a Donald Trump presidency ...             1   \n",
       "19                   \"What existed before the Big Bang?\"             0   \n",
       "20       \"How do I add pictures to my answers on Quora?\"             1   \n",
       "21     \"How hard is it to be accepted into Cambridge ...             0   \n",
       "22     \"What are the common first signs of pregnancy?...             0   \n",
       "23     \"What are the strongest majors in terms of job...             0   \n",
       "24           \"How can I increase the traffic on a site?\"             1   \n",
       "25     \"What is Shah Rukh Khan's best movie till date...             0   \n",
       "26     \"How many years do you think humanity will sti...             0   \n",
       "27                                 \"How commit suicide?\"             1   \n",
       "28     \"Where can I found wide variety of ceramic til...             1   \n",
       "29     \"Has India provided any proof of the surgical ...             1   \n",
       "...                                                  ...           ...   \n",
       "76065                 \"Can skipping increase my height?\"             1   \n",
       "76066         \"How do you lose 20 pounds in two months?\"             0   \n",
       "76067         \"What makes a woman or girl unattractive?\"             0   \n",
       "76068  \"What are the best online web development cour...             0   \n",
       "76069  \"What are some good Android app ideas (no game...             0   \n",
       "76070     \"What are some of the best hotel in Nainital?\"             1   \n",
       "76071  \"Why did Donald Trump win the 2016 American el...             1   \n",
       "76072  \"Why is GST amendment a gamechanger for Indian...             1   \n",
       "76073  \"What is the best way to get free traffic to m...             1   \n",
       "76074  \"Why is Saltwater taffy candy imported in Germ...             1   \n",
       "76075      \"What is the best book to study for the GRE?\"             1   \n",
       "76076                      \"How is it to fall in love.?\"             1   \n",
       "76077        \"Will Littlefinger sit on the Iron Throne?\"             1   \n",
       "76078          \"How do I call a number that blocked me?\"             0   \n",
       "76079  \"Is there any proof of the existence of aliens...             1   \n",
       "76080  \"How many keywords are there in CRYPTOL Progra...             0   \n",
       "76081    \"What will happen if the earth stops rotating?\"             1   \n",
       "76082    \"What is the best way to loose weight quickly?\"             1   \n",
       "76083            \"Why do some people love dogs so much?\"             1   \n",
       "76084       \"What is simply the most important in life?\"             1   \n",
       "76085               \"Does India need reservations? Why?\"             0   \n",
       "76086      \"How can I anonymously add content on Quora?\"             0   \n",
       "76087  \"My face is fat. How do I slim down my moon fa...             1   \n",
       "76088                     \"How do I control my anxiety?\"             1   \n",
       "76089               \"How do you get a girl to like you?\"             0   \n",
       "76090  \"What are the some of the best songs for the g...             1   \n",
       "76091     \"Will we ever run out of pure drinking water?\"             1   \n",
       "76092  \"What traffic laws in Spain are particularly h...             0   \n",
       "76093        \"How do I find good technical co-founders?\"             0   \n",
       "76094       \"What are the best ways to improve English?\"             1   \n",
       "\n",
       "       q1_q2_intersect  \n",
       "0                   10  \n",
       "1                    5  \n",
       "2                    8  \n",
       "3                    0  \n",
       "4                    2  \n",
       "5                    9  \n",
       "6                    3  \n",
       "7                    3  \n",
       "8                    0  \n",
       "9                    2  \n",
       "10                   3  \n",
       "11                   0  \n",
       "12                  11  \n",
       "13                   4  \n",
       "14                   8  \n",
       "15                   0  \n",
       "16                  11  \n",
       "17                  16  \n",
       "18                   0  \n",
       "19                   0  \n",
       "20                   5  \n",
       "21                   0  \n",
       "22                   0  \n",
       "23                   0  \n",
       "24                  11  \n",
       "25                   0  \n",
       "26                   0  \n",
       "27                   3  \n",
       "28                   4  \n",
       "29                   1  \n",
       "...                ...  \n",
       "76065               10  \n",
       "76066                0  \n",
       "76067                0  \n",
       "76068                0  \n",
       "76069                0  \n",
       "76070                0  \n",
       "76071               14  \n",
       "76072                5  \n",
       "76073                5  \n",
       "76074               19  \n",
       "76075                3  \n",
       "76076                1  \n",
       "76077                0  \n",
       "76078                0  \n",
       "76079                3  \n",
       "76080                0  \n",
       "76081                2  \n",
       "76082               10  \n",
       "76083                0  \n",
       "76084                2  \n",
       "76085                0  \n",
       "76086                0  \n",
       "76087                1  \n",
       "76088                1  \n",
       "76089                0  \n",
       "76090                2  \n",
       "76091                0  \n",
       "76092                0  \n",
       "76093                0  \n",
       "76094               13  \n",
       "\n",
       "[76095 rows x 7 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic_trick1():\n",
    "\n",
    "    tic0=timeit.default_timer()\n",
    "    df1 = train_orig[['question1']].copy()\n",
    "    df2 = train_orig[['question2']].copy()\n",
    "    df1_test = test_orig[['question1']].copy()\n",
    "    df2_test = test_orig[['question2']].copy()\n",
    "\n",
    "    df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "    df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "    train_questions = df1.append(df2)\n",
    "    train_questions = train_questions.append(df1_test)\n",
    "    train_questions = train_questions.append(df2_test)\n",
    "    #train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n",
    "    train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "    train_questions.reset_index(inplace=True,drop=True)\n",
    "    questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "    train_cp = train_orig.copy()\n",
    "    test_cp = test_orig.copy()\n",
    "    train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "\n",
    "    test_cp['is_duplicate'] = -1\n",
    "    test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "    comb = pd.concat([train_cp,test_cp])\n",
    "\n",
    "    comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "    comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "    q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "    q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "    def try_apply_dict(x,dict_to_apply):\n",
    "        try:\n",
    "            return dict_to_apply[x]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    #map to frequency space\n",
    "    comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "    comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "    train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "    test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "\n",
    "    return(train_comb[['id','q1_hash','q2_hash','q1_freq','q2_freq']].values,test_comb[['id','q1_hash','q2_hash','q1_freq','q2_freq']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rousselpaul/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creat_features(magic_trick1,'magic_trick1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magic_trick2():\n",
    "\n",
    "    ques = pd.concat([train_orig[['question1', 'question2']], \\\n",
    "            test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "    ques.shape\n",
    "\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(ques.shape[0]):\n",
    "            q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "            q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "    def q1_q2_intersect(row):\n",
    "        return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "    train_orig['q1_q2_intersect'] = train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "    test_orig['q1_q2_intersect'] = test_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "\n",
    "    temp = train_orig.q1_q2_intersect.value_counts()\n",
    "    #sns.barplot(temp.index[:20], temp.values[:20])\n",
    "\n",
    "    train_feat = train_orig[['q1_q2_intersect']]\n",
    "    test_feat = test_orig[['q1_q2_intersect']]\n",
    "    return(train_feat.values,test_feat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "creat_features(magic_trick2,'magic_trick2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features https://github.com/aerdem4/kaggle-quora-dup/blob/master/nlp_feature_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz\n",
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAFE_DIV = 0.0001\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "    \n",
    "def extract_features(df):\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    print(\"token features...\")\n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "\n",
    "    print(\"fuzzy features..\")\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_NLP_features():\n",
    "    train_df = extract_features(train_orig)\n",
    "    train_df.drop([\"id\", \"qid1\", \"qid2\", \"question1\", \"question2\", \"is_duplicate\"], axis=1, inplace=True)\n",
    "    \n",
    "    test_df = extract_features(test_orig)\n",
    "    test_df.drop([\"id\", \"qid1\", \"qid2\", \"question1\", \"question2\"], axis=1, inplace=True)\n",
    "    return(train_df.values,test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token features...\n",
      "fuzzy features..\n",
      "token features...\n",
      "fuzzy features..\n"
     ]
    }
   ],
   "source": [
    "creat_features(extract_NLP_features,'extract_NLP_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "NB_CORES = 10\n",
    "FREQ_UPPER_BOUND = 100\n",
    "NEIGHBOR_UPPER_BOUND = 5\n",
    "\n",
    "\n",
    "def create_question_hash(train_df, test_df):\n",
    "    train_qs = np.dstack([train_df[\"question1\"], train_df[\"question2\"]]).flatten()\n",
    "    test_qs = np.dstack([test_df[\"question1\"], test_df[\"question2\"]]).flatten()\n",
    "    all_qs = np.append(train_qs, test_qs)\n",
    "    all_qs = pd.DataFrame(all_qs)[0].drop_duplicates()\n",
    "    all_qs.reset_index(inplace=True, drop=True)\n",
    "    question_dict = pd.Series(all_qs.index.values, index=all_qs.values).to_dict()\n",
    "    return question_dict\n",
    "\n",
    "\n",
    "def get_hash(df, hash_dict):\n",
    "    df[\"qid1\"] = df[\"question1\"].map(hash_dict)\n",
    "    df[\"qid2\"] = df[\"question2\"].map(hash_dict)\n",
    "    return df.drop([\"question1\", \"question2\"], axis=1)\n",
    "\n",
    "\n",
    "def get_kcore_dict(df):\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(df.qid1)\n",
    "    edges = list(df[[\"qid1\", \"qid2\"]].to_records(index=False))\n",
    "    g.add_edges_from(edges)\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "\n",
    "    df_output = pd.DataFrame(data=g.nodes(), columns=[\"qid\"])\n",
    "    df_output[\"kcore\"] = 0\n",
    "    for k in range(2, NB_CORES + 1):\n",
    "        ck = nx.k_core(g, k=k).nodes()\n",
    "        print(\"kcore\", k)\n",
    "        df_output.ix[df_output.qid.isin(ck), \"kcore\"] = k\n",
    "\n",
    "    return df_output.to_dict()[\"kcore\"]\n",
    "\n",
    "\n",
    "def get_kcore_features(df, kcore_dict):\n",
    "    df[\"kcore1\"] = df[\"qid1\"].apply(lambda x: kcore_dict[x])\n",
    "    df[\"kcore2\"] = df[\"qid2\"].apply(lambda x: kcore_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_minmax(df, col):\n",
    "    sorted_features = np.sort(np.vstack([df[col + \"1\"], df[col + \"2\"]]).T)\n",
    "    df[\"min_\" + col] = sorted_features[:, 0]\n",
    "    df[\"max_\" + col] = sorted_features[:, 1]\n",
    "    return df.drop([col + \"1\", col + \"2\"], axis=1)\n",
    "\n",
    "\n",
    "def get_neighbors(train_df, test_df):\n",
    "    neighbors = defaultdict(set)\n",
    "    for df in [train_df, test_df]:\n",
    "        for q1, q2 in zip(df[\"qid1\"], df[\"qid2\"]):\n",
    "            neighbors[q1].add(q2)\n",
    "            neighbors[q2].add(q1)\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def get_neighbor_features(df, neighbors):\n",
    "    common_nc = df.apply(lambda x: len(neighbors[x.qid1].intersection(neighbors[x.qid2])), axis=1)\n",
    "    min_nc = df.apply(lambda x: min(len(neighbors[x.qid1]), len(neighbors[x.qid2])), axis=1)\n",
    "    df[\"common_neighbor_ratio\"] = common_nc / min_nc\n",
    "    df[\"common_neighbor_count\"] = common_nc.apply(lambda x: min(x, NEIGHBOR_UPPER_BOUND))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_freq_features(df, frequency_map):\n",
    "    df[\"freq1\"] = df[\"qid1\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    df[\"freq2\"] = df[\"qid2\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_not_NLP_features():\n",
    "    train_df = train_orig\n",
    "    test_df = test_orig\n",
    "\n",
    "    print(\"Hashing the questions...\")\n",
    "    question_dict = create_question_hash(train_df, test_df)\n",
    "    train_df = get_hash(train_df, question_dict)\n",
    "    test_df = get_hash(test_df, question_dict)\n",
    "    print(\"Number of unique questions:\", len(question_dict))\n",
    "\n",
    "\n",
    "    print(\"Calculating common neighbor features...\")\n",
    "    neighbors = get_neighbors(train_df, test_df)\n",
    "    train_df = get_neighbor_features(train_df, neighbors)\n",
    "    test_df = get_neighbor_features(test_df, neighbors)\n",
    "\n",
    "    print(\"Calculating frequency features...\")\n",
    "    frequency_map = dict(zip(*np.unique(np.vstack((all_df[\"qid1\"], all_df[\"qid2\"])), return_counts=True)))\n",
    "    train_df = get_freq_features(train_df, frequency_map)\n",
    "    test_df = get_freq_features(test_df, frequency_map)\n",
    "    train_df = convert_to_minmax(train_df, \"freq\")\n",
    "    test_df = convert_to_minmax(test_df, \"freq\")\n",
    "\n",
    "    cols = [ \"common_neighbor_count\", \"common_neighbor_ratio\", \"min_freq\", \"max_freq\"]\n",
    "\n",
    "    return(train_df.loc[:, cols].values,test_df.loc[:, cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing the questions...\n",
      "Number of unique questions: 51856\n",
      "Calculating common neighbor features...\n",
      "Calculating frequency features...\n"
     ]
    }
   ],
   "source": [
    "creat_features(extract_not_NLP_features,'extract_not_NLP_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test = load_all_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'induce_pcfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bbe437282284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViterbiParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minduce_pcfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproductions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'induce_pcfg' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk import Tree\n",
    "from nltk import nonterminals, Nonterminal, Production\n",
    "from nltk.parse import ViterbiParser\n",
    "S = Nonterminal('S')\n",
    "grammar = induce_pcfg(S, productions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7ff6beb7c946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'I saw John with my eyes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#simplified – you can use a tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mViterbiParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#using the grammar induced by the Treebank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#the resulting parse tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grammar' is not defined"
     ]
    }
   ],
   "source": [
    "s= 'I saw John with my eyes'\n",
    "tokens = s.split() #simplified – you can use a tokenizer \n",
    "parser=ViterbiParser(grammar) #using the grammar induced by the Treebank \n",
    "parses = parser.parse_all(tokens) #the resulting parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getBestParse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ContextFreeGrammar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee7fa8352ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContextFreeGrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNonterminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ContextFreeGrammar'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.grammar import ContextFreeGrammar, Nonterminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-0ddb0cc0b49b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-0ddb0cc0b49b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import treebank-grammar as tg\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import treebank-grammar as tg\n",
    "g = tg.extract_simple_pcfg(1000)\n",
    "import nltk.parse as np\n",
    "c = np.ChartParser(g)\n",
    "p = c.parse(’The rates rise .’.split())\n",
    "len(list(p))       # prints number of parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = c.parse(’The rates rise .’.split())\n",
    "print(p.next())    # prints first parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Analytics', 'NNS'),\n",
       " ('Vidhya', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('platform', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('It', 'PRP'),\n",
       " ('helps', 'VBZ'),\n",
       " ('community', 'NN'),\n",
       " ('through', 'IN'),\n",
       " ('blogs', 'NNS'),\n",
       " ('hackathons', 'NNS'),\n",
       " ('discussions', 'NNS'),\n",
       " ('etc', 'FW')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c080f6458562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (__init__.py, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/rousselpaul/Downloads/pattern-2.6/pattern/text/__init__.py\"\u001b[0;36m, line \u001b[0;32m102\u001b[0m\n\u001b[0;31m    print \"\\n\\n\".join([table(sentence, fill=column) for sentence in Text(string, token)])\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.array([int(x) for x in y_train.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1', ..., '0', '0', '1'], dtype='<U21')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 500building tree 3 of 500building tree 1 of 500building tree 4 of 500\n",
      "\n",
      "\n",
      "\n",
      "building tree 5 of 500\n",
      "building tree 6 of 500\n",
      "building tree 7 of 500\n",
      "building tree 8 of 500\n",
      "building tree 9 of 500\n",
      "building tree 10 of 500\n",
      "building tree 11 of 500\n",
      "building tree 12 of 500\n",
      "building tree 13 of 500\n",
      "building tree 14 of 500\n",
      "building tree 15 of 500\n",
      "building tree 16 of 500\n",
      "building tree 17 of 500\n",
      "building tree 18 of 500\n",
      "building tree 19 of 500\n",
      "building tree 20 of 500\n",
      "building tree 21 of 500\n",
      "building tree 22 of 500\n",
      "building tree 23 of 500\n",
      "building tree 24 of 500\n",
      "building tree 25 of 500\n",
      "building tree 26 of 500\n",
      "building tree 27 of 500\n",
      "building tree 28 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 29 of 500\n",
      "building tree 30 of 500\n",
      "building tree 31 of 500\n",
      "building tree 32 of 500\n",
      "building tree 33 of 500\n",
      "building tree 34 of 500\n",
      "building tree 35 of 500\n",
      "building tree 36 of 500\n",
      "building tree 37 of 500\n",
      "building tree 38 of 500\n",
      "building tree 39 of 500building tree 40 of 500\n",
      "\n",
      "building tree 41 of 500\n",
      "building tree 42 of 500\n",
      "building tree 43 of 500\n",
      "building tree 44 of 500\n",
      "building tree 45 of 500\n",
      "building tree 46 of 500\n",
      "building tree 47 of 500\n",
      "building tree 48 of 500\n",
      "building tree 49 of 500\n",
      "building tree 50 of 500\n",
      "building tree 51 of 500\n",
      "building tree 52 of 500\n",
      "building tree 53 of 500\n",
      "building tree 54 of 500\n",
      "building tree 55 of 500\n",
      "building tree 56 of 500\n",
      "building tree 57 of 500\n",
      "building tree 58 of 500\n",
      "building tree 59 of 500\n",
      "building tree 60 of 500\n",
      "building tree 61 of 500\n",
      "building tree 62 of 500\n",
      "building tree 63 of 500\n",
      "building tree 64 of 500\n",
      "building tree 65 of 500\n",
      "building tree 66 of 500\n",
      "building tree 67 of 500\n",
      "building tree 68 of 500\n",
      "building tree 69 of 500\n",
      "building tree 70 of 500\n",
      "building tree 71 of 500\n",
      "building tree 72 of 500\n",
      "building tree 73 of 500\n",
      "building tree 74 of 500\n",
      "building tree 75 of 500\n",
      "building tree 76 of 500\n",
      "building tree 77 of 500\n",
      "building tree 78 of 500\n",
      "building tree 79 of 500\n",
      "building tree 80 of 500\n",
      "building tree 81 of 500\n",
      "building tree 82 of 500\n",
      "building tree 83 of 500\n",
      "building tree 84 of 500\n",
      "building tree 85 of 500\n",
      "building tree 86 of 500\n",
      "building tree 87 of 500\n",
      "building tree 88 of 500\n",
      "building tree 89 of 500\n",
      "building tree 90 of 500\n",
      "building tree 91 of 500\n",
      "building tree 92 of 500\n",
      "building tree 93 of 500\n",
      "building tree 94 of 500\n",
      "building tree 95 of 500\n",
      "building tree 96 of 500\n",
      "building tree 97 of 500\n",
      "building tree 98 of 500\n",
      "building tree 99 of 500\n",
      "building tree 100 of 500\n",
      "building tree 101 of 500\n",
      "building tree 102 of 500\n",
      "building tree 103 of 500\n",
      "building tree 104 of 500\n",
      "building tree 105 of 500\n",
      "building tree 106 of 500\n",
      "building tree 107 of 500\n",
      "building tree 108 of 500\n",
      "building tree 109 of 500\n",
      "building tree 110 of 500\n",
      "building tree 111 of 500\n",
      "building tree 112 of 500\n",
      "building tree 113 of 500\n",
      "building tree 114 of 500\n",
      "building tree 115 of 500\n",
      "building tree 116 of 500\n",
      "building tree 117 of 500\n",
      "building tree 118 of 500\n",
      "building tree 119 of 500\n",
      "building tree 120 of 500\n",
      "building tree 121 of 500\n",
      "building tree 122 of 500\n",
      "building tree 123 of 500\n",
      "building tree 124 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   20.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 125 of 500\n",
      "building tree 126 of 500\n",
      "building tree 127 of 500\n",
      "building tree 128 of 500\n",
      "building tree 129 of 500\n",
      "building tree 130 of 500\n",
      "building tree 131 of 500\n",
      "building tree 132 of 500\n",
      "building tree 133 of 500\n",
      "building tree 134 of 500\n",
      "building tree 135 of 500\n",
      "building tree 136 of 500\n",
      "building tree 137 of 500\n",
      "building tree 138 of 500\n",
      "building tree 139 of 500\n",
      "building tree 140 of 500\n",
      "building tree 141 of 500\n",
      "building tree 142 of 500\n",
      "building tree 143 of 500\n",
      "building tree 144 of 500\n",
      "building tree 145 of 500\n",
      "building tree 146 of 500\n",
      "building tree 147 of 500\n",
      "building tree 148 of 500\n",
      "building tree 149 of 500\n",
      "building tree 150 of 500\n",
      "building tree 151 of 500\n",
      "building tree 152 of 500\n",
      "building tree 153 of 500\n",
      "building tree 154 of 500\n",
      "building tree 155 of 500\n",
      "building tree 156 of 500\n",
      "building tree 157 of 500\n",
      "building tree 158 of 500\n",
      "building tree 159 of 500\n",
      "building tree 160 of 500\n",
      "building tree 161 of 500\n",
      "building tree 162 of 500\n",
      "building tree 163 of 500\n",
      "building tree 164 of 500\n",
      "building tree 165 of 500\n",
      "building tree 166 of 500\n",
      "building tree 167 of 500\n",
      "building tree 168 of 500\n",
      "building tree 169 of 500\n",
      "building tree 170 of 500\n",
      "building tree 171 of 500\n",
      "building tree 172 of 500\n",
      "building tree 173 of 500\n",
      "building tree 174 of 500\n",
      "building tree 175 of 500\n",
      "building tree 176 of 500\n",
      "building tree 177 of 500\n",
      "building tree 178 of 500\n",
      "building tree 179 of 500\n",
      "building tree 180 of 500\n",
      "building tree 181 of 500\n",
      "building tree 182 of 500\n",
      "building tree 183 of 500\n",
      "building tree 184 of 500\n",
      "building tree 185 of 500\n",
      "building tree 186 of 500\n",
      "building tree 187 of 500\n",
      "building tree 188 of 500\n",
      "building tree 189 of 500\n",
      "building tree 190 of 500\n",
      "building tree 191 of 500\n",
      "building tree 192 of 500\n",
      "building tree 193 of 500\n",
      "building tree 194 of 500\n",
      "building tree 195 of 500\n",
      "building tree 196 of 500\n",
      "building tree 197 of 500\n",
      "building tree 198 of 500\n",
      "building tree 199 of 500\n",
      "building tree 200 of 500\n",
      "building tree 201 of 500\n",
      "building tree 202 of 500\n",
      "building tree 203 of 500\n",
      "building tree 204 of 500\n",
      "building tree 205 of 500\n",
      "building tree 206 of 500\n",
      "building tree 207 of 500\n",
      "building tree 208 of 500\n",
      "building tree 209 of 500\n",
      "building tree 210 of 500\n",
      "building tree 211 of 500\n",
      "building tree 212 of 500\n",
      "building tree 213 of 500\n",
      "building tree 214 of 500\n",
      "building tree 215 of 500\n",
      "building tree 216 of 500\n",
      "building tree 217 of 500\n",
      "building tree 218 of 500\n",
      "building tree 219 of 500\n",
      "building tree 220 of 500\n",
      "building tree 221 of 500\n",
      "building tree 222 of 500\n",
      "building tree 223 of 500\n",
      "building tree 224 of 500\n",
      "building tree 225 of 500\n",
      "building tree 226 of 500\n",
      "building tree 227 of 500\n",
      "building tree 228 of 500\n",
      "building tree 229 of 500\n",
      "building tree 230 of 500\n",
      "building tree 231 of 500\n",
      "building tree 232 of 500\n",
      "building tree 233 of 500\n",
      "building tree 234 of 500\n",
      "building tree 235 of 500\n",
      "building tree 236 of 500\n",
      "building tree 237 of 500\n",
      "building tree 238 of 500\n",
      "building tree 239 of 500\n",
      "building tree 240 of 500\n",
      "building tree 241 of 500\n",
      "building tree 242 of 500\n",
      "building tree 243 of 500\n",
      "building tree 244 of 500\n",
      "building tree 245 of 500\n",
      "building tree 246 of 500\n",
      "building tree 247 of 500\n",
      "building tree 248 of 500\n",
      "building tree 249 of 500\n",
      "building tree 250 of 500\n",
      "building tree 251 of 500\n",
      "building tree 252 of 500\n",
      "building tree 253 of 500\n",
      "building tree 254 of 500\n",
      "building tree 255 of 500\n",
      "building tree 256 of 500\n",
      "building tree 257 of 500\n",
      "building tree 258 of 500\n",
      "building tree 259 of 500\n",
      "building tree 260 of 500\n",
      "building tree 261 of 500\n",
      "building tree 262 of 500\n",
      "building tree 263 of 500\n",
      "building tree 264 of 500\n",
      "building tree 265 of 500\n",
      "building tree 266 of 500\n",
      "building tree 267 of 500\n",
      "building tree 268 of 500\n",
      "building tree 269 of 500\n",
      "building tree 270 of 500\n",
      "building tree 271 of 500\n",
      "building tree 272 of 500\n",
      "building tree 273 of 500\n",
      "building tree 274 of 500\n",
      "building tree 275 of 500\n",
      "building tree 276 of 500\n",
      "building tree 277 of 500\n",
      "building tree 278 of 500\n",
      "building tree 279 of 500\n",
      "building tree 280 of 500\n",
      "building tree 281 of 500\n",
      "building tree 282 of 500\n",
      "building tree 283 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   47.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 284 of 500\n",
      "building tree 285 of 500\n",
      "building tree 286 of 500\n",
      "building tree 287 of 500\n",
      "building tree 288 of 500\n",
      "building tree 289 of 500\n",
      "building tree 290 of 500\n",
      "building tree 291 of 500\n",
      "building tree 292 of 500\n",
      "building tree 293 of 500\n",
      "building tree 294 of 500\n",
      "building tree 295 of 500\n",
      "building tree 296 of 500\n",
      "building tree 297 of 500\n",
      "building tree 298 of 500\n",
      "building tree 299 of 500\n",
      "building tree 300 of 500\n",
      "building tree 301 of 500\n",
      "building tree 302 of 500\n",
      "building tree 303 of 500\n",
      "building tree 304 of 500\n",
      "building tree 305 of 500\n",
      "building tree 306 of 500\n",
      "building tree 307 of 500\n",
      "building tree 308 of 500\n",
      "building tree 309 of 500\n",
      "building tree 310 of 500\n",
      "building tree 311 of 500\n",
      "building tree 312 of 500\n",
      "building tree 313 of 500\n",
      "building tree 314 of 500\n",
      "building tree 315 of 500\n",
      "building tree 316 of 500\n",
      "building tree 317 of 500\n",
      "building tree 318 of 500\n",
      "building tree 319 of 500\n",
      "building tree 320 of 500\n",
      "building tree 321 of 500\n",
      "building tree 322 of 500\n",
      "building tree 323 of 500\n",
      "building tree 324 of 500\n",
      "building tree 325 of 500\n",
      "building tree 326 of 500\n",
      "building tree 327 of 500\n",
      "building tree 328 of 500\n",
      "building tree 329 of 500\n",
      "building tree 330 of 500\n",
      "building tree 331 of 500\n",
      "building tree 332 of 500\n",
      "building tree 333 of 500\n",
      "building tree 334 of 500\n",
      "building tree 335 of 500\n",
      "building tree 336 of 500\n",
      "building tree 337 of 500\n",
      "building tree 338 of 500\n",
      "building tree 339 of 500\n",
      "building tree 340 of 500\n",
      "building tree 341 of 500\n",
      "building tree 342 of 500\n",
      "building tree 343 of 500\n",
      "building tree 344 of 500\n",
      "building tree 345 of 500\n",
      "building tree 346 of 500\n",
      "building tree 347 of 500\n",
      "building tree 348 of 500\n",
      "building tree 349 of 500\n",
      "building tree 350 of 500\n",
      "building tree 351 of 500\n",
      "building tree 352 of 500\n",
      "building tree 353 of 500\n",
      "building tree 354 of 500\n",
      "building tree 355 of 500\n",
      "building tree 356 of 500\n",
      "building tree 357 of 500\n",
      "building tree 358 of 500\n",
      "building tree 359 of 500\n",
      "building tree 360 of 500\n",
      "building tree 361 of 500\n",
      "building tree 362 of 500\n",
      "building tree 363 of 500\n",
      "building tree 364 of 500\n",
      "building tree 365 of 500\n",
      "building tree 366 of 500\n",
      "building tree 367 of 500\n",
      "building tree 368 of 500\n",
      "building tree 369 of 500\n",
      "building tree 370 of 500\n",
      "building tree 371 of 500\n",
      "building tree 372 of 500\n",
      "building tree 373 of 500\n",
      "building tree 374 of 500\n",
      "building tree 375 of 500\n",
      "building tree 376 of 500\n",
      "building tree 377 of 500\n",
      "building tree 378 of 500\n",
      "building tree 379 of 500\n",
      "building tree 380 of 500\n",
      "building tree 381 of 500\n",
      "building tree 382 of 500\n",
      "building tree 383 of 500\n",
      "building tree 384 of 500\n",
      "building tree 385 of 500\n",
      "building tree 386 of 500\n",
      "building tree 387 of 500\n",
      "building tree 388 of 500\n",
      "building tree 389 of 500\n",
      "building tree 390 of 500\n",
      "building tree 391 of 500\n",
      "building tree 392 of 500\n",
      "building tree 393 of 500\n",
      "building tree 394 of 500\n",
      "building tree 395 of 500\n",
      "building tree 396 of 500\n",
      "building tree 397 of 500\n",
      "building tree 398 of 500\n",
      "building tree 399 of 500\n",
      "building tree 400 of 500\n",
      "building tree 401 of 500\n",
      "building tree 402 of 500\n",
      "building tree 403 of 500\n",
      "building tree 404 of 500\n",
      "building tree 405 of 500\n",
      "building tree 406 of 500\n",
      "building tree 407 of 500\n",
      "building tree 408 of 500\n",
      "building tree 409 of 500\n",
      "building tree 410 of 500\n",
      "building tree 411 of 500\n",
      "building tree 412 of 500\n",
      "building tree 413 of 500\n",
      "building tree 414 of 500\n",
      "building tree 415 of 500\n",
      "building tree 416 of 500\n",
      "building tree 417 of 500\n",
      "building tree 418 of 500\n",
      "building tree 419 of 500\n",
      "building tree 420 of 500\n",
      "building tree 421 of 500\n",
      "building tree 422 of 500\n",
      "building tree 423 of 500\n",
      "building tree 424 of 500\n",
      "building tree 425 of 500\n",
      "building tree 426 of 500\n",
      "building tree 427 of 500\n",
      "building tree 428 of 500\n",
      "building tree 429 of 500\n",
      "building tree 430 of 500\n",
      "building tree 431 of 500\n",
      "building tree 432 of 500\n",
      "building tree 433 of 500\n",
      "building tree 434 of 500\n",
      "building tree 435 of 500\n",
      "building tree 436 of 500\n",
      "building tree 437 of 500\n",
      "building tree 438 of 500\n",
      "building tree 439 of 500\n",
      "building tree 440 of 500\n",
      "building tree 441 of 500\n",
      "building tree 442 of 500\n",
      "building tree 443 of 500\n",
      "building tree 444 of 500\n",
      "building tree 445 of 500\n",
      "building tree 446 of 500\n",
      "building tree 447 of 500\n",
      "building tree 448 of 500\n",
      "building tree 449 of 500\n",
      "building tree 450 of 500\n",
      "building tree 451 of 500\n",
      "building tree 452 of 500\n",
      "building tree 453 of 500\n",
      "building tree 454 of 500\n",
      "building tree 455 of 500\n",
      "building tree 456 of 500\n",
      "building tree 457 of 500\n",
      "building tree 458 of 500\n",
      "building tree 459 of 500\n",
      "building tree 460 of 500\n",
      "building tree 461 of 500\n",
      "building tree 462 of 500\n",
      "building tree 463 of 500\n",
      "building tree 464 of 500\n",
      "building tree 465 of 500\n",
      "building tree 466 of 500\n",
      "building tree 467 of 500\n",
      "building tree 468 of 500\n",
      "building tree 469 of 500\n",
      "building tree 470 of 500\n",
      "building tree 471 of 500\n",
      "building tree 472 of 500\n",
      "building tree 473 of 500\n",
      "building tree 474 of 500\n",
      "building tree 475 of 500\n",
      "building tree 476 of 500\n",
      "building tree 477 of 500\n",
      "building tree 478 of 500\n",
      "building tree 479 of 500\n",
      "building tree 480 of 500\n",
      "building tree 481 of 500\n",
      "building tree 482 of 500\n",
      "building tree 483 of 500\n",
      "building tree 484 of 500\n",
      "building tree 485 of 500\n",
      "building tree 486 of 500\n",
      "building tree 487 of 500\n",
      "building tree 488 of 500\n",
      "building tree 489 of 500\n",
      "building tree 490 of 500\n",
      "building tree 491 of 500\n",
      "building tree 492 of 500\n",
      "building tree 493 of 500\n",
      "building tree 494 of 500\n",
      "building tree 495 of 500\n",
      "building tree 496 of 500\n",
      "building tree 497 of 500\n",
      "building tree 498 of 500\n",
      "building tree 499 of 500\n",
      "building tree 500 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63769125 0.36230875]\n",
      " [0.09649006 0.90350994]\n",
      " [0.64587736 0.35412264]\n",
      " ...\n",
      " [0.72428397 0.27571603]\n",
      " [0.66371163 0.33628837]\n",
      " [0.07613816 0.92386184]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=500, max_depth=4, n_jobs=-1,verbose=3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2983443718893396"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(np.array([int(x) for x in y_test.tolist()]), y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-8bfec4fcf83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reg:logistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxgb_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rousselpaul/anaconda/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1045\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=200,verbose=3, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='reg:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
    "xgb_prediction = xgb_model.predict_proba(X_test)\n",
    "print(xgb_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(np.array([int(x) for x in y_test.tolist()]), xgb_prediction[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "56070/56070 [==============================] - 4s 67us/step - loss: 0.8459 - acc: 0.6070\n",
      "Epoch 2/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7409 - acc: 0.6563\n",
      "Epoch 3/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7332 - acc: 0.6681\n",
      "Epoch 4/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7377 - acc: 0.6623\n",
      "Epoch 5/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7291 - acc: 0.6695\n",
      "Epoch 6/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7335 - acc: 0.6620\n",
      "Epoch 7/30\n",
      "56070/56070 [==============================] - 1s 12us/step - loss: 0.7302 - acc: 0.6649\n",
      "Epoch 8/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7239 - acc: 0.6710\n",
      "Epoch 9/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7263 - acc: 0.6656\n",
      "Epoch 10/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7212 - acc: 0.6711\n",
      "Epoch 11/30\n",
      "56070/56070 [==============================] - 1s 13us/step - loss: 0.7229 - acc: 0.6662\n",
      "Epoch 12/30\n",
      "56070/56070 [==============================] - 1s 12us/step - loss: 0.7151 - acc: 0.6736\n",
      "Epoch 13/30\n",
      "56070/56070 [==============================] - 1s 14us/step - loss: 0.7151 - acc: 0.6736\n",
      "Epoch 14/30\n",
      "56070/56070 [==============================] - 1s 17us/step - loss: 0.7104 - acc: 0.6794\n",
      "Epoch 15/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7079 - acc: 0.6810\n",
      "Epoch 16/30\n",
      "56070/56070 [==============================] - 1s 12us/step - loss: 0.7072 - acc: 0.6774\n",
      "Epoch 17/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7086 - acc: 0.6727\n",
      "Epoch 18/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7090 - acc: 0.6706\n",
      "Epoch 19/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7049 - acc: 0.6745\n",
      "Epoch 20/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7005 - acc: 0.6800\n",
      "Epoch 21/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7041 - acc: 0.6733\n",
      "Epoch 22/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6996 - acc: 0.6765\n",
      "Epoch 23/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6963 - acc: 0.6789\n",
      "Epoch 24/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6973 - acc: 0.6759\n",
      "Epoch 25/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.7008 - acc: 0.6686\n",
      "Epoch 26/30\n",
      "56070/56070 [==============================] - 1s 14us/step - loss: 0.6959 - acc: 0.6733\n",
      "Epoch 27/30\n",
      "56070/56070 [==============================] - 1s 13us/step - loss: 0.7017 - acc: 0.6645\n",
      "Epoch 28/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6910 - acc: 0.6788\n",
      "Epoch 29/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6924 - acc: 0.6712\n",
      "Epoch 30/30\n",
      "56070/56070 [==============================] - 1s 11us/step - loss: 0.6887 - acc: 0.6789\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l2(0.001),activation='linear'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l2(0.001),activation='linear'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "    \n",
    "])\n",
    "    \n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(X_train, np.array(y_train), epochs=30, batch_size=1000, verbose=1)\n",
    "#predictions\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(np.array([int(x) for x in y_test.tolist()]), predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On enregistre le résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission_file.csv', 'w') as f:\n",
    "\tf.write('Id,Score\\n')\n",
    "\tfor i in range(predictions.shape[0]):\n",
    "\t\tf.write(str(i)+','+str(predictions[i][1])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bordel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(pd.concat((df['question1'],df['question2'])).unique())\n",
    "trainq1_trans = tfidf_vect_ngram_chars.transform(df['question1'].values)\n",
    "trainq2_trans = tfidf_vect_ngram_chars.transform(df['question2'].values)\n",
    "labels = df['is_duplicate'].values\n",
    "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
    "y = labels\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train) \n",
    "xgb_prediction = xgb_model.predict(X_valid)\n",
    "print('character level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
    "print('character level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
    "print(classification_report(y_valid, xgb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
